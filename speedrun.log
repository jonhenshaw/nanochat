Using CPython [36m3.10.17[39m
Creating virtual environment at: [36m.venv[39m
Activate with: [32msource .venv/bin/activate[39m
[2mResolved [1m91 packages[0m [2min 0.75ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/86] [2mInstalling wheels...                                              [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/86] [2mfiles-to-prompt==0.6                                              [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/86] [2mfiles-to-prompt==0.6                                              [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/86] [2mfilelock==3.19.1                                                  [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/86] [2mfilelock==3.19.1                                                  [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/86] [2mfastapi==0.117.1                                                  [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [3/86] [2mfastapi==0.117.1                                                  [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [3/86] [2mmultiprocess==0.70.16                                             [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [4/86] [2mmultiprocess==0.70.16                                             [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [4/86] [2mfsspec==2025.3.0                                                  [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [5/86] [2mfsspec==2025.3.0                                                  [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [5/86] [2mhuggingface-hub==0.34.4                                           [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [6/86] [2mnvidia-cufft-cu12==11.3.3.83                                      [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [7/86] [2mnvidia-cufft-cu12==11.3.3.83                                      [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [7/86] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)      [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [8/86] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)      [0m[2Kâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [8/86] [2mmaturin==1.9.4                                                    [0m[2Kâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [9/86] [2mmaturin==1.9.4                                                    [0m[2Kâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [9/86] [2mregex==2025.9.1                                                   [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [49/86] [2mnvidia-cuda-nvrtc-cu12==12.8.93                                  [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ [83/86] [2mwandb==0.21.3                                                    [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ [85/86] [2mtorch==2.8.0+cu128                                               [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [86/86] [2mtorch==2.8.0+cu128                                               [0m[2K[2mInstalled [1m86 packages[0m [2min 419ms[0m[0m
 [32m+[39m [1maiohappyeyeballs[0m[2m==2.6.1[0m
 [32m+[39m [1maiohttp[0m[2m==3.12.15[0m
 [32m+[39m [1maiosignal[0m[2m==1.4.0[0m
 [32m+[39m [1mannotated-types[0m[2m==0.7.0[0m
 [32m+[39m [1manyio[0m[2m==4.10.0[0m
 [32m+[39m [1masync-timeout[0m[2m==5.0.1[0m
 [32m+[39m [1mattrs[0m[2m==25.3.0[0m
 [32m+[39m [1mcertifi[0m[2m==2025.8.3[0m
 [32m+[39m [1mcharset-normalizer[0m[2m==3.4.3[0m
 [32m+[39m [1mclick[0m[2m==8.2.1[0m
 [32m+[39m [1mdatasets[0m[2m==4.0.0[0m
 [32m+[39m [1mdill[0m[2m==0.3.8[0m
 [32m+[39m [1mexceptiongroup[0m[2m==1.3.0[0m
 [32m+[39m [1mfastapi[0m[2m==0.117.1[0m
 [32m+[39m [1mfilelock[0m[2m==3.19.1[0m
 [32m+[39m [1mfiles-to-prompt[0m[2m==0.6[0m
 [32m+[39m [1mfrozenlist[0m[2m==1.7.0[0m
 [32m+[39m [1mfsspec[0m[2m==2025.3.0[0m
 [32m+[39m [1mgitdb[0m[2m==4.0.12[0m
 [32m+[39m [1mgitpython[0m[2m==3.1.45[0m
 [32m+[39m [1mh11[0m[2m==0.16.0[0m
 [32m+[39m [1mhf-xet[0m[2m==1.1.9[0m
 [32m+[39m [1mhuggingface-hub[0m[2m==0.34.4[0m
 [32m+[39m [1midna[0m[2m==3.10[0m
 [32m+[39m [1miniconfig[0m[2m==2.1.0[0m
 [32m+[39m [1mjinja2[0m[2m==3.1.6[0m
 [32m+[39m [1mmarkupsafe[0m[2m==3.0.2[0m
 [32m+[39m [1mmaturin[0m[2m==1.9.4[0m
 [32m+[39m [1mmpmath[0m[2m==1.3.0[0m
 [32m+[39m [1mmultidict[0m[2m==6.6.4[0m
 [32m+[39m [1mmultiprocess[0m[2m==0.70.16[0m
 [32m+[39m [1mnanochat[0m[2m==0.1.0 (from file:///home/henny/workplace/nanochat)[0m
 [32m+[39m [1mnetworkx[0m[2m==3.4.2[0m
 [32m+[39m [1mnumpy[0m[2m==1.26.4[0m
 [32m+[39m [1mnvidia-cublas-cu12[0m[2m==12.8.4.1[0m
 [32m+[39m [1mnvidia-cuda-cupti-cu12[0m[2m==12.8.90[0m
 [32m+[39m [1mnvidia-cuda-nvrtc-cu12[0m[2m==12.8.93[0m
 [32m+[39m [1mnvidia-cuda-runtime-cu12[0m[2m==12.8.90[0m
 [32m+[39m [1mnvidia-cudnn-cu12[0m[2m==9.10.2.21[0m
 [32m+[39m [1mnvidia-cufft-cu12[0m[2m==11.3.3.83[0m
 [32m+[39m [1mnvidia-cufile-cu12[0m[2m==1.13.1.3[0m
 [32m+[39m [1mnvidia-curand-cu12[0m[2m==10.3.9.90[0m
 [32m+[39m [1mnvidia-cusolver-cu12[0m[2m==11.7.3.90[0m
 [32m+[39m [1mnvidia-cusparse-cu12[0m[2m==12.5.8.93[0m
 [32m+[39m [1mnvidia-cusparselt-cu12[0m[2m==0.7.1[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [32m+[39m [1mnvidia-nvjitlink-cu12[0m[2m==12.8.93[0m
 [32m+[39m [1mnvidia-nvtx-cu12[0m[2m==12.8.90[0m
 [32m+[39m [1mpackaging[0m[2m==25.0[0m
 [32m+[39m [1mpandas[0m[2m==2.3.2[0m
 [32m+[39m [1mplatformdirs[0m[2m==4.4.0[0m
 [32m+[39m [1mpluggy[0m[2m==1.6.0[0m
 [32m+[39m [1mpropcache[0m[2m==0.3.2[0m
 [32m+[39m [1mprotobuf[0m[2m==6.32.0[0m
 [32m+[39m [1mpsutil[0m[2m==7.1.0[0m
 [32m+[39m [1mpyarrow[0m[2m==21.0.0[0m
 [32m+[39m [1mpydantic[0m[2m==2.11.7[0m
 [32m+[39m [1mpydantic-core[0m[2m==2.33.2[0m
 [32m+[39m [1mpygments[0m[2m==2.19.2[0m
 [32m+[39m [1mpytest[0m[2m==8.4.2[0m
 [32m+[39m [1mpython-dateutil[0m[2m==2.9.0.post0[0m
 [32m+[39m [1mpytz[0m[2m==2025.2[0m
 [32m+[39m [1mpyyaml[0m[2m==6.0.2[0m
 [32m+[39m [1mregex[0m[2m==2025.9.1[0m
 [32m+[39m [1mrequests[0m[2m==2.32.5[0m
 [32m+[39m [1msentry-sdk[0m[2m==2.35.2[0m
 [32m+[39m [1msetuptools[0m[2m==80.9.0[0m
 [32m+[39m [1msix[0m[2m==1.17.0[0m
 [32m+[39m [1msmmap[0m[2m==5.0.2[0m
 [32m+[39m [1msniffio[0m[2m==1.3.1[0m
 [32m+[39m [1mstarlette[0m[2m==0.48.0[0m
 [32m+[39m [1msympy[0m[2m==1.14.0[0m
 [32m+[39m [1mtiktoken[0m[2m==0.11.0[0m
 [32m+[39m [1mtokenizers[0m[2m==0.22.0[0m
 [32m+[39m [1mtomli[0m[2m==2.2.1[0m
 [32m+[39m [1mtorch[0m[2m==2.8.0+cu128[0m
 [32m+[39m [1mtqdm[0m[2m==4.67.1[0m
 [32m+[39m [1mtriton[0m[2m==3.4.0[0m
 [32m+[39m [1mtyping-extensions[0m[2m==4.15.0[0m
 [32m+[39m [1mtyping-inspection[0m[2m==0.4.1[0m
 [32m+[39m [1mtzdata[0m[2m==2025.2[0m
 [32m+[39m [1murllib3[0m[2m==2.5.0[0m
 [32m+[39m [1muvicorn[0m[2m==0.36.0[0m
 [32m+[39m [1mwandb[0m[2m==0.21.3[0m
 [32m+[39m [1mxxhash[0m[2m==3.5.0[0m
 [32m+[39m [1myarl[0m[2m==1.20.1[0m
Reset report and wrote header to /home/henny/.cache/nanochat/report/header.md
info: downloading installer
[0m[1m[33mwarn: [0mIt looks like you have an existing rustup settings file at:
[0m[1m[33mwarn: [0m/home/henny/.rustup/settings.toml
[0m[1m[33mwarn: [0mRustup will install the default toolchain as specified in the settings file,
[0m[1m[33mwarn: [0minstead of the one inferred from the default host triple.
[0m[1minfo: [0mprofile set to 'default'
[0m[1minfo: [0mdefault host triple is x86_64-unknown-linux-gnu
[0m[1m[33mwarn: [0mUpdating existing toolchain, profile choice will be ignored
[0m[1minfo: [0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'
[0m[1minfo: [0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'

  [0m[1mstable-x86_64-unknown-linux-gnu unchanged[0m - rustc 1.91.1 (ed61e7d7e 2025-11-07)

[0m[1m
Rust is installed now. Great!
[0m
To get started you may need to restart your current shell.
This would reload your [0m[1mPATH[0m environment variable to include
Cargo's bin directory ($HOME/.cargo/bin).

To configure your current shell, you need to source
the corresponding [0m[1menv[0m file under $HOME/.cargo.

This is usually done by running one of the following (note the leading DOT):
. "$HOME/.cargo/env"            # For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
source $"($nu.home-path)/.cargo/env.nu"  # For nushell
[2mUninstalled [1m1 package[0m [2min 185ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mtorch==2.9.0                                                       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [1/1] [2mtorch==2.9.0                                                       [0m[2K[2mInstalled [1m1 package[0m [2min 380ms[0m[0m
ðŸ“¦ Including license file `LICENSE`
ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.10 at /home/henny/workplace/nanochat/.venv/bin/python
ðŸ“¡ Using build options bindings from pyproject.toml
[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mdatasets==4.0.0                                                                             [0m[2K[37mâ ™[0m [2mfastapi==0.117.1                                                                            [0m[2K[37mâ ™[0m [2mfiles-to-prompt==0.6                                                                        [0m[2K[37mâ ™[0m [2mpsutil==7.1.0                                                                               [0m[2K[37mâ ™[0m [2mregex==2025.9.1                                                                             [0m[2K[37mâ ™[0m [2msetuptools==80.9.0                                                                          [0m[2K[37mâ ™[0m [2mtiktoken==0.11.0                                                                            [0m[2K[37mâ ™[0m [2mtokenizers==0.22.0                                                                          [0m[2K[37mâ ™[0m [2mtorch==2.9.0                                                                                [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mpydantic-core==2.33.2                                                                       [0m[2K[37mâ ™[0m [2mtyping-extensions==4.15.0                                                                   [0m[2K[2mResolved [1m80 packages[0m [2min 186ms[0m[0m
[2mUninstalled [1m2 packages[0m [2min 6ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mtriton==3.5.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [3/3] [2mtriton==3.5.0                                                      [0m[2K[2mInstalled [1m3 packages[0m [2min 25ms[0m[0m
 [31m-[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.5[0m
 [32m+[39m [1mnvidia-nvshmem-cu12[0m[2m==3.3.20[0m
 [31m-[39m [1mtriton[0m[2m==3.4.0[0m
 [32m+[39m [1mtriton[0m[2m==3.5.0[0m
[1m[92m    Finished[0m `release` profile [optimized] target(s) in 0.05s
ðŸ“¦ Built wheel for CPython 3.10 to /tmp/.tmpV02dbD/nanochat-0.1.0-cp310-cp310-linux_x86_64.whl
âœï¸ Setting installed package as editable
ðŸ›  Installed nanochat-0.1.0
Downloading 8 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Done! Downloaded: 8/8 shards to /home/henny/.cache/nanochat/base_data
Downloading 240 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00008.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00009.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00010.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00011.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00012.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00013.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00014.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00015.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00016.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00017.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00018.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00019.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00020.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00021.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00022.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00023.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00024.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00025.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00026.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00027.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00028.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00029.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00030.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00031.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00032.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00045.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00033.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00046.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00047.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00034.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00048.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00049.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00035.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00050.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00051.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00052.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00036.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00053.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00054.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00037.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00055.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00060.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00038.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00056.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00039.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00057.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00061.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00058.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00062.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00040.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00063.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00041.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00059.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00064.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00042.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00043.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00065.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00044.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00066.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00067.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00068.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00069.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00070.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00071.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00072.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00073.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00074.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00075.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00090.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00091.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00076.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00092.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00077.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00093.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00078.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00094.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00079.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00095.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00096.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00097.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00098.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00080.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00105.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00099.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00081.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00100.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00120.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00082.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00106.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00101.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00121.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00083.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00107.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00102.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00122.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00084.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00108.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00103.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00123.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00109.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00085.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00124.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00104.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00110.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00125.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00086.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00126.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00087.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00111.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00127.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00088.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00112.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00128.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00089.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00135.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00113.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00129.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00136.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00130.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00114.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00137.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00131.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00115.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00138.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00132.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00116.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00139.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00133.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00117.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00140.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00134.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00118.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00141.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00150.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00142.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00151.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00119.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00152.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00143.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00144.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00153.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00180.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00145.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00165.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00154.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00146.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00166.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00147.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00155.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00167.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00181.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00148.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00168.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00149.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00182.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00169.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00170.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00183.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00171.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00156.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00172.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00184.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00157.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00173.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00195.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00185.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00174.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00196.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00158.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00175.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00197.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00186.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00176.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00198.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00159.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00177.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00187.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00199.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00160.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00178.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00188.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00161.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00200.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00179.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00189.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00201.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00190.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00162.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00191.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00202.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00192.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00203.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00210.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00163.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00193.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00211.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00204.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00212.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00205.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00164.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00194.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00206.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00213.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00214.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00207.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00215.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00208.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00216.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00209.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00217.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00218.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00219.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00225.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00220.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00226.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00227.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00221.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00228.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00222.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00229.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00223.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00230.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00224.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00231.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00232.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00233.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00234.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00235.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00236.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00237.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00238.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00239.parquet (already exists)
Done! Downloaded: 240/240 shards to /home/henny/.cache/nanochat/base_data
max_chars: 2,000,000,000
doc_cap: 10,000
vocab_size: 65,536
2025-11-10 22:27:47,589 - rustbpe - [32m[1mINFO[0m - Processing sequences from iterator (buffer_size: 8192)
2025-11-10 22:28:16,317 - rustbpe - [32m[1mINFO[0m - Processed 532496 sequences total, 2233873 unique
2025-11-10 22:28:16,480 - rustbpe - [32m[1mINFO[0m - Starting BPE training: 65271 merges to compute
2025-11-10 22:28:16,480 - rustbpe - [32m[1mINFO[0m - Computing initial pair counts from 2233873 unique sequences
2025-11-10 22:28:17,677 - rustbpe - [32m[1mINFO[0m - Building heap with 18337 unique pairs
2025-11-10 22:28:17,679 - rustbpe - [32m[1mINFO[0m - Starting merge loop
2025-11-10 22:28:20,691 - rustbpe - [32m[1mINFO[0m - Progress: [1m1%[0m (653/65271 merges) - Last merge: (32, 568) -> 908 (frequency: 255265)
2025-11-10 22:28:21,101 - rustbpe - [32m[1mINFO[0m - Progress: [1m2%[0m (1306/65271 merges) - Last merge: (87, 101) -> 1561 (frequency: 110111)
2025-11-10 22:28:21,337 - rustbpe - [32m[1mINFO[0m - Progress: [1m3%[0m (1959/65271 merges) - Last merge: (1347, 716) -> 2214 (frequency: 67040)
2025-11-10 22:28:21,561 - rustbpe - [32m[1mINFO[0m - Progress: [1m4%[0m (2611/65271 merges) - Last merge: (714, 280) -> 2866 (frequency: 46401)
2025-11-10 22:28:21,750 - rustbpe - [32m[1mINFO[0m - Progress: [1m5%[0m (3264/65271 merges) - Last merge: (1108, 289) -> 3519 (frequency: 34743)
2025-11-10 22:28:21,922 - rustbpe - [32m[1mINFO[0m - Progress: [1m6%[0m (3917/65271 merges) - Last merge: (420, 262) -> 4172 (frequency: 27219)
2025-11-10 22:28:22,041 - rustbpe - [32m[1mINFO[0m - Progress: [1m7%[0m (4569/65271 merges) - Last merge: (116, 97) -> 4824 (frequency: 22123)
2025-11-10 22:28:22,191 - rustbpe - [32m[1mINFO[0m - Progress: [1m8%[0m (5222/65271 merges) - Last merge: (1381, 108) -> 5477 (frequency: 18395)
2025-11-10 22:28:22,305 - rustbpe - [32m[1mINFO[0m - Progress: [1m9%[0m (5875/65271 merges) - Last merge: (3848, 5283) -> 6130 (frequency: 15547)
2025-11-10 22:28:22,430 - rustbpe - [32m[1mINFO[0m - Progress: [1m10%[0m (6528/65271 merges) - Last merge: (324, 1426) -> 6783 (frequency: 13336)
2025-11-10 22:28:22,510 - rustbpe - [32m[1mINFO[0m - Progress: [1m11%[0m (7180/65271 merges) - Last merge: (5059, 2488) -> 7435 (frequency: 11636)
2025-11-10 22:28:22,605 - rustbpe - [32m[1mINFO[0m - Progress: [1m12%[0m (7833/65271 merges) - Last merge: (509, 273) -> 8088 (frequency: 10199)
2025-11-10 22:28:22,669 - rustbpe - [32m[1mINFO[0m - Progress: [1m13%[0m (8486/65271 merges) - Last merge: (313, 7877) -> 8741 (frequency: 8981)
2025-11-10 22:28:22,754 - rustbpe - [32m[1mINFO[0m - Progress: [1m14%[0m (9138/65271 merges) - Last merge: (4870, 109) -> 9393 (frequency: 8021)
2025-11-10 22:28:22,826 - rustbpe - [32m[1mINFO[0m - Progress: [1m15%[0m (9791/65271 merges) - Last merge: (1045, 3484) -> 10046 (frequency: 7217)
2025-11-10 22:28:22,895 - rustbpe - [32m[1mINFO[0m - Progress: [1m16%[0m (10444/65271 merges) - Last merge: (372, 34) -> 10699 (frequency: 6515)
2025-11-10 22:28:22,947 - rustbpe - [32m[1mINFO[0m - Progress: [1m17%[0m (11097/65271 merges) - Last merge: (46, 8940) -> 11352 (frequency: 5894)
2025-11-10 22:28:23,024 - rustbpe - [32m[1mINFO[0m - Progress: [1m18%[0m (11749/65271 merges) - Last merge: (296, 2782) -> 12004 (frequency: 5367)
2025-11-10 22:28:23,074 - rustbpe - [32m[1mINFO[0m - Progress: [1m19%[0m (12402/65271 merges) - Last merge: (662, 9820) -> 12657 (frequency: 4972)
2025-11-10 22:28:23,120 - rustbpe - [32m[1mINFO[0m - Progress: [1m20%[0m (13055/65271 merges) - Last merge: (265, 2166) -> 13310 (frequency: 4591)
2025-11-10 22:28:23,174 - rustbpe - [32m[1mINFO[0m - Progress: [1m21%[0m (13707/65271 merges) - Last merge: (317, 380) -> 13962 (frequency: 4272)
2025-11-10 22:28:23,245 - rustbpe - [32m[1mINFO[0m - Progress: [1m22%[0m (14360/65271 merges) - Last merge: (6986, 1695) -> 14615 (frequency: 3966)
2025-11-10 22:28:23,296 - rustbpe - [32m[1mINFO[0m - Progress: [1m23%[0m (15013/65271 merges) - Last merge: (12113, 12401) -> 15268 (frequency: 3697)
2025-11-10 22:28:23,343 - rustbpe - [32m[1mINFO[0m - Progress: [1m24%[0m (15666/65271 merges) - Last merge: (949, 12073) -> 15921 (frequency: 3446)
2025-11-10 22:28:23,400 - rustbpe - [32m[1mINFO[0m - Progress: [1m25%[0m (16318/65271 merges) - Last merge: (420, 263) -> 16573 (frequency: 3226)
2025-11-10 22:28:23,484 - rustbpe - [32m[1mINFO[0m - Progress: [1m26%[0m (16971/65271 merges) - Last merge: (342, 263) -> 17226 (frequency: 3027)
2025-11-10 22:28:23,524 - rustbpe - [32m[1mINFO[0m - Progress: [1m27%[0m (17624/65271 merges) - Last merge: (441, 3460) -> 17879 (frequency: 2852)
2025-11-10 22:28:23,579 - rustbpe - [32m[1mINFO[0m - Progress: [1m28%[0m (18276/65271 merges) - Last merge: (313, 13271) -> 18531 (frequency: 2699)
2025-11-10 22:28:23,623 - rustbpe - [32m[1mINFO[0m - Progress: [1m29%[0m (18929/65271 merges) - Last merge: (45, 450) -> 19184 (frequency: 2546)
2025-11-10 22:28:23,657 - rustbpe - [32m[1mINFO[0m - Progress: [1m30%[0m (19582/65271 merges) - Last merge: (428, 3299) -> 19837 (frequency: 2400)
2025-11-10 22:28:23,684 - rustbpe - [32m[1mINFO[0m - Progress: [1m31%[0m (20235/65271 merges) - Last merge: (313, 274) -> 20490 (frequency: 2276)
2025-11-10 22:28:23,722 - rustbpe - [32m[1mINFO[0m - Progress: [1m32%[0m (20887/65271 merges) - Last merge: (2804, 293) -> 21142 (frequency: 2155)
2025-11-10 22:28:23,752 - rustbpe - [32m[1mINFO[0m - Progress: [1m33%[0m (21540/65271 merges) - Last merge: (629, 321) -> 21795 (frequency: 2049)
2025-11-10 22:28:23,784 - rustbpe - [32m[1mINFO[0m - Progress: [1m34%[0m (22193/65271 merges) - Last merge: (10820, 363) -> 22448 (frequency: 1945)
2025-11-10 22:28:23,819 - rustbpe - [32m[1mINFO[0m - Progress: [1m35%[0m (22845/65271 merges) - Last merge: (15499, 10886) -> 23100 (frequency: 1850)
2025-11-10 22:28:23,845 - rustbpe - [32m[1mINFO[0m - Progress: [1m36%[0m (23498/65271 merges) - Last merge: (5037, 756) -> 23753 (frequency: 1758)
2025-11-10 22:28:23,883 - rustbpe - [32m[1mINFO[0m - Progress: [1m37%[0m (24151/65271 merges) - Last merge: (1811, 1627) -> 24406 (frequency: 1673)
2025-11-10 22:28:23,912 - rustbpe - [32m[1mINFO[0m - Progress: [1m38%[0m (24803/65271 merges) - Last merge: (10235, 3181) -> 25058 (frequency: 1598)
2025-11-10 22:28:23,945 - rustbpe - [32m[1mINFO[0m - Progress: [1m39%[0m (25456/65271 merges) - Last merge: (16956, 1121) -> 25711 (frequency: 1525)
2025-11-10 22:28:23,980 - rustbpe - [32m[1mINFO[0m - Progress: [1m40%[0m (26109/65271 merges) - Last merge: (2419, 636) -> 26364 (frequency: 1462)
2025-11-10 22:28:24,013 - rustbpe - [32m[1mINFO[0m - Progress: [1m41%[0m (26762/65271 merges) - Last merge: (2549, 1122) -> 27017 (frequency: 1402)
2025-11-10 22:28:24,047 - rustbpe - [32m[1mINFO[0m - Progress: [1m42%[0m (27414/65271 merges) - Last merge: (99, 1091) -> 27669 (frequency: 1343)
2025-11-10 22:28:24,079 - rustbpe - [32m[1mINFO[0m - Progress: [1m43%[0m (28067/65271 merges) - Last merge: (216, 170) -> 28322 (frequency: 1288)
2025-11-10 22:28:24,113 - rustbpe - [32m[1mINFO[0m - Progress: [1m44%[0m (28720/65271 merges) - Last merge: (402, 263) -> 28975 (frequency: 1233)
2025-11-10 22:28:24,144 - rustbpe - [32m[1mINFO[0m - Progress: [1m45%[0m (29372/65271 merges) - Last merge: (19389, 466) -> 29627 (frequency: 1190)
2025-11-10 22:28:24,175 - rustbpe - [32m[1mINFO[0m - Progress: [1m46%[0m (30025/65271 merges) - Last merge: (4600, 76) -> 30280 (frequency: 1145)
2025-11-10 22:28:24,198 - rustbpe - [32m[1mINFO[0m - Progress: [1m47%[0m (30678/65271 merges) - Last merge: (304, 849) -> 30933 (frequency: 1100)
2025-11-10 22:28:24,225 - rustbpe - [32m[1mINFO[0m - Progress: [1m48%[0m (31331/65271 merges) - Last merge: (5090, 596) -> 31586 (frequency: 1063)
2025-11-10 22:28:24,246 - rustbpe - [32m[1mINFO[0m - Progress: [1m49%[0m (31983/65271 merges) - Last merge: (21702, 9305) -> 32238 (frequency: 1026)
2025-11-10 22:28:24,267 - rustbpe - [32m[1mINFO[0m - Progress: [1m50%[0m (32636/65271 merges) - Last merge: (78, 11441) -> 32891 (frequency: 988)
2025-11-10 22:28:24,293 - rustbpe - [32m[1mINFO[0m - Progress: [1m51%[0m (33289/65271 merges) - Last merge: (17782, 373) -> 33544 (frequency: 956)
2025-11-10 22:28:24,317 - rustbpe - [32m[1mINFO[0m - Progress: [1m52%[0m (33941/65271 merges) - Last merge: (6872, 474) -> 34196 (frequency: 925)
2025-11-10 22:28:24,341 - rustbpe - [32m[1mINFO[0m - Progress: [1m53%[0m (34594/65271 merges) - Last merge: (13160, 1816) -> 34849 (frequency: 893)
2025-11-10 22:28:24,363 - rustbpe - [32m[1mINFO[0m - Progress: [1m54%[0m (35247/65271 merges) - Last merge: (336, 1855) -> 35502 (frequency: 865)
2025-11-10 22:28:24,396 - rustbpe - [32m[1mINFO[0m - Progress: [1m55%[0m (35900/65271 merges) - Last merge: (446, 276) -> 36155 (frequency: 838)
2025-11-10 22:28:24,413 - rustbpe - [32m[1mINFO[0m - Progress: [1m56%[0m (36552/65271 merges) - Last merge: (337, 1876) -> 36807 (frequency: 811)
2025-11-10 22:28:24,430 - rustbpe - [32m[1mINFO[0m - Progress: [1m57%[0m (37205/65271 merges) - Last merge: (27812, 319) -> 37460 (frequency: 787)
2025-11-10 22:28:24,447 - rustbpe - [32m[1mINFO[0m - Progress: [1m58%[0m (37858/65271 merges) - Last merge: (2181, 2035) -> 38113 (frequency: 763)
2025-11-10 22:28:24,472 - rustbpe - [32m[1mINFO[0m - Progress: [1m59%[0m (38510/65271 merges) - Last merge: (529, 35912) -> 38765 (frequency: 739)
2025-11-10 22:28:24,489 - rustbpe - [32m[1mINFO[0m - Progress: [1m60%[0m (39163/65271 merges) - Last merge: (1626, 758) -> 39418 (frequency: 717)
2025-11-10 22:28:24,519 - rustbpe - [32m[1mINFO[0m - Progress: [1m61%[0m (39816/65271 merges) - Last merge: (2884, 36357) -> 40071 (frequency: 696)
2025-11-10 22:28:24,536 - rustbpe - [32m[1mINFO[0m - Progress: [1m62%[0m (40469/65271 merges) - Last merge: (22732, 852) -> 40724 (frequency: 676)
2025-11-10 22:28:24,611 - rustbpe - [32m[1mINFO[0m - Progress: [1m63%[0m (41121/65271 merges) - Last merge: (2718, 80) -> 41376 (frequency: 656)
2025-11-10 22:28:24,634 - rustbpe - [32m[1mINFO[0m - Progress: [1m64%[0m (41774/65271 merges) - Last merge: (5190, 424) -> 42029 (frequency: 637)
2025-11-10 22:28:24,656 - rustbpe - [32m[1mINFO[0m - Progress: [1m65%[0m (42427/65271 merges) - Last merge: (1598, 12834) -> 42682 (frequency: 619)
2025-11-10 22:28:24,679 - rustbpe - [32m[1mINFO[0m - Progress: [1m66%[0m (43079/65271 merges) - Last merge: (1929, 2037) -> 43334 (frequency: 602)
2025-11-10 22:28:24,691 - rustbpe - [32m[1mINFO[0m - Progress: [1m67%[0m (43732/65271 merges) - Last merge: (37877, 14879) -> 43987 (frequency: 585)
2025-11-10 22:28:24,712 - rustbpe - [32m[1mINFO[0m - Progress: [1m68%[0m (44385/65271 merges) - Last merge: (29758, 3004) -> 44640 (frequency: 570)
2025-11-10 22:28:24,724 - rustbpe - [32m[1mINFO[0m - Progress: [1m69%[0m (45037/65271 merges) - Last merge: (12204, 1404) -> 45292 (frequency: 555)
2025-11-10 22:28:24,744 - rustbpe - [32m[1mINFO[0m - Progress: [1m70%[0m (45690/65271 merges) - Last merge: (2806, 259) -> 45945 (frequency: 540)
2025-11-10 22:28:24,763 - rustbpe - [32m[1mINFO[0m - Progress: [1m71%[0m (46343/65271 merges) - Last merge: (9018, 34477) -> 46598 (frequency: 527)
2025-11-10 22:28:24,778 - rustbpe - [32m[1mINFO[0m - Progress: [1m72%[0m (46996/65271 merges) - Last merge: (420, 8269) -> 47251 (frequency: 513)
2025-11-10 22:28:24,796 - rustbpe - [32m[1mINFO[0m - Progress: [1m73%[0m (47648/65271 merges) - Last merge: (7466, 282) -> 47903 (frequency: 501)
2025-11-10 22:28:24,809 - rustbpe - [32m[1mINFO[0m - Progress: [1m74%[0m (48301/65271 merges) - Last merge: (438, 4996) -> 48556 (frequency: 489)
2025-11-10 22:28:24,822 - rustbpe - [32m[1mINFO[0m - Progress: [1m75%[0m (48954/65271 merges) - Last merge: (4604, 276) -> 49209 (frequency: 478)
2025-11-10 22:28:24,844 - rustbpe - [32m[1mINFO[0m - Progress: [1m76%[0m (49606/65271 merges) - Last merge: (1632, 13872) -> 49861 (frequency: 466)
2025-11-10 22:28:24,857 - rustbpe - [32m[1mINFO[0m - Progress: [1m77%[0m (50259/65271 merges) - Last merge: (3250, 8184) -> 50514 (frequency: 455)
2025-11-10 22:28:24,872 - rustbpe - [32m[1mINFO[0m - Progress: [1m78%[0m (50912/65271 merges) - Last merge: (560, 3010) -> 51167 (frequency: 445)
2025-11-10 22:28:24,884 - rustbpe - [32m[1mINFO[0m - Progress: [1m79%[0m (51565/65271 merges) - Last merge: (377, 3255) -> 51820 (frequency: 435)
2025-11-10 22:28:24,903 - rustbpe - [32m[1mINFO[0m - Progress: [1m80%[0m (52217/65271 merges) - Last merge: (37215, 1508) -> 52472 (frequency: 426)
2025-11-10 22:28:24,915 - rustbpe - [32m[1mINFO[0m - Progress: [1m81%[0m (52870/65271 merges) - Last merge: (14361, 31789) -> 53125 (frequency: 417)
2025-11-10 22:28:24,927 - rustbpe - [32m[1mINFO[0m - Progress: [1m82%[0m (53523/65271 merges) - Last merge: (18450, 285) -> 53778 (frequency: 408)
2025-11-10 22:28:24,938 - rustbpe - [32m[1mINFO[0m - Progress: [1m83%[0m (54175/65271 merges) - Last merge: (18546, 431) -> 54430 (frequency: 399)
2025-11-10 22:28:24,950 - rustbpe - [32m[1mINFO[0m - Progress: [1m84%[0m (54828/65271 merges) - Last merge: (1191, 105) -> 55083 (frequency: 390)
2025-11-10 22:28:24,964 - rustbpe - [32m[1mINFO[0m - Progress: [1m85%[0m (55481/65271 merges) - Last merge: (377, 112) -> 55736 (frequency: 382)
2025-11-10 22:28:24,976 - rustbpe - [32m[1mINFO[0m - Progress: [1m86%[0m (56134/65271 merges) - Last merge: (1554, 97) -> 56389 (frequency: 374)
2025-11-10 22:28:24,993 - rustbpe - [32m[1mINFO[0m - Progress: [1m87%[0m (56786/65271 merges) - Last merge: (35419, 791) -> 57041 (frequency: 366)
2025-11-10 22:28:25,006 - rustbpe - [32m[1mINFO[0m - Progress: [1m88%[0m (57439/65271 merges) - Last merge: (1149, 97) -> 57694 (frequency: 358)
2025-11-10 22:28:25,017 - rustbpe - [32m[1mINFO[0m - Progress: [1m89%[0m (58092/65271 merges) - Last merge: (3105, 755) -> 58347 (frequency: 351)
2025-11-10 22:28:25,034 - rustbpe - [32m[1mINFO[0m - Progress: [1m90%[0m (58744/65271 merges) - Last merge: (701, 44294) -> 58999 (frequency: 344)
2025-11-10 22:28:25,047 - rustbpe - [32m[1mINFO[0m - Progress: [1m91%[0m (59397/65271 merges) - Last merge: (441, 431) -> 59652 (frequency: 337)
2025-11-10 22:28:25,061 - rustbpe - [32m[1mINFO[0m - Progress: [1m92%[0m (60050/65271 merges) - Last merge: (111, 69) -> 60305 (frequency: 330)
2025-11-10 22:28:25,074 - rustbpe - [32m[1mINFO[0m - Progress: [1m93%[0m (60703/65271 merges) - Last merge: (313, 25817) -> 60958 (frequency: 323)
2025-11-10 22:28:25,085 - rustbpe - [32m[1mINFO[0m - Progress: [1m94%[0m (61355/65271 merges) - Last merge: (5375, 7100) -> 61610 (frequency: 317)
2025-11-10 22:28:25,096 - rustbpe - [32m[1mINFO[0m - Progress: [1m95%[0m (62008/65271 merges) - Last merge: (3460, 122) -> 62263 (frequency: 311)
2025-11-10 22:28:25,109 - rustbpe - [32m[1mINFO[0m - Progress: [1m96%[0m (62661/65271 merges) - Last merge: (4719, 1622) -> 62916 (frequency: 305)
2025-11-10 22:28:25,120 - rustbpe - [32m[1mINFO[0m - Progress: [1m97%[0m (63313/65271 merges) - Last merge: (58949, 46565) -> 63568 (frequency: 300)
2025-11-10 22:28:25,131 - rustbpe - [32m[1mINFO[0m - Progress: [1m98%[0m (63966/65271 merges) - Last merge: (45, 391) -> 64221 (frequency: 293)
2025-11-10 22:28:25,143 - rustbpe - [32m[1mINFO[0m - Progress: [1m99%[0m (64619/65271 merges) - Last merge: (1588, 18644) -> 64874 (frequency: 288)
2025-11-10 22:28:25,154 - rustbpe - [32m[1mINFO[0m - Progress: [1m100%[0m (65271/65271 merges) - Last merge: (1594, 552) -> 65526 (frequency: 283)
2025-11-10 22:28:25,154 - rustbpe - [32m[1mINFO[0m - Finished training: 65271 merges completed
Training time: 38.13s
Saved tokenizer encoding to /home/henny/.cache/nanochat/tokenizer/tokenizer.pkl
Saved token_bytes to /home/henny/.cache/nanochat/tokenizer/token_bytes.pt

Vocab sizes:
GPT-2: 50257
GPT-4: 100277
Ours: 65536

Comparison with GPT-2:
===============================================================================================
Text Type  Bytes    GPT-2           Ours            Relative     Better    
                    Tokens  Ratio   Tokens  Ratio   Diff %      
-----------------------------------------------------------------------------------------------
news       1819     [91m404    [0m [91m4.50   [0m [92m375    [0m [92m4.85   [0m [92m   +7.2%[0m     Ours      
korean     893      [91m745    [0m [91m1.20   [0m [92m721    [0m [92m1.24   [0m [92m   +3.2%[0m     Ours      
code       1259     [91m576    [0m [91m2.19   [0m [92m493    [0m [92m2.55   [0m [92m  +14.4%[0m     Ours      
math       1834     [92m936    [0m [92m1.96   [0m [91m966    [0m [91m1.90   [0m [91m   -3.2%[0m     GPT-2     
science    1112     [91m260    [0m [91m4.28   [0m [92m225    [0m [92m4.94   [0m [92m  +13.5%[0m     Ours      
fwe-train  4208518  [91m900364 [0m [91m4.67   [0m [92m856901 [0m [92m4.91   [0m [92m   +4.8%[0m     Ours      
fwe-val    4908443  [91m1059062[0m [91m4.63   [0m [92m1010356[0m [92m4.86   [0m [92m   +4.6%[0m     Ours      

Comparison with GPT-4:
===============================================================================================
Text Type  Bytes    GPT-4           Ours            Relative     Better    
                    Tokens  Ratio   Tokens  Ratio   Diff %      
-----------------------------------------------------------------------------------------------
news       1819     [91m387    [0m [91m4.70   [0m [92m375    [0m [92m4.85   [0m [92m   +3.1%[0m     Ours      
korean     893      [92m364    [0m [92m2.45   [0m [91m721    [0m [91m1.24   [0m [91m  -98.1%[0m     GPT-4     
code       1259     [92m309    [0m [92m4.07   [0m [91m493    [0m [91m2.55   [0m [91m  -59.5%[0m     GPT-4     
math       1834     [92m832    [0m [92m2.20   [0m [91m966    [0m [91m1.90   [0m [91m  -16.1%[0m     GPT-4     
science    1112     [91m249    [0m [91m4.47   [0m [92m225    [0m [92m4.94   [0m [92m   +9.6%[0m     Ours      
fwe-train  4208518  [91m874799 [0m [91m4.81   [0m [92m856901 [0m [92m4.91   [0m [92m   +2.0%[0m     Ours      
fwe-val    4908443  [91m1029691[0m [91m4.77   [0m [92m1010356[0m [92m4.86   [0m [92m   +1.9%[0m     Ours      
Waiting for dataset download to complete...
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)

                                                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                                                      â–‘â–‘â–ˆâ–ˆâ–ˆ                â–‘â–‘â–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘
     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–‘  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ
     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘
    
Overriding: depth = 20
Overriding: device_batch_size = 16
Overriding: run = dummy
Autodetected device type: cuda
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,

  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
        exec(code, run_globals)exec(code, run_globals)

  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)    torch.cuda.set_device(device)  # make "cuda" default to this device

  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
        torch.cuda.set_device(device)  # make "cuda" default to this device
torch._C._cuda_setDevice(device)  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device

torch.AcceleratorError:     CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
torch._C._cuda_setDevice(device)

torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this deviceTraceback (most recent call last):

  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
        torch._C._cuda_setDevice(device)return _run_code(code, main_globals, None,
torch
.  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    torch._C._cuda_setDevice(device)
torch.AcceleratorError        : exec(code, run_globals)return _run_code(code, main_globals, None,CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code

        exec(code, run_globals)
ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>

  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
        torch._C._cuda_setDevice(device)Traceback (most recent call last):

torch._C._cuda_setDevice(device)
torch.torch  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
AcceleratorError.: AcceleratorErrorCUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
: 
CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_train.py", line 71, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[W1110 22:28:38.997749013 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.998189494 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.998507062 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.999900805 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.004979438 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.008386643 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1110 22:28:38.043024256 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W1110 22:28:38.495000 91708 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91720 closing signal SIGTERM
E1110 22:28:38.659000 91708 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 91721) of binary: /home/henny/workplace/nanochat/.venv/bin/python3
Traceback (most recent call last):
  File "/home/henny/workplace/nanochat/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts.base_train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 91722)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 91723)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 91724)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 91725)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 91726)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 91727)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-10_22:28:38
  host      : henny-pc.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 91721)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Autodetected device type: cuda
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_loss.py", line 29, in <module>
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1110 22:28:44.219000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91839 closing signal SIGTERM
W1110 22:28:44.219000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91840 closing signal SIGTERM
W1110 22:28:44.220000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91841 closing signal SIGTERM
W1110 22:28:44.220000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91842 closing signal SIGTERM
W1110 22:28:44.220000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91843 closing signal SIGTERM
W1110 22:28:44.220000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91845 closing signal SIGTERM
W1110 22:28:44.221000 91827 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91846 closing signal SIGTERM
E1110 22:28:44.435000 91827 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 5 (pid: 91844) of binary: /home/henny/workplace/nanochat/.venv/bin/python3
Traceback (most recent call last):
  File "/home/henny/workplace/nanochat/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts.base_loss FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-10_22:28:44
  host      : henny-pc.localdomain
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 91844)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^B/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
    main()
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
    main()
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    Traceback (most recent call last):
torch._C._cuda_setDevice(device)
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
    main()
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
    main()
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Autodetected device type: cuda
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,

  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
        exec(code, run_globals)exec(code, run_globals)

  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
        main()main()

  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
        torch._C._cuda_setDevice(device)torch.cuda.set_device(device)  # make "cuda" default to this device

torch  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 212, in <module>
    main()
  File "/home/henny/workplace/nanochat/scripts/base_eval.py", line 156, in main
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
  File "/home/henny/workplace/nanochat/nanochat/common.py", line 165, in compute_init
    torch.cuda.set_device(device)  # make "cuda" default to this device
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
    torch._C._cuda_setDevice(device)
torch.AcceleratorError: CUDA error: invalid device ordinal
GPU device may be out of range, do you have enough GPUs?
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1110 22:28:50.276000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91920 closing signal SIGTERM
W1110 22:28:50.277000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91921 closing signal SIGTERM
W1110 22:28:50.277000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91922 closing signal SIGTERM
W1110 22:28:50.277000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91923 closing signal SIGTERM
W1110 22:28:50.277000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91924 closing signal SIGTERM
W1110 22:28:50.278000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91926 closing signal SIGTERM
W1110 22:28:50.278000 91901 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 91927 closing signal SIGTERM
E1110 22:28:50.492000 91901 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 5 (pid: 91925) of binary: /home/henny/workplace/nanochat/.venv/bin/python3
Traceback (most recent call last):
  File "/home/henny/workplace/nanochat/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts.base_eval FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-10_22:28:50
  host      : henny-pc.localdomain
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 91925)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 2235k  100 2235k    0     0  2402k      0 --:--:-- --:--:-- --:--:-- 2400k
^CTraceback (most recent call last):
  File "/home/henny/workplace/nanochat/.venv/bin/torchrun", line 4, in <module>
    from torch.distributed.run import main
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py", line 2161, in <module>
    from torch import _VF as _VF, functional as functional  # usort: skip
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/functional.py", line 8, in <module>
    import torch.nn.functional as F
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/nn/__init__.py", line 8, in <module>
    from torch.nn.modules import *  # usort: skip # noqa: F403
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 2, in <module>
    from .linear import Bilinear, Identity, LazyLinear, Linear  # usort: skip
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 7, in <module>
    from torch.nn import functional as F, init
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 11, in <module>
    from torch._jit_internal import (
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/_jit_internal.py", line 44, in <module>
    import torch.distributed.rpc
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/rpc/__init__.py", line 85, in <module>
    from .server_process_global_profiler import _server_process_global_profile
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/rpc/server_process_global_profiler.py", line 7, in <module>
    from torch.autograd.profiler_legacy import profile
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 589, in <module>
    from . import profiler
  File "/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py", line 28, in <module>
    from torch.autograd.profiler_util import (
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1012, in get_code
  File "<frozen importlib._bootstrap_external>", line 672, in _compile_bytecode
KeyboardInterrupt
[2mResolved [1m91 packages[0m [2min 2ms[0m[0m
[2mUninstalled [1m5 packages[0m [2min 409ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/4] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/4] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/4] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/4] [2mnvidia-nccl-cu12==2.27.3                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/4] [2mnvidia-nccl-cu12==2.27.3                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/4] [2mtriton==3.4.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ [3/4] [2mtriton==3.4.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ [3/4] [2mtorch==2.8.0+cu128                                                 [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [4/4] [2mtorch==2.8.0+cu128                                                 [0m[2K[2mInstalled [1m4 packages[0m [2min 539ms[0m[0m
 [33m~[39m [1mnanochat[0m[2m==0.1.0 (from file:///home/henny/workplace/nanochat)[0m
 [31m-[39m [1mnvidia-nccl-cu12[0m[2m==2.27.5[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [31m-[39m [1mnvidia-nvshmem-cu12[0m[2m==3.3.20[0m
 [31m-[39m [1mtorch[0m[2m==2.9.0[0m
 [32m+[39m [1mtorch[0m[2m==2.8.0+cu128[0m
 [31m-[39m [1mtriton[0m[2m==3.5.0[0m
 [32m+[39m [1mtriton[0m[2m==3.4.0[0m
Reset report and wrote header to /home/henny/.cache/nanochat/report/header.md
info: downloading installer
[0m[1m[33mwarn: [0mIt looks like you have an existing rustup settings file at:
[0m[1m[33mwarn: [0m/home/henny/.rustup/settings.toml
[0m[1m[33mwarn: [0mRustup will install the default toolchain as specified in the settings file,
[0m[1m[33mwarn: [0minstead of the one inferred from the default host triple.
[0m[1minfo: [0mprofile set to 'default'
[0m[1minfo: [0mdefault host triple is x86_64-unknown-linux-gnu
[0m[1m[33mwarn: [0mUpdating existing toolchain, profile choice will be ignored
[0m[1minfo: [0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'
[0m[1minfo: [0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'

  [0m[1mstable-x86_64-unknown-linux-gnu unchanged[0m - rustc 1.91.1 (ed61e7d7e 2025-11-07)

[0m[1m
Rust is installed now. Great!
[0m
To get started you may need to restart your current shell.
This would reload your [0m[1mPATH[0m environment variable to include
Cargo's bin directory ($HOME/.cargo/bin).

To configure your current shell, you need to source
the corresponding [0m[1menv[0m file under $HOME/.cargo.

This is usually done by running one of the following (note the leading DOT):
. "$HOME/.cargo/env"            # For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
source $"($nu.home-path)/.cargo/env.nu"  # For nushell
[2mUninstalled [1m1 package[0m [2min 193ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mtorch==2.9.0                                                       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [1/1] [2mtorch==2.9.0                                                       [0m[2K[2mInstalled [1m1 package[0m [2min 512ms[0m[0m
ðŸ“¦ Including license file `LICENSE`
ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.10 at /home/henny/workplace/nanochat/.venv/bin/python
ðŸ“¡ Using build options bindings from pyproject.toml
[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mdatasets==4.0.0                                                                             [0m[2K[37mâ ™[0m [2mfastapi==0.117.1                                                                            [0m[2K[37mâ ™[0m [2mfiles-to-prompt==0.6                                                                        [0m[2K[37mâ ™[0m [2mpsutil==7.1.0                                                                               [0m[2K[37mâ ™[0m [2mregex==2025.9.1                                                                             [0m[2K[37mâ ™[0m [2msetuptools==80.9.0                                                                          [0m[2K[37mâ ™[0m [2mtiktoken==0.11.0                                                                            [0m[2K[37mâ ™[0m [2mtokenizers==0.22.0                                                                          [0m[2K[37mâ ™[0m [2mtorch==2.9.0                                                                                [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mpydantic-core==2.33.2                                                                       [0m[2K[37mâ ™[0m [2mtyping-extensions==4.15.0                                                                   [0m[2K[2mResolved [1m80 packages[0m [2min 190ms[0m[0m
[2mUninstalled [1m2 packages[0m [2min 7ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mtriton==3.5.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [3/3] [2mtriton==3.5.0                                                      [0m[2K[2mInstalled [1m3 packages[0m [2min 35ms[0m[0m
 [31m-[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.5[0m
 [32m+[39m [1mnvidia-nvshmem-cu12[0m[2m==3.3.20[0m
 [31m-[39m [1mtriton[0m[2m==3.4.0[0m
 [32m+[39m [1mtriton[0m[2m==3.5.0[0m
[1m[92m    Finished[0m `release` profile [optimized] target(s) in 0.05s
ðŸ“¦ Built wheel for CPython 3.10 to /tmp/.tmpqSwGH4/nanochat-0.1.0-cp310-cp310-linux_x86_64.whl
âœï¸ Setting installed package as editable
ðŸ›  Installed nanochat-0.1.0
Downloading 8 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Done! Downloaded: 8/8 shards to /home/henny/.cache/nanochat/base_data
Downloading 240 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00015.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00016.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00017.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00008.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00018.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00019.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00009.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00020.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00010.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00021.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00011.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00022.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00012.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00013.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00023.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00014.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00024.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00025.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00026.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00027.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00028.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00029.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00030.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00031.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00032.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00033.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00034.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00035.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00036.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00037.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00038.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00039.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00045.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00040.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00041.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00046.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00042.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00043.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00047.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00048.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00060.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00044.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00049.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00061.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00050.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00062.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00063.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00051.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00064.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00052.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00053.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00054.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00055.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00065.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00066.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00056.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00067.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00057.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00068.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00058.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00069.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00059.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00070.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00071.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00075.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00076.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00072.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00077.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00073.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00078.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00074.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00079.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00080.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00081.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00082.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00083.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00084.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00085.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00086.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00090.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00087.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00091.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00088.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00089.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00092.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00093.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00094.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00095.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00096.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00097.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00098.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00105.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00099.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00120.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00106.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00100.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00101.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00121.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00107.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00122.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00108.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00123.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00102.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00124.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00103.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00109.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00125.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00135.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00104.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00110.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00126.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00136.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00127.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00111.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00137.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00128.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00112.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00150.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00138.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00129.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00113.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00151.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00130.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00114.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00139.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00131.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00115.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00132.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00140.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00133.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00152.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00134.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00116.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00141.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00117.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00153.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00118.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00154.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00165.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00142.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00119.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00155.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00166.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00156.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00143.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00167.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00157.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00144.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00158.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00168.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00145.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00159.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00169.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00146.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00170.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00147.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00171.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00148.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00172.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00149.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00180.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00173.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00181.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00160.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00174.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00182.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00175.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00161.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00183.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00162.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00184.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00176.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00185.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00177.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00163.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00186.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00178.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00164.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00187.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00195.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00179.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00188.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00189.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00196.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00190.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00191.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00197.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00192.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00198.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00193.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00194.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00199.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00200.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00210.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00211.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00201.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00212.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00213.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00225.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00214.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00202.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00215.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00203.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00216.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00226.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00217.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00218.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00204.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00227.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00219.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00205.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00228.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00220.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00206.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00221.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00207.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00229.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00222.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00208.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00223.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00209.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00230.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00224.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00231.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00232.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00233.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00234.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00235.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00236.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00237.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00238.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00239.parquet (already exists)
Done! Downloaded: 240/240 shards to /home/henny/.cache/nanochat/base_data
max_chars: 2,000,000,000
doc_cap: 10,000
vocab_size: 65,536
2025-11-10 22:46:12,986 - rustbpe - [32m[1mINFO[0m - Processing sequences from iterator (buffer_size: 8192)
^[c^X^C^CTraceback (most recent call last):
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/henny/.local/share/uv/python/cpython-3.10.17-linux-x86_64-gnu/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/henny/workplace/nanochat/scripts/tok_train.py", line 49, in <module>
    tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)
  File "/home/henny/workplace/nanochat/nanochat/tokenizer.py", line 169, in train_from_iterator
    tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)
  File "/home/henny/workplace/nanochat/scripts/tok_train.py", line 41, in text_iterator
    yield doc_text
KeyboardInterrupt
^C[2mResolved [1m91 packages[0m [2min 2ms[0m[0m
[2mUninstalled [1m5 packages[0m [2min 366ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/4] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/4] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/4] [2mnanochat==0.1.0 (from file:///home/henny/workplace/nanochat)       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/4] [2mnvidia-nccl-cu12==2.27.3                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/4] [2mnvidia-nccl-cu12==2.27.3                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/4] [2mtriton==3.4.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ [3/4] [2mtriton==3.4.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ [3/4] [2mtorch==2.8.0+cu128                                                 [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [4/4] [2mtorch==2.8.0+cu128                                                 [0m[2K[2mInstalled [1m4 packages[0m [2min 541ms[0m[0m
 [33m~[39m [1mnanochat[0m[2m==0.1.0 (from file:///home/henny/workplace/nanochat)[0m
 [31m-[39m [1mnvidia-nccl-cu12[0m[2m==2.27.5[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [31m-[39m [1mnvidia-nvshmem-cu12[0m[2m==3.3.20[0m
 [31m-[39m [1mtorch[0m[2m==2.9.0[0m
 [32m+[39m [1mtorch[0m[2m==2.8.0+cu128[0m
 [31m-[39m [1mtriton[0m[2m==3.5.0[0m
 [32m+[39m [1mtriton[0m[2m==3.4.0[0m
Reset report and wrote header to /home/henny/.cache/nanochat/report/header.md
qinfo: downloading installer
^C[2mResolved [1m91 packages[0m [2min 0.74ms[0m[0m
[2mAudited [1m86 packages[0m [2min 0.98ms[0m[0m
Reset report and wrote header to /home/henny/.cache/nanochat/report/header.md
info: downloading installer
[0m[1m[33mwarn: [0mIt looks like you have an existing rustup settings file at:
[0m[1m[33mwarn: [0m/home/henny/.rustup/settings.toml
[0m[1m[33mwarn: [0mRustup will install the default toolchain as specified in the settings file,
[0m[1m[33mwarn: [0minstead of the one inferred from the default host triple.
[0m[1minfo: [0mprofile set to 'default'
[0m[1minfo: [0mdefault host triple is x86_64-unknown-linux-gnu
[0m[1m[33mwarn: [0mUpdating existing toolchain, profile choice will be ignored
[0m[1minfo: [0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'
[0m[1minfo: [0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'

  [0m[1mstable-x86_64-unknown-linux-gnu unchanged[0m - rustc 1.91.1 (ed61e7d7e 2025-11-07)

[0m[1m
Rust is installed now. Great!
[0m
To get started you may need to restart your current shell.
This would reload your [0m[1mPATH[0m environment variable to include
Cargo's bin directory ($HOME/.cargo/bin).

To configure your current shell, you need to source
the corresponding [0m[1menv[0m file under $HOME/.cargo.

This is usually done by running one of the following (note the leading DOT):
. "$HOME/.cargo/env"            # For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
source $"($nu.home-path)/.cargo/env.nu"  # For nushell
[2mUninstalled [1m1 package[0m [2min 184ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] [2mtorch==2.9.0                                                       [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [1/1] [2mtorch==2.9.0                                                       [0m[2K[2mInstalled [1m1 package[0m [2min 508ms[0m[0m
ðŸ“¦ Including license file `LICENSE`
ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.10 at /home/henny/workplace/nanochat/.venv/bin/python
ðŸ“¡ Using build options bindings from pyproject.toml
[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ‹[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mResolving dependencies...                                                                   [0m[2K[37mâ ™[0m [2mdatasets==4.0.0                                                                             [0m[2K[37mâ ™[0m [2mfastapi==0.117.1                                                                            [0m[2K[37mâ ™[0m [2mfiles-to-prompt==0.6                                                                        [0m[2K[37mâ ™[0m [2mpsutil==7.1.0                                                                               [0m[2K[37mâ ™[0m [2mregex==2025.9.1                                                                             [0m[2K[37mâ ™[0m [2msetuptools==80.9.0                                                                          [0m[2K[37mâ ™[0m [2mtiktoken==0.11.0                                                                            [0m[2K[37mâ ™[0m [2mtokenizers==0.22.0                                                                          [0m[2K[37mâ ™[0m [2mtorch==2.9.0                                                                                [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-nvrtc-cu12==12.8.93                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-runtime-cu12==12.8.90                                                           [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cuda-cupti-cu12==12.8.90                                                             [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cudnn-cu12==9.10.2.21                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mnvidia-cublas-cu12==12.8.4.1                                                                [0m[2K[37mâ ™[0m [2mpydantic-core==2.33.2                                                                       [0m[2K[37mâ ™[0m [2mtyping-extensions==4.15.0                                                                   [0m[2K[2mResolved [1m80 packages[0m [2min 188ms[0m[0m
[2mUninstalled [1m2 packages[0m [2min 7ms[0m[0m
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mInstalling wheels...                                               [0m[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nccl-cu12==2.27.5                                           [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mnvidia-nvshmem-cu12==3.3.20                                        [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ [2/3] [2mtriton==3.5.0                                                      [0m[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [3/3] [2mtriton==3.5.0                                                      [0m[2K[2mInstalled [1m3 packages[0m [2min 35ms[0m[0m
 [31m-[39m [1mnvidia-nccl-cu12[0m[2m==2.27.3[0m
 [32m+[39m [1mnvidia-nccl-cu12[0m[2m==2.27.5[0m
 [32m+[39m [1mnvidia-nvshmem-cu12[0m[2m==3.3.20[0m
 [31m-[39m [1mtriton[0m[2m==3.4.0[0m
 [32m+[39m [1mtriton[0m[2m==3.5.0[0m
[1m[92m    Finished[0m `release` profile [optimized] target(s) in 0.05s
ðŸ“¦ Built wheel for CPython 3.10 to /tmp/.tmplx4MI1/nanochat-0.1.0-cp310-cp310-linux_x86_64.whl
âœï¸ Setting installed package as editable
ðŸ›  Installed nanochat-0.1.0
Downloading 8 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Done! Downloaded: 8/8 shards to /home/henny/.cache/nanochat/base_data
Downloading 240 shards using 4 workers...
Target directory: /home/henny/.cache/nanochat/base_data

Skipping /home/henny/.cache/nanochat/base_data/shard_00000.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00001.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00002.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00003.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00004.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00005.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00006.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00015.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00016.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00007.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00017.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00018.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00008.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00019.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00009.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00010.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00011.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00020.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00012.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00013.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00021.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00030.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00022.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00014.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00023.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00031.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00024.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00045.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00032.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00025.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00046.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00047.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00026.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00033.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00027.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00048.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00034.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00049.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00028.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00050.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00029.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00051.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00035.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00052.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00053.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00036.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00054.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00055.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00037.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00056.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00038.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00057.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00039.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00040.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00058.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00041.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00059.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00042.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00043.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00044.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00060.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00061.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00062.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00063.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00064.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00065.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00066.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00075.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00067.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00076.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00068.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00077.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00069.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00078.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00070.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00079.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00071.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00090.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00072.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00080.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00091.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00073.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00092.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00074.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00093.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00081.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00094.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00082.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00105.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00095.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00083.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00096.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00106.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00084.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00097.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00085.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00107.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00098.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00086.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00099.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00108.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00120.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00100.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00087.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00109.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00101.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00121.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00088.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00110.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00102.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00122.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00089.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00111.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00103.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00123.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00124.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00125.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00104.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00126.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00112.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00127.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00128.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00113.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00129.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00114.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00135.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00115.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00130.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00136.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00150.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00131.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00137.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00116.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00132.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00151.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00133.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00117.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00138.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00152.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00118.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00134.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00139.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00119.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00153.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00140.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00154.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00141.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00155.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00142.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00156.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00143.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00165.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00144.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00157.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00166.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00145.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00158.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00146.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00159.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00147.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00160.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00167.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00148.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00161.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00149.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00168.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00180.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00169.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00162.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00170.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00163.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00181.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00171.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00164.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00182.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00172.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00183.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00173.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00184.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00174.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00195.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00210.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00175.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00185.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00196.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00176.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00211.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00186.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00197.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00177.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00212.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00187.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00198.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00213.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00188.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00178.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00179.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00199.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00214.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00215.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00189.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00200.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00201.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00216.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00202.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00190.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00203.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00225.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00191.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00217.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00226.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00218.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00204.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00192.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00219.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00227.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00220.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00205.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00228.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00193.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00206.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00229.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00221.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00194.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00222.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00207.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00230.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00208.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00231.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00223.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00209.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00232.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00233.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00224.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00234.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00235.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00236.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00237.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00238.parquet (already exists)
Skipping /home/henny/.cache/nanochat/base_data/shard_00239.parquet (already exists)
Done! Downloaded: 240/240 shards to /home/henny/.cache/nanochat/base_data
max_chars: 2,000,000,000
doc_cap: 10,000
vocab_size: 65,536
2025-11-10 22:59:09,107 - rustbpe - [32m[1mINFO[0m - Processing sequences from iterator (buffer_size: 8192)
2025-11-10 22:59:37,712 - rustbpe - [32m[1mINFO[0m - Processed 532496 sequences total, 2233873 unique
2025-11-10 22:59:37,882 - rustbpe - [32m[1mINFO[0m - Starting BPE training: 65271 merges to compute
2025-11-10 22:59:37,882 - rustbpe - [32m[1mINFO[0m - Computing initial pair counts from 2233873 unique sequences
2025-11-10 22:59:39,092 - rustbpe - [32m[1mINFO[0m - Building heap with 18337 unique pairs
2025-11-10 22:59:39,094 - rustbpe - [32m[1mINFO[0m - Starting merge loop
2025-11-10 22:59:42,059 - rustbpe - [32m[1mINFO[0m - Progress: [1m1%[0m (653/65271 merges) - Last merge: (32, 568) -> 908 (frequency: 255265)
2025-11-10 22:59:42,456 - rustbpe - [32m[1mINFO[0m - Progress: [1m2%[0m (1306/65271 merges) - Last merge: (87, 101) -> 1561 (frequency: 110111)
2025-11-10 22:59:42,698 - rustbpe - [32m[1mINFO[0m - Progress: [1m3%[0m (1959/65271 merges) - Last merge: (1347, 716) -> 2214 (frequency: 67040)
2025-11-10 22:59:42,920 - rustbpe - [32m[1mINFO[0m - Progress: [1m4%[0m (2611/65271 merges) - Last merge: (714, 280) -> 2866 (frequency: 46401)
2025-11-10 22:59:43,114 - rustbpe - [32m[1mINFO[0m - Progress: [1m5%[0m (3264/65271 merges) - Last merge: (1108, 289) -> 3519 (frequency: 34743)
2025-11-10 22:59:43,272 - rustbpe - [32m[1mINFO[0m - Progress: [1m6%[0m (3917/65271 merges) - Last merge: (420, 262) -> 4172 (frequency: 27219)
2025-11-10 22:59:43,388 - rustbpe - [32m[1mINFO[0m - Progress: [1m7%[0m (4569/65271 merges) - Last merge: (116, 97) -> 4824 (frequency: 22123)
2025-11-10 22:59:43,535 - rustbpe - [32m[1mINFO[0m - Progress: [1m8%[0m (5222/65271 merges) - Last merge: (1381, 108) -> 5477 (frequency: 18395)
2025-11-10 22:59:43,647 - rustbpe - [32m[1mINFO[0m - Progress: [1m9%[0m (5875/65271 merges) - Last merge: (3848, 5283) -> 6130 (frequency: 15547)
2025-11-10 22:59:43,768 - rustbpe - [32m[1mINFO[0m - Progress: [1m10%[0m (6528/65271 merges) - Last merge: (324, 1426) -> 6783 (frequency: 13336)
2025-11-10 22:59:43,842 - rustbpe - [32m[1mINFO[0m - Progress: [1m11%[0m (7180/65271 merges) - Last merge: (5059, 2488) -> 7435 (frequency: 11636)
2025-11-10 22:59:43,936 - rustbpe - [32m[1mINFO[0m - Progress: [1m12%[0m (7833/65271 merges) - Last merge: (509, 273) -> 8088 (frequency: 10199)
2025-11-10 22:59:44,001 - rustbpe - [32m[1mINFO[0m - Progress: [1m13%[0m (8486/65271 merges) - Last merge: (313, 7877) -> 8741 (frequency: 8981)
2025-11-10 22:59:44,082 - rustbpe - [32m[1mINFO[0m - Progress: [1m14%[0m (9138/65271 merges) - Last merge: (4870, 109) -> 9393 (frequency: 8021)
2025-11-10 22:59:44,151 - rustbpe - [32m[1mINFO[0m - Progress: [1m15%[0m (9791/65271 merges) - Last merge: (1045, 3484) -> 10046 (frequency: 7217)
2025-11-10 22:59:44,217 - rustbpe - [32m[1mINFO[0m - Progress: [1m16%[0m (10444/65271 merges) - Last merge: (372, 34) -> 10699 (frequency: 6515)
2025-11-10 22:59:44,270 - rustbpe - [32m[1mINFO[0m - Progress: [1m17%[0m (11097/65271 merges) - Last merge: (46, 8940) -> 11352 (frequency: 5894)
2025-11-10 22:59:44,347 - rustbpe - [32m[1mINFO[0m - Progress: [1m18%[0m (11749/65271 merges) - Last merge: (296, 2782) -> 12004 (frequency: 5367)
2025-11-10 22:59:44,397 - rustbpe - [32m[1mINFO[0m - Progress: [1m19%[0m (12402/65271 merges) - Last merge: (662, 9820) -> 12657 (frequency: 4972)
2025-11-10 22:59:44,444 - rustbpe - [32m[1mINFO[0m - Progress: [1m20%[0m (13055/65271 merges) - Last merge: (265, 2166) -> 13310 (frequency: 4591)
2025-11-10 22:59:44,497 - rustbpe - [32m[1mINFO[0m - Progress: [1m21%[0m (13707/65271 merges) - Last merge: (317, 380) -> 13962 (frequency: 4272)
2025-11-10 22:59:44,564 - rustbpe - [32m[1mINFO[0m - Progress: [1m22%[0m (14360/65271 merges) - Last merge: (6986, 1695) -> 14615 (frequency: 3966)
2025-11-10 22:59:44,613 - rustbpe - [32m[1mINFO[0m - Progress: [1m23%[0m (15013/65271 merges) - Last merge: (12113, 12401) -> 15268 (frequency: 3697)
2025-11-10 22:59:44,664 - rustbpe - [32m[1mINFO[0m - Progress: [1m24%[0m (15666/65271 merges) - Last merge: (949, 12073) -> 15921 (frequency: 3446)
2025-11-10 22:59:44,722 - rustbpe - [32m[1mINFO[0m - Progress: [1m25%[0m (16318/65271 merges) - Last merge: (420, 263) -> 16573 (frequency: 3226)
2025-11-10 22:59:44,818 - rustbpe - [32m[1mINFO[0m - Progress: [1m26%[0m (16971/65271 merges) - Last merge: (342, 263) -> 17226 (frequency: 3027)
2025-11-10 22:59:44,858 - rustbpe - [32m[1mINFO[0m - Progress: [1m27%[0m (17624/65271 merges) - Last merge: (441, 3460) -> 17879 (frequency: 2852)
2025-11-10 22:59:44,913 - rustbpe - [32m[1mINFO[0m - Progress: [1m28%[0m (18276/65271 merges) - Last merge: (313, 13271) -> 18531 (frequency: 2699)
2025-11-10 22:59:44,955 - rustbpe - [32m[1mINFO[0m - Progress: [1m29%[0m (18929/65271 merges) - Last merge: (45, 450) -> 19184 (frequency: 2546)
2025-11-10 22:59:44,987 - rustbpe - [32m[1mINFO[0m - Progress: [1m30%[0m (19582/65271 merges) - Last merge: (428, 3299) -> 19837 (frequency: 2400)
2025-11-10 22:59:45,014 - rustbpe - [32m[1mINFO[0m - Progress: [1m31%[0m (20235/65271 merges) - Last merge: (313, 274) -> 20490 (frequency: 2276)
2025-11-10 22:59:45,052 - rustbpe - [32m[1mINFO[0m - Progress: [1m32%[0m (20887/65271 merges) - Last merge: (2804, 293) -> 21142 (frequency: 2155)
2025-11-10 22:59:45,081 - rustbpe - [32m[1mINFO[0m - Progress: [1m33%[0m (21540/65271 merges) - Last merge: (629, 321) -> 21795 (frequency: 2049)
2025-11-10 22:59:45,112 - rustbpe - [32m[1mINFO[0m - Progress: [1m34%[0m (22193/65271 merges) - Last merge: (10820, 363) -> 22448 (frequency: 1945)
2025-11-10 22:59:45,146 - rustbpe - [32m[1mINFO[0m - Progress: [1m35%[0m (22845/65271 merges) - Last merge: (15499, 10886) -> 23100 (frequency: 1850)
2025-11-10 22:59:45,169 - rustbpe - [32m[1mINFO[0m - Progress: [1m36%[0m (23498/65271 merges) - Last merge: (5037, 756) -> 23753 (frequency: 1758)
2025-11-10 22:59:45,205 - rustbpe - [32m[1mINFO[0m - Progress: [1m37%[0m (24151/65271 merges) - Last merge: (1811, 1627) -> 24406 (frequency: 1673)
2025-11-10 22:59:45,232 - rustbpe - [32m[1mINFO[0m - Progress: [1m38%[0m (24803/65271 merges) - Last merge: (10235, 3181) -> 25058 (frequency: 1598)
2025-11-10 22:59:45,265 - rustbpe - [32m[1mINFO[0m - Progress: [1m39%[0m (25456/65271 merges) - Last merge: (16956, 1121) -> 25711 (frequency: 1525)
2025-11-10 22:59:45,296 - rustbpe - [32m[1mINFO[0m - Progress: [1m40%[0m (26109/65271 merges) - Last merge: (2419, 636) -> 26364 (frequency: 1462)
2025-11-10 22:59:45,324 - rustbpe - [32m[1mINFO[0m - Progress: [1m41%[0m (26762/65271 merges) - Last merge: (2549, 1122) -> 27017 (frequency: 1402)
2025-11-10 22:59:45,358 - rustbpe - [32m[1mINFO[0m - Progress: [1m42%[0m (27414/65271 merges) - Last merge: (99, 1091) -> 27669 (frequency: 1343)
2025-11-10 22:59:45,390 - rustbpe - [32m[1mINFO[0m - Progress: [1m43%[0m (28067/65271 merges) - Last merge: (216, 170) -> 28322 (frequency: 1288)
2025-11-10 22:59:45,423 - rustbpe - [32m[1mINFO[0m - Progress: [1m44%[0m (28720/65271 merges) - Last merge: (402, 263) -> 28975 (frequency: 1233)
2025-11-10 22:59:45,452 - rustbpe - [32m[1mINFO[0m - Progress: [1m45%[0m (29372/65271 merges) - Last merge: (19389, 466) -> 29627 (frequency: 1190)
2025-11-10 22:59:45,480 - rustbpe - [32m[1mINFO[0m - Progress: [1m46%[0m (30025/65271 merges) - Last merge: (4600, 76) -> 30280 (frequency: 1145)
2025-11-10 22:59:45,501 - rustbpe - [32m[1mINFO[0m - Progress: [1m47%[0m (30678/65271 merges) - Last merge: (304, 849) -> 30933 (frequency: 1100)
2025-11-10 22:59:45,527 - rustbpe - [32m[1mINFO[0m - Progress: [1m48%[0m (31331/65271 merges) - Last merge: (5090, 596) -> 31586 (frequency: 1063)
2025-11-10 22:59:45,548 - rustbpe - [32m[1mINFO[0m - Progress: [1m49%[0m (31983/65271 merges) - Last merge: (21702, 9305) -> 32238 (frequency: 1026)
2025-11-10 22:59:45,569 - rustbpe - [32m[1mINFO[0m - Progress: [1m50%[0m (32636/65271 merges) - Last merge: (78, 11441) -> 32891 (frequency: 988)
2025-11-10 22:59:45,594 - rustbpe - [32m[1mINFO[0m - Progress: [1m51%[0m (33289/65271 merges) - Last merge: (17782, 373) -> 33544 (frequency: 956)
2025-11-10 22:59:45,618 - rustbpe - [32m[1mINFO[0m - Progress: [1m52%[0m (33941/65271 merges) - Last merge: (6872, 474) -> 34196 (frequency: 925)
2025-11-10 22:59:45,642 - rustbpe - [32m[1mINFO[0m - Progress: [1m53%[0m (34594/65271 merges) - Last merge: (13160, 1816) -> 34849 (frequency: 893)
2025-11-10 22:59:45,667 - rustbpe - [32m[1mINFO[0m - Progress: [1m54%[0m (35247/65271 merges) - Last merge: (336, 1855) -> 35502 (frequency: 865)
2025-11-10 22:59:45,701 - rustbpe - [32m[1mINFO[0m - Progress: [1m55%[0m (35900/65271 merges) - Last merge: (446, 276) -> 36155 (frequency: 838)
2025-11-10 22:59:45,718 - rustbpe - [32m[1mINFO[0m - Progress: [1m56%[0m (36552/65271 merges) - Last merge: (337, 1876) -> 36807 (frequency: 811)
2025-11-10 22:59:45,736 - rustbpe - [32m[1mINFO[0m - Progress: [1m57%[0m (37205/65271 merges) - Last merge: (27812, 319) -> 37460 (frequency: 787)
2025-11-10 22:59:45,754 - rustbpe - [32m[1mINFO[0m - Progress: [1m58%[0m (37858/65271 merges) - Last merge: (2181, 2035) -> 38113 (frequency: 763)
2025-11-10 22:59:45,781 - rustbpe - [32m[1mINFO[0m - Progress: [1m59%[0m (38510/65271 merges) - Last merge: (529, 35912) -> 38765 (frequency: 739)
2025-11-10 22:59:45,799 - rustbpe - [32m[1mINFO[0m - Progress: [1m60%[0m (39163/65271 merges) - Last merge: (1626, 758) -> 39418 (frequency: 717)
2025-11-10 22:59:45,828 - rustbpe - [32m[1mINFO[0m - Progress: [1m61%[0m (39816/65271 merges) - Last merge: (2884, 36357) -> 40071 (frequency: 696)
2025-11-10 22:59:45,844 - rustbpe - [32m[1mINFO[0m - Progress: [1m62%[0m (40469/65271 merges) - Last merge: (22732, 852) -> 40724 (frequency: 676)
2025-11-10 22:59:45,919 - rustbpe - [32m[1mINFO[0m - Progress: [1m63%[0m (41121/65271 merges) - Last merge: (2718, 80) -> 41376 (frequency: 656)
2025-11-10 22:59:45,942 - rustbpe - [32m[1mINFO[0m - Progress: [1m64%[0m (41774/65271 merges) - Last merge: (5190, 424) -> 42029 (frequency: 637)
2025-11-10 22:59:45,964 - rustbpe - [32m[1mINFO[0m - Progress: [1m65%[0m (42427/65271 merges) - Last merge: (1598, 12834) -> 42682 (frequency: 619)
2025-11-10 22:59:45,986 - rustbpe - [32m[1mINFO[0m - Progress: [1m66%[0m (43079/65271 merges) - Last merge: (1929, 2037) -> 43334 (frequency: 602)
2025-11-10 22:59:45,998 - rustbpe - [32m[1mINFO[0m - Progress: [1m67%[0m (43732/65271 merges) - Last merge: (37877, 14879) -> 43987 (frequency: 585)
2025-11-10 22:59:46,018 - rustbpe - [32m[1mINFO[0m - Progress: [1m68%[0m (44385/65271 merges) - Last merge: (29758, 3004) -> 44640 (frequency: 570)
2025-11-10 22:59:46,031 - rustbpe - [32m[1mINFO[0m - Progress: [1m69%[0m (45037/65271 merges) - Last merge: (12204, 1404) -> 45292 (frequency: 555)
2025-11-10 22:59:46,050 - rustbpe - [32m[1mINFO[0m - Progress: [1m70%[0m (45690/65271 merges) - Last merge: (2806, 259) -> 45945 (frequency: 540)
2025-11-10 22:59:46,068 - rustbpe - [32m[1mINFO[0m - Progress: [1m71%[0m (46343/65271 merges) - Last merge: (9018, 34477) -> 46598 (frequency: 527)
2025-11-10 22:59:46,083 - rustbpe - [32m[1mINFO[0m - Progress: [1m72%[0m (46996/65271 merges) - Last merge: (420, 8269) -> 47251 (frequency: 513)
2025-11-10 22:59:46,101 - rustbpe - [32m[1mINFO[0m - Progress: [1m73%[0m (47648/65271 merges) - Last merge: (7466, 282) -> 47903 (frequency: 501)
2025-11-10 22:59:46,113 - rustbpe - [32m[1mINFO[0m - Progress: [1m74%[0m (48301/65271 merges) - Last merge: (438, 4996) -> 48556 (frequency: 489)
2025-11-10 22:59:46,128 - rustbpe - [32m[1mINFO[0m - Progress: [1m75%[0m (48954/65271 merges) - Last merge: (4604, 276) -> 49209 (frequency: 478)
2025-11-10 22:59:46,150 - rustbpe - [32m[1mINFO[0m - Progress: [1m76%[0m (49606/65271 merges) - Last merge: (1632, 13872) -> 49861 (frequency: 466)
2025-11-10 22:59:46,164 - rustbpe - [32m[1mINFO[0m - Progress: [1m77%[0m (50259/65271 merges) - Last merge: (3250, 8184) -> 50514 (frequency: 455)
2025-11-10 22:59:46,178 - rustbpe - [32m[1mINFO[0m - Progress: [1m78%[0m (50912/65271 merges) - Last merge: (560, 3010) -> 51167 (frequency: 445)
2025-11-10 22:59:46,191 - rustbpe - [32m[1mINFO[0m - Progress: [1m79%[0m (51565/65271 merges) - Last merge: (377, 3255) -> 51820 (frequency: 435)
2025-11-10 22:59:46,211 - rustbpe - [32m[1mINFO[0m - Progress: [1m80%[0m (52217/65271 merges) - Last merge: (37215, 1508) -> 52472 (frequency: 426)
2025-11-10 22:59:46,223 - rustbpe - [32m[1mINFO[0m - Progress: [1m81%[0m (52870/65271 merges) - Last merge: (14361, 31789) -> 53125 (frequency: 417)
2025-11-10 22:59:46,235 - rustbpe - [32m[1mINFO[0m - Progress: [1m82%[0m (53523/65271 merges) - Last merge: (18450, 285) -> 53778 (frequency: 408)
2025-11-10 22:59:46,247 - rustbpe - [32m[1mINFO[0m - Progress: [1m83%[0m (54175/65271 merges) - Last merge: (18546, 431) -> 54430 (frequency: 399)
2025-11-10 22:59:46,258 - rustbpe - [32m[1mINFO[0m - Progress: [1m84%[0m (54828/65271 merges) - Last merge: (1191, 105) -> 55083 (frequency: 390)
2025-11-10 22:59:46,272 - rustbpe - [32m[1mINFO[0m - Progress: [1m85%[0m (55481/65271 merges) - Last merge: (377, 112) -> 55736 (frequency: 382)
2025-11-10 22:59:46,286 - rustbpe - [32m[1mINFO[0m - Progress: [1m86%[0m (56134/65271 merges) - Last merge: (1554, 97) -> 56389 (frequency: 374)
2025-11-10 22:59:46,304 - rustbpe - [32m[1mINFO[0m - Progress: [1m87%[0m (56786/65271 merges) - Last merge: (35419, 791) -> 57041 (frequency: 366)
2025-11-10 22:59:46,318 - rustbpe - [32m[1mINFO[0m - Progress: [1m88%[0m (57439/65271 merges) - Last merge: (1149, 97) -> 57694 (frequency: 358)
2025-11-10 22:59:46,329 - rustbpe - [32m[1mINFO[0m - Progress: [1m89%[0m (58092/65271 merges) - Last merge: (3105, 755) -> 58347 (frequency: 351)
2025-11-10 22:59:46,345 - rustbpe - [32m[1mINFO[0m - Progress: [1m90%[0m (58744/65271 merges) - Last merge: (701, 44294) -> 58999 (frequency: 344)
2025-11-10 22:59:46,359 - rustbpe - [32m[1mINFO[0m - Progress: [1m91%[0m (59397/65271 merges) - Last merge: (441, 431) -> 59652 (frequency: 337)
2025-11-10 22:59:46,372 - rustbpe - [32m[1mINFO[0m - Progress: [1m92%[0m (60050/65271 merges) - Last merge: (111, 69) -> 60305 (frequency: 330)
2025-11-10 22:59:46,385 - rustbpe - [32m[1mINFO[0m - Progress: [1m93%[0m (60703/65271 merges) - Last merge: (313, 25817) -> 60958 (frequency: 323)
2025-11-10 22:59:46,396 - rustbpe - [32m[1mINFO[0m - Progress: [1m94%[0m (61355/65271 merges) - Last merge: (5375, 7100) -> 61610 (frequency: 317)
2025-11-10 22:59:46,407 - rustbpe - [32m[1mINFO[0m - Progress: [1m95%[0m (62008/65271 merges) - Last merge: (3460, 122) -> 62263 (frequency: 311)
2025-11-10 22:59:46,419 - rustbpe - [32m[1mINFO[0m - Progress: [1m96%[0m (62661/65271 merges) - Last merge: (4719, 1622) -> 62916 (frequency: 305)
2025-11-10 22:59:46,429 - rustbpe - [32m[1mINFO[0m - Progress: [1m97%[0m (63313/65271 merges) - Last merge: (58949, 46565) -> 63568 (frequency: 300)
2025-11-10 22:59:46,440 - rustbpe - [32m[1mINFO[0m - Progress: [1m98%[0m (63966/65271 merges) - Last merge: (45, 391) -> 64221 (frequency: 293)
2025-11-10 22:59:46,451 - rustbpe - [32m[1mINFO[0m - Progress: [1m99%[0m (64619/65271 merges) - Last merge: (1588, 18644) -> 64874 (frequency: 288)
2025-11-10 22:59:46,461 - rustbpe - [32m[1mINFO[0m - Progress: [1m100%[0m (65271/65271 merges) - Last merge: (1594, 552) -> 65526 (frequency: 283)
2025-11-10 22:59:46,461 - rustbpe - [32m[1mINFO[0m - Finished training: 65271 merges completed
Training time: 37.89s
Saved tokenizer encoding to /home/henny/.cache/nanochat/tokenizer/tokenizer.pkl
Saved token_bytes to /home/henny/.cache/nanochat/tokenizer/token_bytes.pt

Vocab sizes:
GPT-2: 50257
GPT-4: 100277
Ours: 65536

Comparison with GPT-2:
===============================================================================================
Text Type  Bytes    GPT-2           Ours            Relative     Better    
                    Tokens  Ratio   Tokens  Ratio   Diff %      
-----------------------------------------------------------------------------------------------
news       1819     [91m404    [0m [91m4.50   [0m [92m375    [0m [92m4.85   [0m [92m   +7.2%[0m     Ours      
korean     893      [91m745    [0m [91m1.20   [0m [92m721    [0m [92m1.24   [0m [92m   +3.2%[0m     Ours      
code       1259     [91m576    [0m [91m2.19   [0m [92m493    [0m [92m2.55   [0m [92m  +14.4%[0m     Ours      
math       1834     [92m936    [0m [92m1.96   [0m [91m966    [0m [91m1.90   [0m [91m   -3.2%[0m     GPT-2     
science    1112     [91m260    [0m [91m4.28   [0m [92m225    [0m [92m4.94   [0m [92m  +13.5%[0m     Ours      
fwe-train  4208518  [91m900364 [0m [91m4.67   [0m [92m856901 [0m [92m4.91   [0m [92m   +4.8%[0m     Ours      
fwe-val    4908443  [91m1059062[0m [91m4.63   [0m [92m1010356[0m [92m4.86   [0m [92m   +4.6%[0m     Ours      

Comparison with GPT-4:
===============================================================================================
Text Type  Bytes    GPT-4           Ours            Relative     Better    
                    Tokens  Ratio   Tokens  Ratio   Diff %      
-----------------------------------------------------------------------------------------------
news       1819     [91m387    [0m [91m4.70   [0m [92m375    [0m [92m4.85   [0m [92m   +3.1%[0m     Ours      
korean     893      [92m364    [0m [92m2.45   [0m [91m721    [0m [91m1.24   [0m [91m  -98.1%[0m     GPT-4     
code       1259     [92m309    [0m [92m4.07   [0m [91m493    [0m [91m2.55   [0m [91m  -59.5%[0m     GPT-4     
math       1834     [92m832    [0m [92m2.20   [0m [91m966    [0m [91m1.90   [0m [91m  -16.1%[0m     GPT-4     
science    1112     [91m249    [0m [91m4.47   [0m [92m225    [0m [92m4.94   [0m [92m   +9.6%[0m     Ours      
fwe-train  4208518  [91m874799 [0m [91m4.81   [0m [92m856901 [0m [92m4.91   [0m [92m   +2.0%[0m     Ours      
fwe-val    4908443  [91m1029691[0m [91m4.77   [0m [92m1010356[0m [92m4.86   [0m [92m   +1.9%[0m     Ours      
Waiting for dataset download to complete...

                                                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                                                      â–‘â–‘â–ˆâ–ˆâ–ˆ                â–‘â–‘â–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘
     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–‘  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ
     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘
    
Overriding: depth = 20
Overriding: device_batch_size = 4
Overriding: max_seq_len = 1024
Overriding: run = djhenny-nanochat-speedrun-4090
Autodetected device type: cuda
/home/henny/workplace/nanochat/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
2025-11-10 22:59:56,400 - nanochat.common - [32m[1mINFO[0m - Distributed world size: 1
[34m[1mwandb[0m: Currently logged in as: [33mjon-henshaw[0m ([33mdjhenny[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
[34m[1mwandb[0m: [38;5;178mâ¢¿[0m Waiting for wandb.init()...
[Am[2K[34m[1mwandb[0m: [38;5;178mâ£»[0m Waiting for wandb.init()...
[Am[2K[34m[1mwandb[0m: Tracking run with wandb version 0.21.3
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/home/henny/workplace/nanochat/wandb/run-20251110_225956-url355b1[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
[34m[1mwandb[0m: Syncing run [33mdjhenny-nanochat-speedrun-4090[0m
[34m[1mwandb[0m: â­ï¸ View project at [34m[4mhttps://wandb.ai/djhenny/nanochat[0m
[34m[1mwandb[0m: ðŸš€ View run at [34m[4mhttps://wandb.ai/djhenny/nanochat/runs/url355b1[0m
Vocab size: 65,536
num_layers: 20
model_dim: 1280
num_heads: 10
num_kv_heads: 10
Tokens / micro-batch / rank: 4 x 1024 = 4,096
Tokens / micro-batch: 4,096
Total batch size 524,288 => gradient accumulation steps: 128
Number of parameters: 560,988,160
Estimated FLOPs per token: 3.177185e+09
Calculated number of iterations from target data:param ratio: 21,400
Total number of training tokens: 11,219,763,200
Tokens : Params ratio: 20.00
Total training FLOPs estimate: 3.564727e+19
Scaling the LR for the AdamW parameters âˆ1/âˆš(1280/768) = 0.774597
Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32
Step 00000 | Validation bpb: 3.3015
step 00000/21400 (0.00%) | loss: 11.090355 | grad norm: 0.4282 | lrm: 1.00 | dt: 46222.92ms | tok/sec: 11,342 | mfu: 3.64 | total time: 0.00m
step 00001/21400 (0.00%) | loss: 10.809341 | grad norm: 11.0336 | lrm: 1.00 | dt: 13651.72ms | tok/sec: 38,404 | mfu: 12.34 | total time: 0.00m
step 00002/21400 (0.01%) | loss: 10.280090 | grad norm: 5.7968 | lrm: 1.00 | dt: 13750.12ms | tok/sec: 38,129 | mfu: 12.25 | total time: 0.00m
step 00003/21400 (0.01%) | loss: 9.546899 | grad norm: 4.3585 | lrm: 1.00 | dt: 13822.51ms | tok/sec: 37,930 | mfu: 12.19 | total time: 0.00m
step 00004/21400 (0.02%) | loss: 8.984335 | grad norm: 7.0214 | lrm: 1.00 | dt: 13779.48ms | tok/sec: 38,048 | mfu: 12.22 | total time: 0.00m
step 00005/21400 (0.02%) | loss: 8.630783 | grad norm: 5.4103 | lrm: 1.00 | dt: 13832.70ms | tok/sec: 37,902 | mfu: 12.18 | total time: 0.00m
^[[A^[[A^[[Astep 00006/21400 (0.03%) | loss: 8.355317 | grad norm: 4.3408 | lrm: 1.00 | dt: 13798.32ms | tok/sec: 37,996 | mfu: 12.21 | total time: 0.00m
step 00007/21400 (0.03%) | loss: 8.064442 | grad norm: 4.6350 | lrm: 1.00 | dt: 13841.86ms | tok/sec: 37,876 | mfu: 12.17 | total time: 0.00m
step 00008/21400 (0.04%) | loss: 7.850240 | grad norm: 4.9607 | lrm: 1.00 | dt: 13831.02ms | tok/sec: 37,906 | mfu: 12.18 | total time: 0.00m
step 00009/21400 (0.04%) | loss: 7.627156 | grad norm: 2.1864 | lrm: 1.00 | dt: 13864.44ms | tok/sec: 37,815 | mfu: 12.15 | total time: 0.00m
step 00010/21400 (0.05%) | loss: 7.481029 | grad norm: 2.7468 | lrm: 1.00 | dt: 13844.53ms | tok/sec: 37,869 | mfu: 12.17 | total time: 0.00m
step 00011/21400 (0.05%) | loss: 7.332423 | grad norm: 1.5518 | lrm: 1.00 | dt: 14086.52ms | tok/sec: 37,219 | mfu: 11.96 | total time: 0.23m
step 00012/21400 (0.06%) | loss: 7.282009 | grad norm: 2.7430 | lrm: 1.00 | dt: 14296.24ms | tok/sec: 36,673 | mfu: 11.78 | total time: 0.47m
step 00013/21400 (0.06%) | loss: 7.197951 | grad norm: 1.5703 | lrm: 1.00 | dt: 13876.91ms | tok/sec: 37,781 | mfu: 12.14 | total time: 0.70m
step 00014/21400 (0.07%) | loss: 7.131426 | grad norm: 1.7138 | lrm: 1.00 | dt: 13868.62ms | tok/sec: 37,803 | mfu: 12.14 | total time: 0.94m
step 00015/21400 (0.07%) | loss: 7.049208 | grad norm: 1.8439 | lrm: 1.00 | dt: 13879.62ms | tok/sec: 37,773 | mfu: 12.13 | total time: 1.17m
step 00016/21400 (0.07%) | loss: 6.972758 | grad norm: 2.7638 | lrm: 1.00 | dt: 13899.56ms | tok/sec: 37,719 | mfu: 12.12 | total time: 1.40m
step 00017/21400 (0.08%) | loss: 6.877913 | grad norm: 1.9343 | lrm: 1.00 | dt: 13884.63ms | tok/sec: 37,760 | mfu: 12.13 | total time: 1.63m
step 00018/21400 (0.08%) | loss: 6.844389 | grad norm: 1.2897 | lrm: 1.00 | dt: 13903.97ms | tok/sec: 37,707 | mfu: 12.11 | total time: 1.86m
step 00019/21400 (0.09%) | loss: 6.813870 | grad norm: 1.0137 | lrm: 1.00 | dt: 13862.51ms | tok/sec: 37,820 | mfu: 12.15 | total time: 2.09m
step 00020/21400 (0.09%) | loss: 6.792552 | grad norm: 2.3735 | lrm: 1.00 | dt: 13899.14ms | tok/sec: 37,720 | mfu: 12.12 | total time: 2.32m
step 00021/21400 (0.10%) | loss: 6.735534 | grad norm: 1.2934 | lrm: 1.00 | dt: 13864.85ms | tok/sec: 37,814 | mfu: 12.15 | total time: 2.56m
step 00022/21400 (0.10%) | loss: 6.643918 | grad norm: 0.6786 | lrm: 1.00 | dt: 13914.86ms | tok/sec: 37,678 | mfu: 12.10 | total time: 2.79m
step 00023/21400 (0.11%) | loss: 6.607830 | grad norm: 1.1614 | lrm: 1.00 | dt: 13884.63ms | tok/sec: 37,760 | mfu: 12.13 | total time: 3.02m
step 00024/21400 (0.11%) | loss: 6.595737 | grad norm: 1.0471 | lrm: 1.00 | dt: 13916.65ms | tok/sec: 37,673 | mfu: 12.10 | total time: 3.25m
step 00025/21400 (0.12%) | loss: 6.548690 | grad norm: 1.1386 | lrm: 1.00 | dt: 13902.86ms | tok/sec: 37,710 | mfu: 12.11 | total time: 3.48m
step 00026/21400 (0.12%) | loss: 6.521559 | grad norm: 1.3380 | lrm: 1.00 | dt: 13902.15ms | tok/sec: 37,712 | mfu: 12.12 | total time: 3.71m
step 00027/21400 (0.13%) | loss: 6.484196 | grad norm: 1.5310 | lrm: 1.00 | dt: 13905.06ms | tok/sec: 37,704 | mfu: 12.11 | total time: 3.95m
step 00028/21400 (0.13%) | loss: 6.426401 | grad norm: 1.0640 | lrm: 1.00 | dt: 13907.55ms | tok/sec: 37,698 | mfu: 12.11 | total time: 4.18m
step 00029/21400 (0.14%) | loss: 6.386684 | grad norm: 1.1734 | lrm: 1.00 | dt: 13917.55ms | tok/sec: 37,671 | mfu: 12.10 | total time: 4.41m
step 00030/21400 (0.14%) | loss: 6.405534 | grad norm: 0.8538 | lrm: 1.00 | dt: 13907.97ms | tok/sec: 37,696 | mfu: 12.11 | total time: 4.64m
step 00031/21400 (0.14%) | loss: 6.383107 | grad norm: 0.7864 | lrm: 1.00 | dt: 13944.48ms | tok/sec: 37,598 | mfu: 12.08 | total time: 4.87m
step 00032/21400 (0.15%) | loss: 6.345270 | grad norm: 0.7692 | lrm: 1.00 | dt: 14015.90ms | tok/sec: 37,406 | mfu: 12.02 | total time: 5.11m
step 00033/21400 (0.15%) | loss: 6.295992 | grad norm: 0.9264 | lrm: 1.00 | dt: 13927.16ms | tok/sec: 37,645 | mfu: 12.09 | total time: 5.34m
step 00034/21400 (0.16%) | loss: 6.247307 | grad norm: 1.1083 | lrm: 1.00 | dt: 13883.21ms | tok/sec: 37,764 | mfu: 12.13 | total time: 5.57m
step 00035/21400 (0.16%) | loss: 6.223995 | grad norm: 1.2173 | lrm: 1.00 | dt: 13915.77ms | tok/sec: 37,675 | mfu: 12.10 | total time: 5.80m
step 00036/21400 (0.17%) | loss: 6.171683 | grad norm: 1.0457 | lrm: 1.00 | dt: 13880.05ms | tok/sec: 37,772 | mfu: 12.13 | total time: 6.03m
step 00037/21400 (0.17%) | loss: 6.124653 | grad norm: 1.4523 | lrm: 1.00 | dt: 13923.91ms | tok/sec: 37,653 | mfu: 12.10 | total time: 6.27m
step 00038/21400 (0.18%) | loss: 6.118306 | grad norm: 0.7822 | lrm: 1.00 | dt: 13906.19ms | tok/sec: 37,701 | mfu: 12.11 | total time: 6.50m
step 00039/21400 (0.18%) | loss: 6.060619 | grad norm: 0.9275 | lrm: 1.00 | dt: 13927.64ms | tok/sec: 37,643 | mfu: 12.09 | total time: 6.73m
step 00040/21400 (0.19%) | loss: 6.046127 | grad norm: 0.9010 | lrm: 1.00 | dt: 13888.51ms | tok/sec: 37,749 | mfu: 12.13 | total time: 6.96m
step 00041/21400 (0.19%) | loss: 6.016890 | grad norm: 1.0967 | lrm: 1.00 | dt: 13920.60ms | tok/sec: 37,662 | mfu: 12.10 | total time: 7.19m
step 00042/21400 (0.20%) | loss: 6.008477 | grad norm: 0.7787 | lrm: 1.00 | dt: 13911.08ms | tok/sec: 37,688 | mfu: 12.11 | total time: 7.43m
step 00043/21400 (0.20%) | loss: 5.991795 | grad norm: 0.6177 | lrm: 1.00 | dt: 13905.46ms | tok/sec: 37,703 | mfu: 12.11 | total time: 7.66m
step 00044/21400 (0.21%) | loss: 5.992837 | grad norm: 0.6998 | lrm: 1.00 | dt: 13930.82ms | tok/sec: 37,635 | mfu: 12.09 | total time: 7.89m
step 00045/21400 (0.21%) | loss: 5.987331 | grad norm: 0.9368 | lrm: 1.00 | dt: 13896.13ms | tok/sec: 37,729 | mfu: 12.12 | total time: 8.12m
step 00046/21400 (0.21%) | loss: 5.973700 | grad norm: 0.7853 | lrm: 1.00 | dt: 13937.63ms | tok/sec: 37,616 | mfu: 12.08 | total time: 8.35m
step 00047/21400 (0.22%) | loss: 5.979125 | grad norm: 0.6236 | lrm: 1.00 | dt: 13888.21ms | tok/sec: 37,750 | mfu: 12.13 | total time: 8.58m
step 00048/21400 (0.22%) | loss: 5.984094 | grad norm: 0.6901 | lrm: 1.00 | dt: 13938.27ms | tok/sec: 37,614 | mfu: 12.08 | total time: 8.82m
step 00049/21400 (0.23%) | loss: 5.975103 | grad norm: 0.7670 | lrm: 1.00 | dt: 13880.80ms | tok/sec: 37,770 | mfu: 12.13 | total time: 9.05m
step 00050/21400 (0.23%) | loss: 5.952630 | grad norm: 0.7424 | lrm: 1.00 | dt: 13923.50ms | tok/sec: 37,654 | mfu: 12.10 | total time: 9.28m
step 00051/21400 (0.24%) | loss: 5.942785 | grad norm: 0.9140 | lrm: 1.00 | dt: 13881.20ms | tok/sec: 37,769 | mfu: 12.13 | total time: 9.51m
step 00052/21400 (0.24%) | loss: 5.900841 | grad norm: 0.8693 | lrm: 1.00 | dt: 14084.49ms | tok/sec: 37,224 | mfu: 11.96 | total time: 9.75m
step 00053/21400 (0.25%) | loss: 5.893991 | grad norm: 0.7844 | lrm: 1.00 | dt: 13902.53ms | tok/sec: 37,711 | mfu: 12.11 | total time: 9.98m
step 00054/21400 (0.25%) | loss: 5.863275 | grad norm: 0.7705 | lrm: 1.00 | dt: 13929.76ms | tok/sec: 37,637 | mfu: 12.09 | total time: 10.21m
step 00055/21400 (0.26%) | loss: 5.846888 | grad norm: 1.0157 | lrm: 1.00 | dt: 13908.59ms | tok/sec: 37,695 | mfu: 12.11 | total time: 10.44m
step 00056/21400 (0.26%) | loss: 5.770640 | grad norm: 0.8112 | lrm: 1.00 | dt: 13908.75ms | tok/sec: 37,694 | mfu: 12.11 | total time: 10.67m
step 00057/21400 (0.27%) | loss: 5.755816 | grad norm: 0.7269 | lrm: 1.00 | dt: 13910.39ms | tok/sec: 37,690 | mfu: 12.11 | total time: 10.91m
step 00058/21400 (0.27%) | loss: 5.765514 | grad norm: 0.7844 | lrm: 1.00 | dt: 14041.38ms | tok/sec: 37,338 | mfu: 12.00 | total time: 11.14m
step 00059/21400 (0.28%) | loss: 5.751389 | grad norm: 0.7395 | lrm: 1.00 | dt: 13895.64ms | tok/sec: 37,730 | mfu: 12.12 | total time: 11.37m
step 00060/21400 (0.28%) | loss: 5.738083 | grad norm: 0.5928 | lrm: 1.00 | dt: 13852.13ms | tok/sec: 37,848 | mfu: 12.16 | total time: 11.60m
step 00061/21400 (0.29%) | loss: 5.730036 | grad norm: 0.5968 | lrm: 1.00 | dt: 13894.19ms | tok/sec: 37,734 | mfu: 12.12 | total time: 11.83m
step 00062/21400 (0.29%) | loss: 5.696683 | grad norm: 0.7109 | lrm: 1.00 | dt: 13849.48ms | tok/sec: 37,856 | mfu: 12.16 | total time: 12.06m
step 00063/21400 (0.29%) | loss: 5.668236 | grad norm: 0.6502 | lrm: 1.00 | dt: 13900.70ms | tok/sec: 37,716 | mfu: 12.12 | total time: 12.30m
step 00064/21400 (0.30%) | loss: 5.654005 | grad norm: 0.7109 | lrm: 1.00 | dt: 13851.08ms | tok/sec: 37,851 | mfu: 12.16 | total time: 12.53m
step 00065/21400 (0.30%) | loss: 5.622186 | grad norm: 0.6783 | lrm: 1.00 | dt: 13899.16ms | tok/sec: 37,720 | mfu: 12.12 | total time: 12.76m
step 00066/21400 (0.31%) | loss: 5.671345 | grad norm: 0.6973 | lrm: 1.00 | dt: 13863.54ms | tok/sec: 37,817 | mfu: 12.15 | total time: 12.99m
step 00067/21400 (0.31%) | loss: 5.660718 | grad norm: 0.7588 | lrm: 1.00 | dt: 13890.60ms | tok/sec: 37,744 | mfu: 12.13 | total time: 13.22m
step 00068/21400 (0.32%) | loss: 5.638690 | grad norm: 0.7444 | lrm: 1.00 | dt: 13877.36ms | tok/sec: 37,780 | mfu: 12.14 | total time: 13.45m
step 00069/21400 (0.32%) | loss: 5.615518 | grad norm: 0.6806 | lrm: 1.00 | dt: 13875.35ms | tok/sec: 37,785 | mfu: 12.14 | total time: 13.68m
step 00070/21400 (0.33%) | loss: 5.548139 | grad norm: 0.6378 | lrm: 1.00 | dt: 13884.62ms | tok/sec: 37,760 | mfu: 12.13 | total time: 13.92m
step 00071/21400 (0.33%) | loss: 5.532087 | grad norm: 0.6838 | lrm: 1.00 | dt: 13867.92ms | tok/sec: 37,805 | mfu: 12.15 | total time: 14.15m
step 00072/21400 (0.34%) | loss: 5.554338 | grad norm: 0.8110 | lrm: 1.00 | dt: 14054.86ms | tok/sec: 37,302 | mfu: 11.98 | total time: 14.38m
step 00073/21400 (0.34%) | loss: 5.529721 | grad norm: 0.8175 | lrm: 1.00 | dt: 13845.78ms | tok/sec: 37,866 | mfu: 12.16 | total time: 14.61m
step 00074/21400 (0.35%) | loss: 5.505290 | grad norm: 0.5617 | lrm: 1.00 | dt: 13910.76ms | tok/sec: 37,689 | mfu: 12.11 | total time: 14.84m
step 00075/21400 (0.35%) | loss: 5.497499 | grad norm: 0.6695 | lrm: 1.00 | dt: 13844.17ms | tok/sec: 37,870 | mfu: 12.17 | total time: 15.07m
step 00076/21400 (0.36%) | loss: 5.451693 | grad norm: 0.7488 | lrm: 1.00 | dt: 13903.38ms | tok/sec: 37,709 | mfu: 12.11 | total time: 15.31m
step 00077/21400 (0.36%) | loss: 5.521119 | grad norm: 0.6641 | lrm: 1.00 | dt: 13852.24ms | tok/sec: 37,848 | mfu: 12.16 | total time: 15.54m
step 00078/21400 (0.36%) | loss: 5.537152 | grad norm: 0.6219 | lrm: 1.00 | dt: 13899.38ms | tok/sec: 37,720 | mfu: 12.12 | total time: 15.77m
step 00079/21400 (0.37%) | loss: 5.474614 | grad norm: 0.6536 | lrm: 1.00 | dt: 13859.05ms | tok/sec: 37,830 | mfu: 12.15 | total time: 16.00m
step 00080/21400 (0.37%) | loss: 5.502131 | grad norm: 0.7144 | lrm: 1.00 | dt: 13915.65ms | tok/sec: 37,676 | mfu: 12.10 | total time: 16.23m
step 00081/21400 (0.38%) | loss: 5.457355 | grad norm: 0.7372 | lrm: 1.00 | dt: 13861.06ms | tok/sec: 37,824 | mfu: 12.15 | total time: 16.46m
step 00082/21400 (0.38%) | loss: 5.427270 | grad norm: 0.7979 | lrm: 1.00 | dt: 13880.07ms | tok/sec: 37,772 | mfu: 12.13 | total time: 16.69m
step 00083/21400 (0.39%) | loss: 5.366001 | grad norm: 0.8056 | lrm: 1.00 | dt: 13874.07ms | tok/sec: 37,789 | mfu: 12.14 | total time: 16.92m
step 00084/21400 (0.39%) | loss: 5.325730 | grad norm: 0.7785 | lrm: 1.00 | dt: 13923.41ms | tok/sec: 37,655 | mfu: 12.10 | total time: 17.16m
step 00085/21400 (0.40%) | loss: 5.297982 | grad norm: 0.7194 | lrm: 1.00 | dt: 13880.79ms | tok/sec: 37,770 | mfu: 12.13 | total time: 17.39m
step 00086/21400 (0.40%) | loss: 5.281479 | grad norm: 0.6631 | lrm: 1.00 | dt: 13862.84ms | tok/sec: 37,819 | mfu: 12.15 | total time: 17.62m
step 00087/21400 (0.41%) | loss: 5.265992 | grad norm: 0.7092 | lrm: 1.00 | dt: 13893.15ms | tok/sec: 37,737 | mfu: 12.12 | total time: 17.85m
step 00088/21400 (0.41%) | loss: 5.235992 | grad norm: 0.7726 | lrm: 1.00 | dt: 13836.61ms | tok/sec: 37,891 | mfu: 12.17 | total time: 18.08m
step 00089/21400 (0.42%) | loss: 5.248057 | grad norm: 0.7710 | lrm: 1.00 | dt: 13894.79ms | tok/sec: 37,732 | mfu: 12.12 | total time: 18.31m
step 00090/21400 (0.42%) | loss: 5.231326 | grad norm: 0.6439 | lrm: 1.00 | dt: 13843.16ms | tok/sec: 37,873 | mfu: 12.17 | total time: 18.54m
step 00091/21400 (0.43%) | loss: 5.262545 | grad norm: 0.6417 | lrm: 1.00 | dt: 13899.62ms | tok/sec: 37,719 | mfu: 12.12 | total time: 18.78m
step 00092/21400 (0.43%) | loss: 5.280684 | grad norm: 0.6863 | lrm: 1.00 | dt: 13851.44ms | tok/sec: 37,850 | mfu: 12.16 | total time: 19.01m
step 00093/21400 (0.43%) | loss: 5.262710 | grad norm: 0.6454 | lrm: 1.00 | dt: 14026.16ms | tok/sec: 37,379 | mfu: 12.01 | total time: 19.24m
step 00094/21400 (0.44%) | loss: 5.216778 | grad norm: 0.6355 | lrm: 1.00 | dt: 13861.02ms | tok/sec: 37,824 | mfu: 12.15 | total time: 19.47m
step 00095/21400 (0.44%) | loss: 5.215335 | grad norm: 0.6613 | lrm: 1.00 | dt: 13914.01ms | tok/sec: 37,680 | mfu: 12.10 | total time: 19.70m
step 00096/21400 (0.45%) | loss: 5.181420 | grad norm: 0.6841 | lrm: 1.00 | dt: 13867.95ms | tok/sec: 37,805 | mfu: 12.15 | total time: 19.93m
step 00097/21400 (0.45%) | loss: 5.122320 | grad norm: 0.5755 | lrm: 1.00 | dt: 13865.26ms | tok/sec: 37,813 | mfu: 12.15 | total time: 20.17m
step 00098/21400 (0.46%) | loss: 5.134050 | grad norm: 0.5858 | lrm: 1.00 | dt: 13877.67ms | tok/sec: 37,779 | mfu: 12.14 | total time: 20.40m
step 00099/21400 (0.46%) | loss: 5.136521 | grad norm: 0.7146 | lrm: 1.00 | dt: 13881.85ms | tok/sec: 37,767 | mfu: 12.13 | total time: 20.63m
step 00100/21400 (0.47%) | loss: 5.152749 | grad norm: 0.6577 | lrm: 1.00 | dt: 13904.98ms | tok/sec: 37,705 | mfu: 12.11 | total time: 20.86m
step 00101/21400 (0.47%) | loss: 5.135966 | grad norm: 0.6972 | lrm: 1.00 | dt: 13866.37ms | tok/sec: 37,810 | mfu: 12.15 | total time: 21.09m
step 00102/21400 (0.48%) | loss: 5.094516 | grad norm: 0.6618 | lrm: 1.00 | dt: 13896.52ms | tok/sec: 37,728 | mfu: 12.12 | total time: 21.32m
step 00103/21400 (0.48%) | loss: 5.057800 | grad norm: 0.6547 | lrm: 1.00 | dt: 13836.21ms | tok/sec: 37,892 | mfu: 12.17 | total time: 21.55m
step 00104/21400 (0.49%) | loss: 5.024664 | grad norm: 0.6342 | lrm: 1.00 | dt: 13900.90ms | tok/sec: 37,716 | mfu: 12.12 | total time: 21.78m
step 00105/21400 (0.49%) | loss: 5.029204 | grad norm: 0.6141 | lrm: 1.00 | dt: 13858.59ms | tok/sec: 37,831 | mfu: 12.15 | total time: 22.02m
step 00106/21400 (0.50%) | loss: 5.014625 | grad norm: 0.5753 | lrm: 1.00 | dt: 13889.03ms | tok/sec: 37,748 | mfu: 12.13 | total time: 22.25m
step 00107/21400 (0.50%) | loss: 4.963495 | grad norm: 0.6944 | lrm: 1.00 | dt: 13859.62ms | tok/sec: 37,828 | mfu: 12.15 | total time: 22.48m
step 00108/21400 (0.50%) | loss: 4.965066 | grad norm: 0.7014 | lrm: 1.00 | dt: 13880.82ms | tok/sec: 37,770 | mfu: 12.13 | total time: 22.71m
step 00109/21400 (0.51%) | loss: 4.955968 | grad norm: 0.5474 | lrm: 1.00 | dt: 13866.74ms | tok/sec: 37,809 | mfu: 12.15 | total time: 22.94m
step 00110/21400 (0.51%) | loss: 4.962066 | grad norm: 0.5589 | lrm: 1.00 | dt: 13873.21ms | tok/sec: 37,791 | mfu: 12.14 | total time: 23.17m
step 00111/21400 (0.52%) | loss: 5.016094 | grad norm: 0.6006 | lrm: 1.00 | dt: 13880.67ms | tok/sec: 37,771 | mfu: 12.13 | total time: 23.40m
step 00112/21400 (0.52%) | loss: 4.986665 | grad norm: 0.6342 | lrm: 1.00 | dt: 13867.08ms | tok/sec: 37,808 | mfu: 12.15 | total time: 23.63m
step 00113/21400 (0.53%) | loss: 4.964591 | grad norm: 0.6090 | lrm: 1.00 | dt: 14052.41ms | tok/sec: 37,309 | mfu: 11.99 | total time: 23.87m
step 00114/21400 (0.53%) | loss: 5.001001 | grad norm: 0.5839 | lrm: 1.00 | dt: 13841.92ms | tok/sec: 37,876 | mfu: 12.17 | total time: 24.10m
step 00115/21400 (0.54%) | loss: 5.006043 | grad norm: 0.6537 | lrm: 1.00 | dt: 13899.69ms | tok/sec: 37,719 | mfu: 12.12 | total time: 24.33m
step 00116/21400 (0.54%) | loss: 4.976300 | grad norm: 0.6576 | lrm: 1.00 | dt: 13843.06ms | tok/sec: 37,873 | mfu: 12.17 | total time: 24.56m
step 00117/21400 (0.55%) | loss: 4.962663 | grad norm: 0.5865 | lrm: 1.00 | dt: 13899.44ms | tok/sec: 37,720 | mfu: 12.12 | total time: 24.79m
step 00118/21400 (0.55%) | loss: 4.933047 | grad norm: 0.6714 | lrm: 1.00 | dt: 13845.90ms | tok/sec: 37,865 | mfu: 12.16 | total time: 25.02m
step 00119/21400 (0.56%) | loss: 4.912970 | grad norm: 0.5887 | lrm: 1.00 | dt: 13900.71ms | tok/sec: 37,716 | mfu: 12.12 | total time: 25.26m
step 00120/21400 (0.56%) | loss: 4.874860 | grad norm: 0.5631 | lrm: 1.00 | dt: 13863.53ms | tok/sec: 37,817 | mfu: 12.15 | total time: 25.49m
step 00121/21400 (0.57%) | loss: 4.862859 | grad norm: 0.5491 | lrm: 1.00 | dt: 13891.52ms | tok/sec: 37,741 | mfu: 12.12 | total time: 25.72m
step 00122/21400 (0.57%) | loss: 4.845309 | grad norm: 0.5459 | lrm: 1.00 | dt: 13851.17ms | tok/sec: 37,851 | mfu: 12.16 | total time: 25.95m
step 00123/21400 (0.57%) | loss: 4.856935 | grad norm: 0.5768 | lrm: 1.00 | dt: 13877.14ms | tok/sec: 37,780 | mfu: 12.14 | total time: 26.18m
step 00124/21400 (0.58%) | loss: 4.837781 | grad norm: 0.6237 | lrm: 1.00 | dt: 13872.16ms | tok/sec: 37,794 | mfu: 12.14 | total time: 26.41m
step 00125/21400 (0.58%) | loss: 4.829713 | grad norm: 0.5753 | lrm: 1.00 | dt: 13864.30ms | tok/sec: 37,815 | mfu: 12.15 | total time: 26.64m
step 00126/21400 (0.59%) | loss: 4.784398 | grad norm: 0.5047 | lrm: 1.00 | dt: 13913.72ms | tok/sec: 37,681 | mfu: 12.11 | total time: 26.87m
step 00127/21400 (0.59%) | loss: 4.771444 | grad norm: 0.4511 | lrm: 1.00 | dt: 13828.55ms | tok/sec: 37,913 | mfu: 12.18 | total time: 27.11m
step 00128/21400 (0.60%) | loss: 4.771406 | grad norm: 0.4672 | lrm: 1.00 | dt: 13913.52ms | tok/sec: 37,681 | mfu: 12.11 | total time: 27.34m
step 00129/21400 (0.60%) | loss: 4.752294 | grad norm: 0.5787 | lrm: 1.00 | dt: 13839.15ms | tok/sec: 37,884 | mfu: 12.17 | total time: 27.57m
step 00130/21400 (0.61%) | loss: 4.789721 | grad norm: 0.6071 | lrm: 1.00 | dt: 13898.01ms | tok/sec: 37,723 | mfu: 12.12 | total time: 27.80m
step 00131/21400 (0.61%) | loss: 4.782291 | grad norm: 0.4972 | lrm: 1.00 | dt: 13846.70ms | tok/sec: 37,863 | mfu: 12.16 | total time: 28.03m
step 00132/21400 (0.62%) | loss: 4.776093 | grad norm: 0.4853 | lrm: 1.00 | dt: 13893.37ms | tok/sec: 37,736 | mfu: 12.12 | total time: 28.26m
step 00133/21400 (0.62%) | loss: 4.788817 | grad norm: 0.5353 | lrm: 1.00 | dt: 13856.23ms | tok/sec: 37,837 | mfu: 12.16 | total time: 28.49m
step 00134/21400 (0.63%) | loss: 4.778665 | grad norm: 0.4902 | lrm: 1.00 | dt: 14038.28ms | tok/sec: 37,347 | mfu: 12.00 | total time: 28.73m
step 00135/21400 (0.63%) | loss: 4.727465 | grad norm: 0.4514 | lrm: 1.00 | dt: 13850.13ms | tok/sec: 37,854 | mfu: 12.16 | total time: 28.96m
step 00136/21400 (0.64%) | loss: 4.737472 | grad norm: 0.4281 | lrm: 1.00 | dt: 13872.15ms | tok/sec: 37,794 | mfu: 12.14 | total time: 29.19m
step 00137/21400 (0.64%) | loss: 4.707101 | grad norm: 0.4582 | lrm: 1.00 | dt: 13875.00ms | tok/sec: 37,786 | mfu: 12.14 | total time: 29.42m
step 00138/21400 (0.64%) | loss: 4.684236 | grad norm: 0.4989 | lrm: 1.00 | dt: 13867.47ms | tok/sec: 37,807 | mfu: 12.15 | total time: 29.65m
step 00139/21400 (0.65%) | loss: 4.665245 | grad norm: 0.4697 | lrm: 1.00 | dt: 13875.48ms | tok/sec: 37,785 | mfu: 12.14 | total time: 29.88m
step 00140/21400 (0.65%) | loss: 4.620807 | grad norm: 0.4365 | lrm: 1.00 | dt: 13859.73ms | tok/sec: 37,828 | mfu: 12.15 | total time: 30.11m
step 00141/21400 (0.66%) | loss: 4.684780 | grad norm: 0.4676 | lrm: 1.00 | dt: 13896.60ms | tok/sec: 37,727 | mfu: 12.12 | total time: 30.34m
step 00142/21400 (0.66%) | loss: 4.677379 | grad norm: 0.4865 | lrm: 1.00 | dt: 13834.15ms | tok/sec: 37,898 | mfu: 12.17 | total time: 30.58m
step 00143/21400 (0.67%) | loss: 4.673185 | grad norm: 0.4243 | lrm: 1.00 | dt: 13893.88ms | tok/sec: 37,735 | mfu: 12.12 | total time: 30.81m
step 00144/21400 (0.67%) | loss: 4.662874 | grad norm: 0.4008 | lrm: 1.00 | dt: 13831.19ms | tok/sec: 37,906 | mfu: 12.18 | total time: 31.04m
step 00145/21400 (0.68%) | loss: 4.675017 | grad norm: 0.4554 | lrm: 1.00 | dt: 13885.75ms | tok/sec: 37,757 | mfu: 12.13 | total time: 31.27m
step 00146/21400 (0.68%) | loss: 4.667940 | grad norm: 0.4502 | lrm: 1.00 | dt: 13845.27ms | tok/sec: 37,867 | mfu: 12.17 | total time: 31.50m
step 00147/21400 (0.69%) | loss: 4.632030 | grad norm: 0.4251 | lrm: 1.00 | dt: 13885.62ms | tok/sec: 37,757 | mfu: 12.13 | total time: 31.73m
step 00148/21400 (0.69%) | loss: 4.600521 | grad norm: 0.5019 | lrm: 1.00 | dt: 13852.76ms | tok/sec: 37,847 | mfu: 12.16 | total time: 31.96m
step 00149/21400 (0.70%) | loss: 4.585337 | grad norm: 0.5034 | lrm: 1.00 | dt: 13877.96ms | tok/sec: 37,778 | mfu: 12.14 | total time: 32.19m
step 00150/21400 (0.70%) | loss: 4.620181 | grad norm: 0.5134 | lrm: 1.00 | dt: 13856.92ms | tok/sec: 37,835 | mfu: 12.15 | total time: 32.42m
step 00151/21400 (0.71%) | loss: 4.611523 | grad norm: 0.5095 | lrm: 1.00 | dt: 13877.91ms | tok/sec: 37,778 | mfu: 12.14 | total time: 32.66m
step 00152/21400 (0.71%) | loss: 4.593729 | grad norm: 0.4886 | lrm: 1.00 | dt: 13891.30ms | tok/sec: 37,742 | mfu: 12.12 | total time: 32.89m
step 00153/21400 (0.71%) | loss: 4.561794 | grad norm: 0.4843 | lrm: 1.00 | dt: 13850.20ms | tok/sec: 37,854 | mfu: 12.16 | total time: 33.12m
step 00154/21400 (0.72%) | loss: 4.547768 | grad norm: 0.5050 | lrm: 1.00 | dt: 13899.98ms | tok/sec: 37,718 | mfu: 12.12 | total time: 33.35m
step 00155/21400 (0.72%) | loss: 4.543779 | grad norm: 0.4841 | lrm: 1.00 | dt: 13841.93ms | tok/sec: 37,876 | mfu: 12.17 | total time: 33.58m
step 00156/21400 (0.73%) | loss: 4.522011 | grad norm: 0.4717 | lrm: 1.00 | dt: 14040.28ms | tok/sec: 37,341 | mfu: 12.00 | total time: 33.81m
step 00157/21400 (0.73%) | loss: 4.503802 | grad norm: 0.4145 | lrm: 1.00 | dt: 13833.42ms | tok/sec: 37,900 | mfu: 12.18 | total time: 34.04m
step 00158/21400 (0.74%) | loss: 4.512888 | grad norm: 0.3918 | lrm: 1.00 | dt: 13902.47ms | tok/sec: 37,711 | mfu: 12.12 | total time: 34.28m
step 00159/21400 (0.74%) | loss: 4.505799 | grad norm: 0.4187 | lrm: 1.00 | dt: 13880.02ms | tok/sec: 37,772 | mfu: 12.13 | total time: 34.51m
step 00160/21400 (0.75%) | loss: 4.494213 | grad norm: 0.4559 | lrm: 1.00 | dt: 13840.64ms | tok/sec: 37,880 | mfu: 12.17 | total time: 34.74m
step 00161/21400 (0.75%) | loss: 4.520669 | grad norm: 0.4525 | lrm: 1.00 | dt: 13838.58ms | tok/sec: 37,885 | mfu: 12.17 | total time: 34.97m
step 00162/21400 (0.76%) | loss: 4.511493 | grad norm: 0.4761 | lrm: 1.00 | dt: 13866.06ms | tok/sec: 37,810 | mfu: 12.15 | total time: 35.20m
step 00163/21400 (0.76%) | loss: 4.524997 | grad norm: 0.4733 | lrm: 1.00 | dt: 13859.08ms | tok/sec: 37,829 | mfu: 12.15 | total time: 35.43m
step 00164/21400 (0.77%) | loss: 4.520543 | grad norm: 0.4089 | lrm: 1.00 | dt: 13873.32ms | tok/sec: 37,791 | mfu: 12.14 | total time: 35.66m
step 00165/21400 (0.77%) | loss: 4.509051 | grad norm: 0.4309 | lrm: 1.00 | dt: 13844.16ms | tok/sec: 37,870 | mfu: 12.17 | total time: 35.89m
step 00166/21400 (0.78%) | loss: 4.459073 | grad norm: 0.4435 | lrm: 1.00 | dt: 13851.06ms | tok/sec: 37,851 | mfu: 12.16 | total time: 36.12m
step 00167/21400 (0.78%) | loss: 4.474041 | grad norm: 0.3805 | lrm: 1.00 | dt: 13909.26ms | tok/sec: 37,693 | mfu: 12.11 | total time: 36.36m
step 00168/21400 (0.79%) | loss: 4.496343 | grad norm: 0.4177 | lrm: 1.00 | dt: 13825.79ms | tok/sec: 37,921 | mfu: 12.18 | total time: 36.59m
step 00169/21400 (0.79%) | loss: 4.498470 | grad norm: 0.4462 | lrm: 1.00 | dt: 13882.21ms | tok/sec: 37,766 | mfu: 12.13 | total time: 36.82m
step 00170/21400 (0.79%) | loss: 4.505782 | grad norm: 0.4522 | lrm: 1.00 | dt: 13830.83ms | tok/sec: 37,907 | mfu: 12.18 | total time: 37.05m
step 00171/21400 (0.80%) | loss: 4.542308 | grad norm: 0.4950 | lrm: 1.00 | dt: 13892.61ms | tok/sec: 37,738 | mfu: 12.12 | total time: 37.28m
step 00172/21400 (0.80%) | loss: 4.544766 | grad norm: 0.4460 | lrm: 1.00 | dt: 13831.78ms | tok/sec: 37,904 | mfu: 12.18 | total time: 37.51m
step 00173/21400 (0.81%) | loss: 4.588458 | grad norm: 0.4281 | lrm: 1.00 | dt: 13888.81ms | tok/sec: 37,748 | mfu: 12.13 | total time: 37.74m
step 00174/21400 (0.81%) | loss: 4.606758 | grad norm: 0.4201 | lrm: 1.00 | dt: 13823.03ms | tok/sec: 37,928 | mfu: 12.18 | total time: 37.97m
step 00175/21400 (0.82%) | loss: 4.578347 | grad norm: 0.4160 | lrm: 1.00 | dt: 13871.80ms | tok/sec: 37,795 | mfu: 12.14 | total time: 38.20m
step 00176/21400 (0.82%) | loss: 4.559724 | grad norm: 0.4000 | lrm: 1.00 | dt: 13845.74ms | tok/sec: 37,866 | mfu: 12.16 | total time: 38.43m
step 00177/21400 (0.83%) | loss: 4.567395 | grad norm: 0.4177 | lrm: 1.00 | dt: 14028.48ms | tok/sec: 37,373 | mfu: 12.01 | total time: 38.67m
step 00178/21400 (0.83%) | loss: 4.581964 | grad norm: 0.4354 | lrm: 1.00 | dt: 13863.08ms | tok/sec: 37,819 | mfu: 12.15 | total time: 38.90m
step 00179/21400 (0.84%) | loss: 4.560965 | grad norm: 0.4434 | lrm: 1.00 | dt: 13856.74ms | tok/sec: 37,836 | mfu: 12.16 | total time: 39.13m
step 00180/21400 (0.84%) | loss: 4.508952 | grad norm: 0.3965 | lrm: 1.00 | dt: 13891.86ms | tok/sec: 37,740 | mfu: 12.12 | total time: 39.36m
step 00181/21400 (0.85%) | loss: 4.478447 | grad norm: 0.4513 | lrm: 1.00 | dt: 13824.03ms | tok/sec: 37,925 | mfu: 12.18 | total time: 39.59m
step 00182/21400 (0.85%) | loss: 4.503279 | grad norm: 0.4590 | lrm: 1.00 | dt: 13916.10ms | tok/sec: 37,674 | mfu: 12.10 | total time: 39.82m
step 00183/21400 (0.86%) | loss: 4.455701 | grad norm: 0.3635 | lrm: 1.00 | dt: 13823.30ms | tok/sec: 37,927 | mfu: 12.18 | total time: 40.05m
step 00184/21400 (0.86%) | loss: 4.439036 | grad norm: 0.4123 | lrm: 1.00 | dt: 13891.57ms | tok/sec: 37,741 | mfu: 12.12 | total time: 40.29m
step 00185/21400 (0.86%) | loss: 4.448116 | grad norm: 0.4056 | lrm: 1.00 | dt: 13829.83ms | tok/sec: 37,909 | mfu: 12.18 | total time: 40.52m
step 00186/21400 (0.87%) | loss: 4.409805 | grad norm: 0.5519 | lrm: 1.00 | dt: 13871.93ms | tok/sec: 37,794 | mfu: 12.14 | total time: 40.75m
step 00187/21400 (0.87%) | loss: 4.414036 | grad norm: 0.4123 | lrm: 1.00 | dt: 13836.41ms | tok/sec: 37,891 | mfu: 12.17 | total time: 40.98m
step 00188/21400 (0.88%) | loss: 4.336584 | grad norm: 0.3784 | lrm: 1.00 | dt: 13875.95ms | tok/sec: 37,783 | mfu: 12.14 | total time: 41.21m
step 00189/21400 (0.88%) | loss: 4.336620 | grad norm: 0.3948 | lrm: 1.00 | dt: 13845.81ms | tok/sec: 37,866 | mfu: 12.16 | total time: 41.44m
step 00190/21400 (0.89%) | loss: 4.294427 | grad norm: 0.4359 | lrm: 1.00 | dt: 13884.33ms | tok/sec: 37,761 | mfu: 12.13 | total time: 41.67m
step 00191/21400 (0.89%) | loss: 4.305027 | grad norm: 0.4321 | lrm: 1.00 | dt: 13856.88ms | tok/sec: 37,835 | mfu: 12.15 | total time: 41.90m
step 00192/21400 (0.90%) | loss: 4.330184 | grad norm: 0.4203 | lrm: 1.00 | dt: 13838.26ms | tok/sec: 37,886 | mfu: 12.17 | total time: 42.13m
step 00193/21400 (0.90%) | loss: 4.331940 | grad norm: 0.4475 | lrm: 1.00 | dt: 13870.38ms | tok/sec: 37,799 | mfu: 12.14 | total time: 42.36m
step 00194/21400 (0.91%) | loss: 4.312325 | grad norm: 0.4740 | lrm: 1.00 | dt: 13845.01ms | tok/sec: 37,868 | mfu: 12.17 | total time: 42.59m
step 00195/21400 (0.91%) | loss: 4.262921 | grad norm: 0.4006 | lrm: 1.00 | dt: 13876.56ms | tok/sec: 37,782 | mfu: 12.14 | total time: 42.83m
step 00196/21400 (0.92%) | loss: 4.341553 | grad norm: 0.3569 | lrm: 1.00 | dt: 13815.70ms | tok/sec: 37,948 | mfu: 12.19 | total time: 43.06m
step 00197/21400 (0.92%) | loss: 4.320125 | grad norm: 0.4238 | lrm: 1.00 | dt: 13897.47ms | tok/sec: 37,725 | mfu: 12.12 | total time: 43.29m
step 00198/21400 (0.93%) | loss: 4.307129 | grad norm: 0.4063 | lrm: 1.00 | dt: 13833.49ms | tok/sec: 37,899 | mfu: 12.18 | total time: 43.52m
step 00199/21400 (0.93%) | loss: 4.286573 | grad norm: 0.3491 | lrm: 1.00 | dt: 14021.48ms | tok/sec: 37,391 | mfu: 12.01 | total time: 43.75m
step 00200/21400 (0.93%) | loss: 4.247301 | grad norm: 0.3726 | lrm: 1.00 | dt: 13830.75ms | tok/sec: 37,907 | mfu: 12.18 | total time: 43.98m
step 00201/21400 (0.94%) | loss: 4.289327 | grad norm: 0.3684 | lrm: 1.00 | dt: 13861.61ms | tok/sec: 37,823 | mfu: 12.15 | total time: 44.21m
step 00202/21400 (0.94%) | loss: 4.239883 | grad norm: 0.3325 | lrm: 1.00 | dt: 13834.34ms | tok/sec: 37,897 | mfu: 12.17 | total time: 44.44m
step 00203/21400 (0.95%) | loss: 4.188414 | grad norm: 0.3732 | lrm: 1.00 | dt: 13861.33ms | tok/sec: 37,823 | mfu: 12.15 | total time: 44.68m
step 00204/21400 (0.95%) | loss: 4.197911 | grad norm: 0.4013 | lrm: 1.00 | dt: 13858.23ms | tok/sec: 37,832 | mfu: 12.15 | total time: 44.91m
step 00205/21400 (0.96%) | loss: 4.154974 | grad norm: 0.4188 | lrm: 1.00 | dt: 13853.44ms | tok/sec: 37,845 | mfu: 12.16 | total time: 45.14m
step 00206/21400 (0.96%) | loss: 4.126209 | grad norm: 0.3346 | lrm: 1.00 | dt: 13870.62ms | tok/sec: 37,798 | mfu: 12.14 | total time: 45.37m
step 00207/21400 (0.97%) | loss: 4.096416 | grad norm: 0.3747 | lrm: 1.00 | dt: 13843.85ms | tok/sec: 37,871 | mfu: 12.17 | total time: 45.60m
step 00208/21400 (0.97%) | loss: 4.101238 | grad norm: 0.3690 | lrm: 1.00 | dt: 13889.67ms | tok/sec: 37,746 | mfu: 12.13 | total time: 45.83m
step 00209/21400 (0.98%) | loss: 4.025664 | grad norm: 0.3383 | lrm: 1.00 | dt: 13815.24ms | tok/sec: 37,949 | mfu: 12.19 | total time: 46.06m
step 00210/21400 (0.98%) | loss: 4.082033 | grad norm: 0.3383 | lrm: 1.00 | dt: 13875.73ms | tok/sec: 37,784 | mfu: 12.14 | total time: 46.29m
step 00211/21400 (0.99%) | loss: 4.048759 | grad norm: 0.3373 | lrm: 1.00 | dt: 13830.92ms | tok/sec: 37,906 | mfu: 12.18 | total time: 46.52m
step 00212/21400 (0.99%) | loss: 4.035040 | grad norm: 0.3524 | lrm: 1.00 | dt: 13891.60ms | tok/sec: 37,741 | mfu: 12.12 | total time: 46.75m
step 00213/21400 (1.00%) | loss: 4.057115 | grad norm: 0.3714 | lrm: 1.00 | dt: 13823.80ms | tok/sec: 37,926 | mfu: 12.18 | total time: 46.98m
step 00214/21400 (1.00%) | loss: 4.052705 | grad norm: 0.3607 | lrm: 1.00 | dt: 13863.29ms | tok/sec: 37,818 | mfu: 12.15 | total time: 47.22m
step 00215/21400 (1.00%) | loss: 4.054219 | grad norm: 0.3465 | lrm: 1.00 | dt: 13838.98ms | tok/sec: 37,884 | mfu: 12.17 | total time: 47.45m
step 00216/21400 (1.01%) | loss: 4.076721 | grad norm: 0.3265 | lrm: 1.00 | dt: 13870.39ms | tok/sec: 37,799 | mfu: 12.14 | total time: 47.68m
step 00217/21400 (1.01%) | loss: 4.094501 | grad norm: 0.3529 | lrm: 1.00 | dt: 13846.39ms | tok/sec: 37,864 | mfu: 12.16 | total time: 47.91m
step 00218/21400 (1.02%) | loss: 4.081100 | grad norm: 0.3716 | lrm: 1.00 | dt: 13844.49ms | tok/sec: 37,869 | mfu: 12.17 | total time: 48.14m
step 00219/21400 (1.02%) | loss: 4.088048 | grad norm: 0.3690 | lrm: 1.00 | dt: 13850.99ms | tok/sec: 37,852 | mfu: 12.16 | total time: 48.37m
step 00220/21400 (1.03%) | loss: 4.091775 | grad norm: 0.3797 | lrm: 1.00 | dt: 13980.83ms | tok/sec: 37,500 | mfu: 12.05 | total time: 48.60m
step 00221/21400 (1.03%) | loss: 4.093814 | grad norm: 0.4042 | lrm: 1.00 | dt: 13888.44ms | tok/sec: 37,749 | mfu: 12.13 | total time: 48.83m
step 00222/21400 (1.04%) | loss: 4.115037 | grad norm: 0.4304 | lrm: 1.00 | dt: 13821.98ms | tok/sec: 37,931 | mfu: 12.19 | total time: 49.06m
step 00223/21400 (1.04%) | loss: 4.121836 | grad norm: 0.3952 | lrm: 1.00 | dt: 13888.88ms | tok/sec: 37,748 | mfu: 12.13 | total time: 49.30m
step 00224/21400 (1.05%) | loss: 4.093474 | grad norm: 0.3526 | lrm: 1.00 | dt: 13825.88ms | tok/sec: 37,920 | mfu: 12.18 | total time: 49.53m
step 00225/21400 (1.05%) | loss: 4.089423 | grad norm: 0.3344 | lrm: 1.00 | dt: 13873.71ms | tok/sec: 37,790 | mfu: 12.14 | total time: 49.76m
step 00226/21400 (1.06%) | loss: 4.067555 | grad norm: 0.3169 | lrm: 1.00 | dt: 13830.26ms | tok/sec: 37,908 | mfu: 12.18 | total time: 49.99m
step 00227/21400 (1.06%) | loss: 4.065075 | grad norm: 0.2998 | lrm: 1.00 | dt: 13864.47ms | tok/sec: 37,815 | mfu: 12.15 | total time: 50.22m
step 00228/21400 (1.07%) | loss: 4.016151 | grad norm: 0.3197 | lrm: 1.00 | dt: 13849.94ms | tok/sec: 37,854 | mfu: 12.16 | total time: 50.45m
step 00229/21400 (1.07%) | loss: 4.033918 | grad norm: 0.3469 | lrm: 1.00 | dt: 13880.40ms | tok/sec: 37,771 | mfu: 12.13 | total time: 50.68m
step 00230/21400 (1.07%) | loss: 4.031875 | grad norm: 0.3317 | lrm: 1.00 | dt: 13850.24ms | tok/sec: 37,854 | mfu: 12.16 | total time: 50.91m
step 00231/21400 (1.08%) | loss: 4.049609 | grad norm: 0.3260 | lrm: 1.00 | dt: 13852.56ms | tok/sec: 37,847 | mfu: 12.16 | total time: 51.14m
step 00232/21400 (1.08%) | loss: 4.047381 | grad norm: 0.3329 | lrm: 1.00 | dt: 13872.36ms | tok/sec: 37,793 | mfu: 12.14 | total time: 51.37m
step 00233/21400 (1.09%) | loss: 4.063304 | grad norm: 0.3027 | lrm: 1.00 | dt: 13899.18ms | tok/sec: 37,720 | mfu: 12.12 | total time: 51.61m
step 00234/21400 (1.09%) | loss: 4.045008 | grad norm: 0.2873 | lrm: 1.00 | dt: 13808.75ms | tok/sec: 37,967 | mfu: 12.20 | total time: 51.84m
step 00235/21400 (1.10%) | loss: 4.069828 | grad norm: 0.3017 | lrm: 1.00 | dt: 13850.72ms | tok/sec: 37,852 | mfu: 12.16 | total time: 52.07m
step 00236/21400 (1.10%) | loss: 4.080804 | grad norm: 0.2963 | lrm: 1.00 | dt: 13885.22ms | tok/sec: 37,758 | mfu: 12.13 | total time: 52.30m
step 00237/21400 (1.11%) | loss: 4.037993 | grad norm: 0.3175 | lrm: 1.00 | dt: 13836.65ms | tok/sec: 37,891 | mfu: 12.17 | total time: 52.53m
step 00238/21400 (1.11%) | loss: 4.047833 | grad norm: 0.3247 | lrm: 1.00 | dt: 13866.26ms | tok/sec: 37,810 | mfu: 12.15 | total time: 52.76m
step 00239/21400 (1.12%) | loss: 4.016516 | grad norm: 0.3189 | lrm: 1.00 | dt: 13825.84ms | tok/sec: 37,920 | mfu: 12.18 | total time: 52.99m
step 00240/21400 (1.12%) | loss: 4.016487 | grad norm: 0.3487 | lrm: 1.00 | dt: 13873.90ms | tok/sec: 37,789 | mfu: 12.14 | total time: 53.22m
step 00241/21400 (1.13%) | loss: 4.021149 | grad norm: 0.3763 | lrm: 1.00 | dt: 13841.83ms | tok/sec: 37,877 | mfu: 12.17 | total time: 53.45m
step 00242/21400 (1.13%) | loss: 4.039763 | grad norm: 0.3595 | lrm: 1.00 | dt: 13866.51ms | tok/sec: 37,809 | mfu: 12.15 | total time: 53.68m
step 00243/21400 (1.14%) | loss: 4.056911 | grad norm: 0.3545 | lrm: 1.00 | dt: 14100.87ms | tok/sec: 37,181 | mfu: 11.94 | total time: 53.92m
step 00244/21400 (1.14%) | loss: 4.034566 | grad norm: 0.3390 | lrm: 1.00 | dt: 13898.38ms | tok/sec: 37,722 | mfu: 12.12 | total time: 54.15m
step 00245/21400 (1.14%) | loss: 4.030394 | grad norm: 0.3434 | lrm: 1.00 | dt: 13844.43ms | tok/sec: 37,869 | mfu: 12.17 | total time: 54.38m
step 00246/21400 (1.15%) | loss: 4.042706 | grad norm: 0.3461 | lrm: 1.00 | dt: 13829.92ms | tok/sec: 37,909 | mfu: 12.18 | total time: 54.61m
step 00247/21400 (1.15%) | loss: 4.048060 | grad norm: 0.3462 | lrm: 1.00 | dt: 13904.30ms | tok/sec: 37,706 | mfu: 12.11 | total time: 54.84m
step 00248/21400 (1.16%) | loss: 4.049196 | grad norm: 0.3752 | lrm: 1.00 | dt: 13807.29ms | tok/sec: 37,971 | mfu: 12.20 | total time: 55.07m
step 00249/21400 (1.16%) | loss: 4.106027 | grad norm: 0.3554 | lrm: 1.00 | dt: 13882.42ms | tok/sec: 37,766 | mfu: 12.13 | total time: 55.30m
Step 00250 | Validation bpb: 1.1989
step 00250/21400 (1.17%) | loss: 4.129027 | grad norm: 0.2951 | lrm: 1.00 | dt: 13826.83ms | tok/sec: 37,918 | mfu: 12.18 | total time: 55.54m
step 00251/21400 (1.17%) | loss: 4.114199 | grad norm: 0.2876 | lrm: 1.00 | dt: 13854.61ms | tok/sec: 37,842 | mfu: 12.16 | total time: 55.77m
step 00252/21400 (1.18%) | loss: 4.096955 | grad norm: 0.2624 | lrm: 1.00 | dt: 13840.30ms | tok/sec: 37,881 | mfu: 12.17 | total time: 56.00m
step 00253/21400 (1.18%) | loss: 4.085331 | grad norm: 0.2817 | lrm: 1.00 | dt: 13856.34ms | tok/sec: 37,837 | mfu: 12.16 | total time: 56.23m
step 00254/21400 (1.19%) | loss: 4.077127 | grad norm: 0.3363 | lrm: 1.00 | dt: 13840.00ms | tok/sec: 37,882 | mfu: 12.17 | total time: 56.46m
step 00255/21400 (1.19%) | loss: 4.085231 | grad norm: 0.3268 | lrm: 1.00 | dt: 13848.61ms | tok/sec: 37,858 | mfu: 12.16 | total time: 56.69m
step 00256/21400 (1.20%) | loss: 4.069500 | grad norm: 0.3296 | lrm: 1.00 | dt: 13855.82ms | tok/sec: 37,838 | mfu: 12.16 | total time: 56.92m
step 00257/21400 (1.20%) | loss: 4.026961 | grad norm: 0.3399 | lrm: 1.00 | dt: 13827.97ms | tok/sec: 37,915 | mfu: 12.18 | total time: 57.15m
step 00258/21400 (1.21%) | loss: 4.033394 | grad norm: 0.2766 | lrm: 1.00 | dt: 13891.41ms | tok/sec: 37,741 | mfu: 12.12 | total time: 57.38m
step 00259/21400 (1.21%) | loss: 4.052650 | grad norm: 0.3053 | lrm: 1.00 | dt: 13833.45ms | tok/sec: 37,900 | mfu: 12.18 | total time: 57.61m
step 00260/21400 (1.21%) | loss: 4.050843 | grad norm: 0.2731 | lrm: 1.00 | dt: 13875.00ms | tok/sec: 37,786 | mfu: 12.14 | total time: 57.84m
step 00261/21400 (1.22%) | loss: 4.059598 | grad norm: 0.2654 | lrm: 1.00 | dt: 13864.88ms | tok/sec: 37,814 | mfu: 12.15 | total time: 58.08m
step 00262/21400 (1.22%) | loss: 4.105172 | grad norm: 0.2886 | lrm: 1.00 | dt: 13971.89ms | tok/sec: 37,524 | mfu: 12.05 | total time: 58.31m
step 00263/21400 (1.23%) | loss: 4.228098 | grad norm: 0.3161 | lrm: 1.00 | dt: 13870.35ms | tok/sec: 37,799 | mfu: 12.14 | total time: 58.54m
step 00264/21400 (1.23%) | loss: 4.182748 | grad norm: 0.2805 | lrm: 1.00 | dt: 13870.83ms | tok/sec: 37,797 | mfu: 12.14 | total time: 58.77m
step 00265/21400 (1.24%) | loss: 4.124622 | grad norm: 0.2756 | lrm: 1.00 | dt: 13741.44ms | tok/sec: 38,153 | mfu: 12.26 | total time: 59.00m
step 00266/21400 (1.24%) | loss: 4.101463 | grad norm: 0.2845 | lrm: 1.00 | dt: 13840.22ms | tok/sec: 37,881 | mfu: 12.17 | total time: 59.23m
step 00267/21400 (1.25%) | loss: 4.034062 | grad norm: 0.2745 | lrm: 1.00 | dt: 13896.62ms | tok/sec: 37,727 | mfu: 12.12 | total time: 59.46m
step 00268/21400 (1.25%) | loss: 3.999240 | grad norm: 0.2664 | lrm: 1.00 | dt: 13851.22ms | tok/sec: 37,851 | mfu: 12.16 | total time: 59.69m
step 00269/21400 (1.26%) | loss: 3.978565 | grad norm: 0.2901 | lrm: 1.00 | dt: 13792.18ms | tok/sec: 38,013 | mfu: 12.21 | total time: 59.92m
step 00270/21400 (1.26%) | loss: 3.980482 | grad norm: 0.2932 | lrm: 1.00 | dt: 13865.52ms | tok/sec: 37,812 | mfu: 12.15 | total time: 60.15m
step 00271/21400 (1.27%) | loss: 3.954025 | grad norm: 0.2635 | lrm: 1.00 | dt: 13725.43ms | tok/sec: 38,198 | mfu: 12.27 | total time: 60.38m
step 00272/21400 (1.27%) | loss: 3.947527 | grad norm: 0.2757 | lrm: 1.00 | dt: 13903.35ms | tok/sec: 37,709 | mfu: 12.11 | total time: 60.61m
step 00273/21400 (1.28%) | loss: 3.927086 | grad norm: 0.2734 | lrm: 1.00 | dt: 13749.50ms | tok/sec: 38,131 | mfu: 12.25 | total time: 60.84m
step 00274/21400 (1.28%) | loss: 3.954555 | grad norm: 0.2942 | lrm: 1.00 | dt: 13878.74ms | tok/sec: 37,776 | mfu: 12.14 | total time: 61.07m
step 00275/21400 (1.29%) | loss: 3.985155 | grad norm: 0.2967 | lrm: 1.00 | dt: 13759.23ms | tok/sec: 38,104 | mfu: 12.24 | total time: 61.30m
step 00276/21400 (1.29%) | loss: 3.972508 | grad norm: 0.2879 | lrm: 1.00 | dt: 13838.49ms | tok/sec: 37,886 | mfu: 12.17 | total time: 61.53m
step 00277/21400 (1.29%) | loss: 3.968338 | grad norm: 0.2613 | lrm: 1.00 | dt: 13847.34ms | tok/sec: 37,861 | mfu: 12.16 | total time: 61.77m
step 00278/21400 (1.30%) | loss: 3.963785 | grad norm: 0.2679 | lrm: 1.00 | dt: 13849.63ms | tok/sec: 37,855 | mfu: 12.16 | total time: 62.00m
step 00279/21400 (1.30%) | loss: 3.960028 | grad norm: 0.2691 | lrm: 1.00 | dt: 13815.88ms | tok/sec: 37,948 | mfu: 12.19 | total time: 62.23m
step 00280/21400 (1.31%) | loss: 3.957981 | grad norm: 0.2825 | lrm: 1.00 | dt: 13827.28ms | tok/sec: 37,916 | mfu: 12.18 | total time: 62.46m
step 00281/21400 (1.31%) | loss: 3.974779 | grad norm: 0.2876 | lrm: 1.00 | dt: 13846.80ms | tok/sec: 37,863 | mfu: 12.16 | total time: 62.69m
step 00282/21400 (1.32%) | loss: 3.973416 | grad norm: 0.2817 | lrm: 1.00 | dt: 13788.22ms | tok/sec: 38,024 | mfu: 12.22 | total time: 62.92m
step 00283/21400 (1.32%) | loss: 3.960138 | grad norm: 0.2650 | lrm: 1.00 | dt: 13866.74ms | tok/sec: 37,809 | mfu: 12.15 | total time: 63.15m
step 00284/21400 (1.33%) | loss: 3.936998 | grad norm: 0.2617 | lrm: 1.00 | dt: 13733.27ms | tok/sec: 38,176 | mfu: 12.26 | total time: 63.38m
step 00285/21400 (1.33%) | loss: 3.945053 | grad norm: 0.2604 | lrm: 1.00 | dt: 13996.11ms | tok/sec: 37,459 | mfu: 12.03 | total time: 63.61m
step 00286/21400 (1.34%) | loss: 3.935640 | grad norm: 0.2784 | lrm: 1.00 | dt: 13859.90ms | tok/sec: 37,827 | mfu: 12.15 | total time: 63.84m
step 00287/21400 (1.34%) | loss: 3.956711 | grad norm: 0.2628 | lrm: 1.00 | dt: 13847.80ms | tok/sec: 37,860 | mfu: 12.16 | total time: 64.07m
step 00288/21400 (1.35%) | loss: 3.958825 | grad norm: 0.2678 | lrm: 1.00 | dt: 13762.33ms | tok/sec: 38,095 | mfu: 12.24 | total time: 64.30m
step 00289/21400 (1.35%) | loss: 3.958670 | grad norm: 0.2723 | lrm: 1.00 | dt: 13857.86ms | tok/sec: 37,833 | mfu: 12.15 | total time: 64.53m
step 00290/21400 (1.36%) | loss: 3.950689 | grad norm: 0.2501 | lrm: 1.00 | dt: 13772.61ms | tok/sec: 38,067 | mfu: 12.23 | total time: 64.76m
step 00291/21400 (1.36%) | loss: 3.951892 | grad norm: 0.2505 | lrm: 1.00 | dt: 13854.97ms | tok/sec: 37,841 | mfu: 12.16 | total time: 64.99m
step 00292/21400 (1.36%) | loss: 3.934354 | grad norm: 0.2410 | lrm: 1.00 | dt: 13818.41ms | tok/sec: 37,941 | mfu: 12.19 | total time: 65.22m
step 00293/21400 (1.37%) | loss: 3.938267 | grad norm: 0.2366 | lrm: 1.00 | dt: 13798.58ms | tok/sec: 37,995 | mfu: 12.21 | total time: 65.45m
step 00294/21400 (1.37%) | loss: 3.998275 | grad norm: 0.2358 | lrm: 1.00 | dt: 13803.72ms | tok/sec: 37,981 | mfu: 12.20 | total time: 65.68m
step 00295/21400 (1.38%) | loss: 3.983075 | grad norm: 0.2541 | lrm: 1.00 | dt: 13852.15ms | tok/sec: 37,848 | mfu: 12.16 | total time: 65.91m
step 00296/21400 (1.38%) | loss: 4.029271 | grad norm: 0.2573 | lrm: 1.00 | dt: 13866.90ms | tok/sec: 37,808 | mfu: 12.15 | total time: 66.15m
step 00297/21400 (1.39%) | loss: 4.007366 | grad norm: 0.2415 | lrm: 1.00 | dt: 13755.57ms | tok/sec: 38,114 | mfu: 12.24 | total time: 66.37m
step 00298/21400 (1.39%) | loss: 4.007111 | grad norm: 0.2436 | lrm: 1.00 | dt: 13814.35ms | tok/sec: 37,952 | mfu: 12.19 | total time: 66.60m
step 00299/21400 (1.40%) | loss: 3.995109 | grad norm: 0.2292 | lrm: 1.00 | dt: 13845.76ms | tok/sec: 37,866 | mfu: 12.16 | total time: 66.84m
step 00300/21400 (1.40%) | loss: 4.019240 | grad norm: 0.2329 | lrm: 1.00 | dt: 13868.05ms | tok/sec: 37,805 | mfu: 12.15 | total time: 67.07m
step 00301/21400 (1.41%) | loss: 4.030283 | grad norm: 0.2606 | lrm: 1.00 | dt: 13787.69ms | tok/sec: 38,025 | mfu: 12.22 | total time: 67.30m
step 00302/21400 (1.41%) | loss: 4.046308 | grad norm: 0.2573 | lrm: 1.00 | dt: 13854.10ms | tok/sec: 37,843 | mfu: 12.16 | total time: 67.53m
step 00303/21400 (1.42%) | loss: 4.025602 | grad norm: 0.2484 | lrm: 1.00 | dt: 13814.46ms | tok/sec: 37,952 | mfu: 12.19 | total time: 67.76m
step 00304/21400 (1.42%) | loss: 4.186553 | grad norm: 0.2666 | lrm: 1.00 | dt: 13827.75ms | tok/sec: 37,915 | mfu: 12.18 | total time: 67.99m
step 00305/21400 (1.43%) | loss: 4.172468 | grad norm: 0.2520 | lrm: 1.00 | dt: 13813.32ms | tok/sec: 37,955 | mfu: 12.19 | total time: 68.22m
step 00306/21400 (1.43%) | loss: 4.167838 | grad norm: 0.2288 | lrm: 1.00 | dt: 13822.97ms | tok/sec: 37,928 | mfu: 12.18 | total time: 68.45m
step 00307/21400 (1.43%) | loss: 4.106843 | grad norm: 0.2569 | lrm: 1.00 | dt: 13854.70ms | tok/sec: 37,841 | mfu: 12.16 | total time: 68.68m
step 00308/21400 (1.44%) | loss: 4.121622 | grad norm: 0.2587 | lrm: 1.00 | dt: 13877.07ms | tok/sec: 37,780 | mfu: 12.14 | total time: 68.91m
step 00309/21400 (1.44%) | loss: 4.074167 | grad norm: 0.2696 | lrm: 1.00 | dt: 13910.06ms | tok/sec: 37,691 | mfu: 12.11 | total time: 69.14m
step 00310/21400 (1.45%) | loss: 4.168025 | grad norm: 0.2532 | lrm: 1.00 | dt: 13723.64ms | tok/sec: 38,203 | mfu: 12.27 | total time: 69.37m
step 00311/21400 (1.45%) | loss: 4.105127 | grad norm: 0.2402 | lrm: 1.00 | dt: 13904.65ms | tok/sec: 37,705 | mfu: 12.11 | total time: 69.60m
step 00312/21400 (1.46%) | loss: 4.081283 | grad norm: 0.2488 | lrm: 1.00 | dt: 13742.71ms | tok/sec: 38,150 | mfu: 12.26 | total time: 69.83m
step 00313/21400 (1.46%) | loss: 4.072449 | grad norm: 0.3656 | lrm: 1.00 | dt: 13837.12ms | tok/sec: 37,889 | mfu: 12.17 | total time: 70.06m
step 00314/21400 (1.47%) | loss: 4.026081 | grad norm: 0.1981 | lrm: 1.00 | dt: 13846.59ms | tok/sec: 37,864 | mfu: 12.16 | total time: 70.29m
step 00315/21400 (1.47%) | loss: 3.979747 | grad norm: 0.2066 | lrm: 1.00 | dt: 13830.59ms | tok/sec: 37,907 | mfu: 12.18 | total time: 70.52m
step 00316/21400 (1.48%) | loss: 3.956719 | grad norm: 0.2271 | lrm: 1.00 | dt: 13787.81ms | tok/sec: 38,025 | mfu: 12.22 | total time: 70.75m
step 00317/21400 (1.48%) | loss: 3.991026 | grad norm: 0.2127 | lrm: 1.00 | dt: 13861.52ms | tok/sec: 37,823 | mfu: 12.15 | total time: 70.99m
step 00318/21400 (1.49%) | loss: 3.970247 | grad norm: 0.2366 | lrm: 1.00 | dt: 13814.84ms | tok/sec: 37,951 | mfu: 12.19 | total time: 71.22m
step 00319/21400 (1.49%) | loss: 3.968716 | grad norm: 0.2401 | lrm: 1.00 | dt: 13827.50ms | tok/sec: 37,916 | mfu: 12.18 | total time: 71.45m
step 00320/21400 (1.50%) | loss: 3.946412 | grad norm: 0.2134 | lrm: 1.00 | dt: 13868.01ms | tok/sec: 37,805 | mfu: 12.15 | total time: 71.68m
step 00321/21400 (1.50%) | loss: 3.906867 | grad norm: 0.2167 | lrm: 1.00 | dt: 13874.29ms | tok/sec: 37,788 | mfu: 12.14 | total time: 71.91m
step 00322/21400 (1.50%) | loss: 3.847535 | grad norm: 0.2428 | lrm: 1.00 | dt: 13680.89ms | tok/sec: 38,322 | mfu: 12.31 | total time: 72.14m
step 00323/21400 (1.51%) | loss: 3.848613 | grad norm: 0.2462 | lrm: 1.00 | dt: 13861.61ms | tok/sec: 37,823 | mfu: 12.15 | total time: 72.37m
step 00324/21400 (1.51%) | loss: 3.854299 | grad norm: 0.2193 | lrm: 1.00 | dt: 13894.14ms | tok/sec: 37,734 | mfu: 12.12 | total time: 72.60m
step 00325/21400 (1.52%) | loss: 3.852849 | grad norm: 0.2043 | lrm: 1.00 | dt: 13741.55ms | tok/sec: 38,153 | mfu: 12.26 | total time: 72.83m
step 00326/21400 (1.52%) | loss: 3.854690 | grad norm: 0.2224 | lrm: 1.00 | dt: 13812.88ms | tok/sec: 37,956 | mfu: 12.19 | total time: 73.06m
step 00327/21400 (1.53%) | loss: 3.863972 | grad norm: 0.2458 | lrm: 1.00 | dt: 13860.43ms | tok/sec: 37,826 | mfu: 12.15 | total time: 73.29m
step 00328/21400 (1.53%) | loss: 3.833233 | grad norm: 0.2538 | lrm: 1.00 | dt: 13820.45ms | tok/sec: 37,935 | mfu: 12.19 | total time: 73.52m
step 00329/21400 (1.54%) | loss: 3.829406 | grad norm: 0.2259 | lrm: 1.00 | dt: 13777.71ms | tok/sec: 38,053 | mfu: 12.22 | total time: 73.75m
step 00330/21400 (1.54%) | loss: 3.830408 | grad norm: 0.2180 | lrm: 1.00 | dt: 13809.26ms | tok/sec: 37,966 | mfu: 12.20 | total time: 73.98m
step 00331/21400 (1.55%) | loss: 3.801540 | grad norm: 0.2215 | lrm: 1.00 | dt: 13991.50ms | tok/sec: 37,471 | mfu: 12.04 | total time: 74.21m
step 00332/21400 (1.55%) | loss: 3.761679 | grad norm: 0.2234 | lrm: 1.00 | dt: 13837.25ms | tok/sec: 37,889 | mfu: 12.17 | total time: 74.44m
step 00333/21400 (1.56%) | loss: 3.776024 | grad norm: 0.1993 | lrm: 1.00 | dt: 13854.72ms | tok/sec: 37,841 | mfu: 12.16 | total time: 74.67m
step 00334/21400 (1.56%) | loss: 3.779891 | grad norm: 0.2035 | lrm: 1.00 | dt: 13726.12ms | tok/sec: 38,196 | mfu: 12.27 | total time: 74.90m
step 00335/21400 (1.57%) | loss: 3.774975 | grad norm: 0.2187 | lrm: 1.00 | dt: 13862.48ms | tok/sec: 37,820 | mfu: 12.15 | total time: 75.13m
step 00336/21400 (1.57%) | loss: 3.739039 | grad norm: 0.2331 | lrm: 1.00 | dt: 13868.06ms | tok/sec: 37,805 | mfu: 12.15 | total time: 75.36m
step 00337/21400 (1.57%) | loss: 3.749939 | grad norm: 0.2276 | lrm: 1.00 | dt: 13777.36ms | tok/sec: 38,054 | mfu: 12.23 | total time: 75.59m
step 00338/21400 (1.58%) | loss: 3.760242 | grad norm: 0.2171 | lrm: 1.00 | dt: 13866.49ms | tok/sec: 37,809 | mfu: 12.15 | total time: 75.83m
step 00339/21400 (1.58%) | loss: 3.795865 | grad norm: 0.2051 | lrm: 1.00 | dt: 13819.20ms | tok/sec: 37,939 | mfu: 12.19 | total time: 76.06m
step 00340/21400 (1.59%) | loss: 3.785150 | grad norm: 0.2050 | lrm: 1.00 | dt: 13763.33ms | tok/sec: 38,093 | mfu: 12.24 | total time: 76.29m
step 00341/21400 (1.59%) | loss: 3.778584 | grad norm: 0.2363 | lrm: 1.00 | dt: 13894.02ms | tok/sec: 37,734 | mfu: 12.12 | total time: 76.52m
step 00342/21400 (1.60%) | loss: 3.789712 | grad norm: 0.2494 | lrm: 1.00 | dt: 13740.80ms | tok/sec: 38,155 | mfu: 12.26 | total time: 76.75m
step 00343/21400 (1.60%) | loss: 3.798150 | grad norm: 0.2676 | lrm: 1.00 | dt: 13871.38ms | tok/sec: 37,796 | mfu: 12.14 | total time: 76.98m
step 00344/21400 (1.61%) | loss: 3.833435 | grad norm: 0.2259 | lrm: 1.00 | dt: 13844.34ms | tok/sec: 37,870 | mfu: 12.17 | total time: 77.21m
step 00345/21400 (1.61%) | loss: 3.795031 | grad norm: 0.2407 | lrm: 1.00 | dt: 13781.67ms | tok/sec: 38,042 | mfu: 12.22 | total time: 77.44m
step 00346/21400 (1.62%) | loss: 3.782899 | grad norm: 0.2726 | lrm: 1.00 | dt: 13803.54ms | tok/sec: 37,982 | mfu: 12.20 | total time: 77.67m
step 00347/21400 (1.62%) | loss: 3.767198 | grad norm: 0.2447 | lrm: 1.00 | dt: 13865.58ms | tok/sec: 37,812 | mfu: 12.15 | total time: 77.90m
step 00348/21400 (1.63%) | loss: 3.756487 | grad norm: 0.2401 | lrm: 1.00 | dt: 13775.85ms | tok/sec: 38,058 | mfu: 12.23 | total time: 78.13m
step 00349/21400 (1.63%) | loss: 3.761104 | grad norm: 0.2371 | lrm: 1.00 | dt: 13854.84ms | tok/sec: 37,841 | mfu: 12.16 | total time: 78.36m
step 00350/21400 (1.64%) | loss: 3.777889 | grad norm: 0.2540 | lrm: 1.00 | dt: 13776.30ms | tok/sec: 38,057 | mfu: 12.23 | total time: 78.59m
step 00351/21400 (1.64%) | loss: 3.787396 | grad norm: 0.2435 | lrm: 1.00 | dt: 13850.35ms | tok/sec: 37,853 | mfu: 12.16 | total time: 78.82m
step 00352/21400 (1.64%) | loss: 3.793636 | grad norm: 0.2171 | lrm: 1.00 | dt: 13772.40ms | tok/sec: 38,068 | mfu: 12.23 | total time: 79.05m
step 00353/21400 (1.65%) | loss: 3.843819 | grad norm: 0.1938 | lrm: 1.00 | dt: 13850.77ms | tok/sec: 37,852 | mfu: 12.16 | total time: 79.28m
step 00354/21400 (1.65%) | loss: 3.845816 | grad norm: 0.1913 | lrm: 1.00 | dt: 13789.71ms | tok/sec: 38,020 | mfu: 12.21 | total time: 79.51m
step 00355/21400 (1.66%) | loss: 3.835034 | grad norm: 0.2240 | lrm: 1.00 | dt: 13999.25ms | tok/sec: 37,451 | mfu: 12.03 | total time: 79.74m
step 00356/21400 (1.66%) | loss: 3.814428 | grad norm: 0.1915 | lrm: 1.00 | dt: 13851.89ms | tok/sec: 37,849 | mfu: 12.16 | total time: 79.97m
step 00357/21400 (1.67%) | loss: 3.832943 | grad norm: 0.1853 | lrm: 1.00 | dt: 13863.27ms | tok/sec: 37,818 | mfu: 12.15 | total time: 80.21m
step 00358/21400 (1.67%) | loss: 3.861302 | grad norm: 0.2129 | lrm: 1.00 | dt: 13796.68ms | tok/sec: 38,001 | mfu: 12.21 | total time: 80.44m
step 00359/21400 (1.68%) | loss: 3.875014 | grad norm: 0.2261 | lrm: 1.00 | dt: 14107.09ms | tok/sec: 37,164 | mfu: 11.94 | total time: 80.67m
step 00360/21400 (1.68%) | loss: 3.877462 | grad norm: 0.2186 | lrm: 1.00 | dt: 13953.08ms | tok/sec: 37,575 | mfu: 12.07 | total time: 80.90m
step 00361/21400 (1.69%) | loss: 3.896116 | grad norm: 0.2194 | lrm: 1.00 | dt: 13828.40ms | tok/sec: 37,913 | mfu: 12.18 | total time: 81.13m
step 00362/21400 (1.69%) | loss: 3.862753 | grad norm: 0.2143 | lrm: 1.00 | dt: 13916.31ms | tok/sec: 37,674 | mfu: 12.10 | total time: 81.37m
step 00363/21400 (1.70%) | loss: 3.861299 | grad norm: 0.2267 | lrm: 1.00 | dt: 13826.46ms | tok/sec: 37,919 | mfu: 12.18 | total time: 81.60m
step 00364/21400 (1.70%) | loss: 3.834813 | grad norm: 0.2326 | lrm: 1.00 | dt: 13907.86ms | tok/sec: 37,697 | mfu: 12.11 | total time: 81.83m
step 00365/21400 (1.71%) | loss: 3.777235 | grad norm: 0.2097 | lrm: 1.00 | dt: 13838.43ms | tok/sec: 37,886 | mfu: 12.17 | total time: 82.06m
step 00366/21400 (1.71%) | loss: 3.754658 | grad norm: 0.2123 | lrm: 1.00 | dt: 14282.58ms | tok/sec: 36,708 | mfu: 11.79 | total time: 82.30m
step 00367/21400 (1.71%) | loss: 3.723955 | grad norm: 0.2097 | lrm: 1.00 | dt: 14002.82ms | tok/sec: 37,441 | mfu: 12.03 | total time: 82.53m
step 00368/21400 (1.72%) | loss: 3.711424 | grad norm: 0.1957 | lrm: 1.00 | dt: 13907.41ms | tok/sec: 37,698 | mfu: 12.11 | total time: 82.76m
step 00369/21400 (1.72%) | loss: 3.763041 | grad norm: 0.2122 | lrm: 1.00 | dt: 13922.84ms | tok/sec: 37,656 | mfu: 12.10 | total time: 82.99m
step 00370/21400 (1.73%) | loss: 3.772548 | grad norm: 0.2143 | lrm: 1.00 | dt: 13887.75ms | tok/sec: 37,751 | mfu: 12.13 | total time: 83.22m
step 00371/21400 (1.73%) | loss: 3.790051 | grad norm: 0.2215 | lrm: 1.00 | dt: 13897.63ms | tok/sec: 37,725 | mfu: 12.12 | total time: 83.46m
step 00372/21400 (1.74%) | loss: 3.789628 | grad norm: 0.2103 | lrm: 1.00 | dt: 13916.91ms | tok/sec: 37,672 | mfu: 12.10 | total time: 83.69m
step 00373/21400 (1.74%) | loss: 3.771456 | grad norm: 0.2165 | lrm: 1.00 | dt: 13891.18ms | tok/sec: 37,742 | mfu: 12.12 | total time: 83.92m
step 00374/21400 (1.75%) | loss: 3.774367 | grad norm: 0.2055 | lrm: 1.00 | dt: 13845.77ms | tok/sec: 37,866 | mfu: 12.16 | total time: 84.15m
step 00375/21400 (1.75%) | loss: 3.774425 | grad norm: 0.1901 | lrm: 1.00 | dt: 13926.03ms | tok/sec: 37,648 | mfu: 12.09 | total time: 84.38m
step 00376/21400 (1.76%) | loss: 3.746191 | grad norm: 0.1939 | lrm: 1.00 | dt: 13997.56ms | tok/sec: 37,455 | mfu: 12.03 | total time: 84.62m
step 00377/21400 (1.76%) | loss: 3.753286 | grad norm: 0.1785 | lrm: 1.00 | dt: 14123.40ms | tok/sec: 37,121 | mfu: 11.93 | total time: 84.85m
step 00378/21400 (1.77%) | loss: 3.751129 | grad norm: 0.2280 | lrm: 1.00 | dt: 13796.69ms | tok/sec: 38,001 | mfu: 12.21 | total time: 85.08m
step 00379/21400 (1.77%) | loss: 3.825953 | grad norm: 0.1903 | lrm: 1.00 | dt: 14037.07ms | tok/sec: 37,350 | mfu: 12.00 | total time: 85.32m
step 00380/21400 (1.78%) | loss: 3.794189 | grad norm: 0.1857 | lrm: 1.00 | dt: 13816.76ms | tok/sec: 37,945 | mfu: 12.19 | total time: 85.55m
step 00381/21400 (1.78%) | loss: 3.765235 | grad norm: 0.1840 | lrm: 1.00 | dt: 14034.32ms | tok/sec: 37,357 | mfu: 12.00 | total time: 85.78m
step 00382/21400 (1.79%) | loss: 3.762724 | grad norm: 0.1757 | lrm: 1.00 | dt: 13867.27ms | tok/sec: 37,807 | mfu: 12.15 | total time: 86.01m
step 00383/21400 (1.79%) | loss: 3.773042 | grad norm: 0.1752 | lrm: 1.00 | dt: 13898.90ms | tok/sec: 37,721 | mfu: 12.12 | total time: 86.24m
step 00384/21400 (1.79%) | loss: 3.795555 | grad norm: 0.1841 | lrm: 1.00 | dt: 13840.84ms | tok/sec: 37,879 | mfu: 12.17 | total time: 86.47m
step 00385/21400 (1.80%) | loss: 3.778578 | grad norm: 0.1880 | lrm: 1.00 | dt: 13846.18ms | tok/sec: 37,865 | mfu: 12.16 | total time: 86.70m
step 00386/21400 (1.80%) | loss: 3.749758 | grad norm: 0.1809 | lrm: 1.00 | dt: 13911.01ms | tok/sec: 37,688 | mfu: 12.11 | total time: 86.94m
step 00387/21400 (1.81%) | loss: 3.777112 | grad norm: 0.1817 | lrm: 1.00 | dt: 13835.62ms | tok/sec: 37,894 | mfu: 12.17 | total time: 87.17m
step 00388/21400 (1.81%) | loss: 3.786519 | grad norm: 0.2108 | lrm: 1.00 | dt: 13884.00ms | tok/sec: 37,762 | mfu: 12.13 | total time: 87.40m
step 00389/21400 (1.82%) | loss: 3.800486 | grad norm: 0.2142 | lrm: 1.00 | dt: 13803.36ms | tok/sec: 37,982 | mfu: 12.20 | total time: 87.63m
step 00390/21400 (1.82%) | loss: 3.743340 | grad norm: 0.2138 | lrm: 1.00 | dt: 13880.91ms | tok/sec: 37,770 | mfu: 12.13 | total time: 87.86m
step 00391/21400 (1.83%) | loss: 3.820842 | grad norm: 0.2166 | lrm: 1.00 | dt: 13811.19ms | tok/sec: 37,961 | mfu: 12.20 | total time: 88.09m
step 00392/21400 (1.83%) | loss: 3.790805 | grad norm: 0.2138 | lrm: 1.00 | dt: 13869.34ms | tok/sec: 37,801 | mfu: 12.14 | total time: 88.32m
step 00393/21400 (1.84%) | loss: 3.807270 | grad norm: 0.2170 | lrm: 1.00 | dt: 13816.97ms | tok/sec: 37,945 | mfu: 12.19 | total time: 88.55m
step 00394/21400 (1.84%) | loss: 3.817933 | grad norm: 0.1863 | lrm: 1.00 | dt: 13900.88ms | tok/sec: 37,716 | mfu: 12.12 | total time: 88.78m
step 00395/21400 (1.85%) | loss: 3.835485 | grad norm: 0.2015 | lrm: 1.00 | dt: 13822.21ms | tok/sec: 37,930 | mfu: 12.19 | total time: 89.01m
step 00396/21400 (1.85%) | loss: 3.826810 | grad norm: 0.2345 | lrm: 1.00 | dt: 13870.89ms | tok/sec: 37,797 | mfu: 12.14 | total time: 89.24m
step 00397/21400 (1.86%) | loss: 3.863531 | grad norm: 0.2413 | lrm: 1.00 | dt: 13831.44ms | tok/sec: 37,905 | mfu: 12.18 | total time: 89.47m
step 00398/21400 (1.86%) | loss: 3.832965 | grad norm: 0.2218 | lrm: 1.00 | dt: 13855.37ms | tok/sec: 37,840 | mfu: 12.16 | total time: 89.71m
step 00399/21400 (1.86%) | loss: 3.814173 | grad norm: 0.1885 | lrm: 1.00 | dt: 13843.65ms | tok/sec: 37,872 | mfu: 12.17 | total time: 89.94m
step 00400/21400 (1.87%) | loss: 3.802847 | grad norm: 0.1717 | lrm: 1.00 | dt: 13849.65ms | tok/sec: 37,855 | mfu: 12.16 | total time: 90.17m
step 00401/21400 (1.87%) | loss: 3.795040 | grad norm: 0.1654 | lrm: 1.00 | dt: 13883.60ms | tok/sec: 37,763 | mfu: 12.13 | total time: 90.40m
step 00402/21400 (1.88%) | loss: 3.753320 | grad norm: 0.1804 | lrm: 1.00 | dt: 13786.12ms | tok/sec: 38,030 | mfu: 12.22 | total time: 90.63m
step 00403/21400 (1.88%) | loss: 3.719141 | grad norm: 0.2048 | lrm: 1.00 | dt: 14076.53ms | tok/sec: 37,245 | mfu: 11.97 | total time: 90.86m
step 00404/21400 (1.89%) | loss: 3.686337 | grad norm: 0.2014 | lrm: 1.00 | dt: 13797.85ms | tok/sec: 37,997 | mfu: 12.21 | total time: 91.09m
step 00405/21400 (1.89%) | loss: 3.693289 | grad norm: 0.1812 | lrm: 1.00 | dt: 13881.08ms | tok/sec: 37,769 | mfu: 12.13 | total time: 91.32m
step 00406/21400 (1.90%) | loss: 3.755101 | grad norm: 0.1783 | lrm: 1.00 | dt: 13803.02ms | tok/sec: 37,983 | mfu: 12.20 | total time: 91.55m
step 00407/21400 (1.90%) | loss: 3.773856 | grad norm: 0.2018 | lrm: 1.00 | dt: 13871.34ms | tok/sec: 37,796 | mfu: 12.14 | total time: 91.79m
step 00408/21400 (1.91%) | loss: 3.775819 | grad norm: 0.2051 | lrm: 1.00 | dt: 13811.00ms | tok/sec: 37,961 | mfu: 12.20 | total time: 92.02m
step 00409/21400 (1.91%) | loss: 3.813456 | grad norm: 0.1988 | lrm: 1.00 | dt: 13866.37ms | tok/sec: 37,810 | mfu: 12.15 | total time: 92.25m
step 00410/21400 (1.92%) | loss: 3.818782 | grad norm: 0.1898 | lrm: 1.00 | dt: 13826.35ms | tok/sec: 37,919 | mfu: 12.18 | total time: 92.48m
step 00411/21400 (1.92%) | loss: 3.805629 | grad norm: 0.1775 | lrm: 1.00 | dt: 13858.17ms | tok/sec: 37,832 | mfu: 12.15 | total time: 92.71m
step 00412/21400 (1.93%) | loss: 3.782319 | grad norm: 0.1813 | lrm: 1.00 | dt: 13840.32ms | tok/sec: 37,881 | mfu: 12.17 | total time: 92.94m
step 00413/21400 (1.93%) | loss: 3.796690 | grad norm: 0.1760 | lrm: 1.00 | dt: 13855.32ms | tok/sec: 37,840 | mfu: 12.16 | total time: 93.17m
step 00414/21400 (1.93%) | loss: 3.751892 | grad norm: 0.1842 | lrm: 1.00 | dt: 13842.23ms | tok/sec: 37,875 | mfu: 12.17 | total time: 93.40m
step 00415/21400 (1.94%) | loss: 3.730730 | grad norm: 0.1968 | lrm: 1.00 | dt: 13836.66ms | tok/sec: 37,891 | mfu: 12.17 | total time: 93.63m
step 00416/21400 (1.94%) | loss: 3.658838 | grad norm: 0.2183 | lrm: 1.00 | dt: 13883.43ms | tok/sec: 37,763 | mfu: 12.13 | total time: 93.86m
step 00417/21400 (1.95%) | loss: 3.638951 | grad norm: 0.2200 | lrm: 1.00 | dt: 13821.39ms | tok/sec: 37,933 | mfu: 12.19 | total time: 94.09m
step 00418/21400 (1.95%) | loss: 3.637779 | grad norm: 0.2100 | lrm: 1.00 | dt: 13877.46ms | tok/sec: 37,779 | mfu: 12.14 | total time: 94.32m
step 00419/21400 (1.96%) | loss: 3.611683 | grad norm: 0.2136 | lrm: 1.00 | dt: 13796.94ms | tok/sec: 38,000 | mfu: 12.21 | total time: 94.55m
step 00420/21400 (1.96%) | loss: 3.599729 | grad norm: 0.2123 | lrm: 1.00 | dt: 13897.70ms | tok/sec: 37,724 | mfu: 12.12 | total time: 94.79m
step 00421/21400 (1.97%) | loss: 3.617644 | grad norm: 0.2031 | lrm: 1.00 | dt: 13822.43ms | tok/sec: 37,930 | mfu: 12.19 | total time: 95.02m
step 00422/21400 (1.97%) | loss: 3.608778 | grad norm: 0.1977 | lrm: 1.00 | dt: 13872.46ms | tok/sec: 37,793 | mfu: 12.14 | total time: 95.25m
step 00423/21400 (1.98%) | loss: 3.630494 | grad norm: 0.2050 | lrm: 1.00 | dt: 13833.24ms | tok/sec: 37,900 | mfu: 12.18 | total time: 95.48m
step 00424/21400 (1.98%) | loss: 3.626577 | grad norm: 0.2047 | lrm: 1.00 | dt: 13855.11ms | tok/sec: 37,840 | mfu: 12.16 | total time: 95.71m
step 00425/21400 (1.99%) | loss: 3.641518 | grad norm: 0.1912 | lrm: 1.00 | dt: 13854.78ms | tok/sec: 37,841 | mfu: 12.16 | total time: 95.94m
step 00426/21400 (1.99%) | loss: 3.645675 | grad norm: 0.1935 | lrm: 1.00 | dt: 13878.39ms | tok/sec: 37,777 | mfu: 12.14 | total time: 96.17m
step 00427/21400 (2.00%) | loss: 3.636127 | grad norm: 0.1934 | lrm: 1.00 | dt: 13983.77ms | tok/sec: 37,492 | mfu: 12.04 | total time: 96.40m
step 00428/21400 (2.00%) | loss: 3.603571 | grad norm: 0.1732 | lrm: 1.00 | dt: 13821.93ms | tok/sec: 37,931 | mfu: 12.19 | total time: 96.63m
step 00429/21400 (2.00%) | loss: 3.640594 | grad norm: 0.1714 | lrm: 1.00 | dt: 13885.04ms | tok/sec: 37,759 | mfu: 12.13 | total time: 96.87m
step 00430/21400 (2.01%) | loss: 3.679170 | grad norm: 0.1913 | lrm: 1.00 | dt: 13794.84ms | tok/sec: 38,006 | mfu: 12.21 | total time: 97.10m
step 00431/21400 (2.01%) | loss: 3.639809 | grad norm: 0.1960 | lrm: 1.00 | dt: 13877.90ms | tok/sec: 37,778 | mfu: 12.14 | total time: 97.33m
step 00432/21400 (2.02%) | loss: 3.720416 | grad norm: 0.2257 | lrm: 1.00 | dt: 13803.26ms | tok/sec: 37,982 | mfu: 12.20 | total time: 97.56m
step 00433/21400 (2.02%) | loss: 3.692053 | grad norm: 0.2125 | lrm: 1.00 | dt: 13871.15ms | tok/sec: 37,797 | mfu: 12.14 | total time: 97.79m
step 00434/21400 (2.03%) | loss: 3.685716 | grad norm: 0.2065 | lrm: 1.00 | dt: 13814.57ms | tok/sec: 37,951 | mfu: 12.19 | total time: 98.02m
step 00435/21400 (2.03%) | loss: 3.682131 | grad norm: 0.2047 | lrm: 1.00 | dt: 13867.20ms | tok/sec: 37,807 | mfu: 12.15 | total time: 98.25m
step 00436/21400 (2.04%) | loss: 3.698423 | grad norm: 0.2195 | lrm: 1.00 | dt: 13826.06ms | tok/sec: 37,920 | mfu: 12.18 | total time: 98.48m
step 00437/21400 (2.04%) | loss: 3.765099 | grad norm: 0.2042 | lrm: 1.00 | dt: 13853.09ms | tok/sec: 37,846 | mfu: 12.16 | total time: 98.71m
step 00438/21400 (2.05%) | loss: 3.801560 | grad norm: 0.1988 | lrm: 1.00 | dt: 13836.73ms | tok/sec: 37,891 | mfu: 12.17 | total time: 98.94m
step 00439/21400 (2.05%) | loss: 3.790389 | grad norm: 0.1975 | lrm: 1.00 | dt: 13838.79ms | tok/sec: 37,885 | mfu: 12.17 | total time: 99.17m
step 00440/21400 (2.06%) | loss: 3.746781 | grad norm: 0.1823 | lrm: 1.00 | dt: 13868.46ms | tok/sec: 37,804 | mfu: 12.14 | total time: 99.40m
step 00441/21400 (2.06%) | loss: 3.768769 | grad norm: 0.1826 | lrm: 1.00 | dt: 13827.19ms | tok/sec: 37,917 | mfu: 12.18 | total time: 99.63m
step 00442/21400 (2.07%) | loss: 3.755455 | grad norm: 0.1787 | lrm: 1.00 | dt: 13880.79ms | tok/sec: 37,770 | mfu: 12.13 | total time: 99.86m
step 00443/21400 (2.07%) | loss: 3.743877 | grad norm: 0.1672 | lrm: 1.00 | dt: 13803.16ms | tok/sec: 37,983 | mfu: 12.20 | total time: 100.09m
step 00444/21400 (2.07%) | loss: 3.694279 | grad norm: 0.1745 | lrm: 1.00 | dt: 13880.73ms | tok/sec: 37,770 | mfu: 12.13 | total time: 100.33m
step 00445/21400 (2.08%) | loss: 3.681718 | grad norm: 0.1693 | lrm: 1.00 | dt: 13811.79ms | tok/sec: 37,959 | mfu: 12.19 | total time: 100.56m
step 00446/21400 (2.08%) | loss: 3.692018 | grad norm: 0.1730 | lrm: 1.00 | dt: 13868.40ms | tok/sec: 37,804 | mfu: 12.14 | total time: 100.79m
step 00447/21400 (2.09%) | loss: 3.710584 | grad norm: 0.1801 | lrm: 1.00 | dt: 13811.95ms | tok/sec: 37,959 | mfu: 12.19 | total time: 101.02m
step 00448/21400 (2.09%) | loss: 3.735771 | grad norm: 0.1942 | lrm: 1.00 | dt: 13863.80ms | tok/sec: 37,817 | mfu: 12.15 | total time: 101.25m
step 00449/21400 (2.10%) | loss: 3.756204 | grad norm: 0.2078 | lrm: 1.00 | dt: 13822.95ms | tok/sec: 37,928 | mfu: 12.18 | total time: 101.48m
step 00450/21400 (2.10%) | loss: 3.780710 | grad norm: 0.2049 | lrm: 1.00 | dt: 13858.78ms | tok/sec: 37,830 | mfu: 12.15 | total time: 101.71m
step 00451/21400 (2.11%) | loss: 3.795752 | grad norm: 0.1984 | lrm: 1.00 | dt: 13841.85ms | tok/sec: 37,877 | mfu: 12.17 | total time: 101.94m
step 00452/21400 (2.11%) | loss: 3.792810 | grad norm: 0.2065 | lrm: 1.00 | dt: 14026.19ms | tok/sec: 37,379 | mfu: 12.01 | total time: 102.17m
step 00453/21400 (2.12%) | loss: 3.794368 | grad norm: 0.2050 | lrm: 1.00 | dt: 13859.52ms | tok/sec: 37,828 | mfu: 12.15 | total time: 102.41m
step 00454/21400 (2.12%) | loss: 3.774657 | grad norm: 0.1920 | lrm: 1.00 | dt: 13827.17ms | tok/sec: 37,917 | mfu: 12.18 | total time: 102.64m
step 00455/21400 (2.13%) | loss: 3.745163 | grad norm: 0.1740 | lrm: 1.00 | dt: 13887.21ms | tok/sec: 37,753 | mfu: 12.13 | total time: 102.87m
step 00456/21400 (2.13%) | loss: 3.726338 | grad norm: 0.1763 | lrm: 1.00 | dt: 13806.65ms | tok/sec: 37,973 | mfu: 12.20 | total time: 103.10m
step 00457/21400 (2.14%) | loss: 3.762376 | grad norm: 0.1791 | lrm: 1.00 | dt: 13886.14ms | tok/sec: 37,756 | mfu: 12.13 | total time: 103.33m
step 00458/21400 (2.14%) | loss: 3.730945 | grad norm: 0.1877 | lrm: 1.00 | dt: 13815.48ms | tok/sec: 37,949 | mfu: 12.19 | total time: 103.56m
step 00459/21400 (2.14%) | loss: 3.709961 | grad norm: 0.1938 | lrm: 1.00 | dt: 13879.45ms | tok/sec: 37,774 | mfu: 12.14 | total time: 103.79m
step 00460/21400 (2.15%) | loss: 3.640621 | grad norm: 0.1942 | lrm: 1.00 | dt: 13808.95ms | tok/sec: 37,967 | mfu: 12.20 | total time: 104.02m
step 00461/21400 (2.15%) | loss: 3.613824 | grad norm: 0.1946 | lrm: 1.00 | dt: 13881.92ms | tok/sec: 37,767 | mfu: 12.13 | total time: 104.25m
step 00462/21400 (2.16%) | loss: 3.560089 | grad norm: 0.2007 | lrm: 1.00 | dt: 13820.89ms | tok/sec: 37,934 | mfu: 12.19 | total time: 104.48m
step 00463/21400 (2.16%) | loss: 3.607210 | grad norm: 0.1890 | lrm: 1.00 | dt: 13859.41ms | tok/sec: 37,829 | mfu: 12.15 | total time: 104.71m
step 00464/21400 (2.17%) | loss: 3.588601 | grad norm: 0.1757 | lrm: 1.00 | dt: 13841.76ms | tok/sec: 37,877 | mfu: 12.17 | total time: 104.94m
step 00465/21400 (2.17%) | loss: 3.622089 | grad norm: 0.1873 | lrm: 1.00 | dt: 13837.21ms | tok/sec: 37,889 | mfu: 12.17 | total time: 105.17m
step 00466/21400 (2.18%) | loss: 3.635816 | grad norm: 0.1799 | lrm: 1.00 | dt: 13858.00ms | tok/sec: 37,832 | mfu: 12.15 | total time: 105.41m
step 00467/21400 (2.18%) | loss: 3.646331 | grad norm: 0.1844 | lrm: 1.00 | dt: 13835.38ms | tok/sec: 37,894 | mfu: 12.17 | total time: 105.64m
step 00468/21400 (2.19%) | loss: 3.659696 | grad norm: 0.1960 | lrm: 1.00 | dt: 13892.85ms | tok/sec: 37,737 | mfu: 12.12 | total time: 105.87m
step 00469/21400 (2.19%) | loss: 3.703723 | grad norm: 0.2013 | lrm: 1.00 | dt: 13799.42ms | tok/sec: 37,993 | mfu: 12.21 | total time: 106.10m
step 00470/21400 (2.20%) | loss: 3.706902 | grad norm: 0.1845 | lrm: 1.00 | dt: 13876.18ms | tok/sec: 37,783 | mfu: 12.14 | total time: 106.33m
step 00471/21400 (2.20%) | loss: 3.724372 | grad norm: 0.1616 | lrm: 1.00 | dt: 13805.29ms | tok/sec: 37,977 | mfu: 12.20 | total time: 106.56m
step 00472/21400 (2.21%) | loss: 3.755502 | grad norm: 0.1643 | lrm: 1.00 | dt: 13888.48ms | tok/sec: 37,749 | mfu: 12.13 | total time: 106.79m
step 00473/21400 (2.21%) | loss: 3.702280 | grad norm: 0.1706 | lrm: 1.00 | dt: 13808.81ms | tok/sec: 37,967 | mfu: 12.20 | total time: 107.02m
step 00474/21400 (2.21%) | loss: 3.751705 | grad norm: 0.1781 | lrm: 1.00 | dt: 13870.50ms | tok/sec: 37,798 | mfu: 12.14 | total time: 107.25m
step 00475/21400 (2.22%) | loss: 3.732505 | grad norm: 0.1900 | lrm: 1.00 | dt: 13822.11ms | tok/sec: 37,931 | mfu: 12.19 | total time: 107.48m
step 00476/21400 (2.22%) | loss: 3.698602 | grad norm: 0.1758 | lrm: 1.00 | dt: 13860.84ms | tok/sec: 37,825 | mfu: 12.15 | total time: 107.71m
step 00477/21400 (2.23%) | loss: 3.692556 | grad norm: 0.1789 | lrm: 1.00 | dt: 14000.65ms | tok/sec: 37,447 | mfu: 12.03 | total time: 107.95m
step 00478/21400 (2.23%) | loss: 3.709958 | grad norm: 0.1794 | lrm: 1.00 | dt: 13845.74ms | tok/sec: 37,866 | mfu: 12.16 | total time: 108.18m
step 00479/21400 (2.24%) | loss: 3.757942 | grad norm: 0.1861 | lrm: 1.00 | dt: 13846.10ms | tok/sec: 37,865 | mfu: 12.16 | total time: 108.41m
step 00480/21400 (2.24%) | loss: 3.704567 | grad norm: 0.1852 | lrm: 1.00 | dt: 13817.95ms | tok/sec: 37,942 | mfu: 12.19 | total time: 108.64m
step 00481/21400 (2.25%) | loss: 3.659952 | grad norm: 0.1884 | lrm: 1.00 | dt: 13884.08ms | tok/sec: 37,761 | mfu: 12.13 | total time: 108.87m
step 00482/21400 (2.25%) | loss: 3.673239 | grad norm: 0.1909 | lrm: 1.00 | dt: 13872.54ms | tok/sec: 37,793 | mfu: 12.14 | total time: 109.10m
step 00483/21400 (2.26%) | loss: 3.649406 | grad norm: 0.1780 | lrm: 1.00 | dt: 13785.84ms | tok/sec: 38,030 | mfu: 12.22 | total time: 109.33m
step 00484/21400 (2.26%) | loss: 3.642007 | grad norm: 0.1674 | lrm: 1.00 | dt: 13827.38ms | tok/sec: 37,916 | mfu: 12.18 | total time: 109.56m
step 00485/21400 (2.27%) | loss: 3.630751 | grad norm: 0.2031 | lrm: 1.00 | dt: 13883.20ms | tok/sec: 37,764 | mfu: 12.13 | total time: 109.79m
step 00486/21400 (2.27%) | loss: 3.659404 | grad norm: 0.2139 | lrm: 1.00 | dt: 13796.79ms | tok/sec: 38,000 | mfu: 12.21 | total time: 110.02m
step 00487/21400 (2.28%) | loss: 3.633747 | grad norm: 0.2059 | lrm: 1.00 | dt: 13876.57ms | tok/sec: 37,782 | mfu: 12.14 | total time: 110.25m
step 00488/21400 (2.28%) | loss: 3.604412 | grad norm: 0.2098 | lrm: 1.00 | dt: 13809.32ms | tok/sec: 37,966 | mfu: 12.20 | total time: 110.48m
step 00489/21400 (2.29%) | loss: 3.642206 | grad norm: 0.2122 | lrm: 1.00 | dt: 13878.74ms | tok/sec: 37,776 | mfu: 12.14 | total time: 110.72m
step 00490/21400 (2.29%) | loss: 3.735382 | grad norm: 0.2068 | lrm: 1.00 | dt: 13830.43ms | tok/sec: 37,908 | mfu: 12.18 | total time: 110.95m
step 00491/21400 (2.29%) | loss: 3.663504 | grad norm: 0.1968 | lrm: 1.00 | dt: 13847.62ms | tok/sec: 37,861 | mfu: 12.16 | total time: 111.18m
step 00492/21400 (2.30%) | loss: 3.702270 | grad norm: 0.2012 | lrm: 1.00 | dt: 13854.16ms | tok/sec: 37,843 | mfu: 12.16 | total time: 111.41m
step 00493/21400 (2.30%) | loss: 3.718494 | grad norm: 0.2076 | lrm: 1.00 | dt: 13840.14ms | tok/sec: 37,881 | mfu: 12.17 | total time: 111.64m
step 00494/21400 (2.31%) | loss: 3.698258 | grad norm: 0.1900 | lrm: 1.00 | dt: 13893.63ms | tok/sec: 37,735 | mfu: 12.12 | total time: 111.87m
step 00495/21400 (2.31%) | loss: 3.632277 | grad norm: 0.1819 | lrm: 1.00 | dt: 13789.96ms | tok/sec: 38,019 | mfu: 12.21 | total time: 112.10m
step 00496/21400 (2.32%) | loss: 3.607337 | grad norm: 0.1938 | lrm: 1.00 | dt: 13889.05ms | tok/sec: 37,748 | mfu: 12.13 | total time: 112.33m
step 00497/21400 (2.32%) | loss: 3.622443 | grad norm: 0.1824 | lrm: 1.00 | dt: 13803.28ms | tok/sec: 37,982 | mfu: 12.20 | total time: 112.56m
step 00498/21400 (2.33%) | loss: 3.657201 | grad norm: 0.1727 | lrm: 1.00 | dt: 13888.50ms | tok/sec: 37,749 | mfu: 12.13 | total time: 112.79m
step 00499/21400 (2.33%) | loss: 3.736659 | grad norm: 0.1741 | lrm: 1.00 | dt: 13808.29ms | tok/sec: 37,969 | mfu: 12.20 | total time: 113.02m
Step 00500 | Validation bpb: 1.0845
step 00500/21400 (2.34%) | loss: 3.728649 | grad norm: 0.1866 | lrm: 1.00 | dt: 14045.02ms | tok/sec: 37,329 | mfu: 11.99 | total time: 113.26m
2025-11-11 01:00:51,302 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model file to: /home/henny/.cache/nanochat/base_checkpoints/d20/model_000500.pt
2025-11-11 01:00:55,628 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer file to: /home/henny/.cache/nanochat/base_checkpoints/d20/optim_000500.pt
2025-11-11 01:00:55,629 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata file to: /home/henny/.cache/nanochat/base_checkpoints/d20/meta_000500.json
step 00501/21400 (2.34%) | loss: 3.730062 | grad norm: 0.1684 | lrm: 1.00 | dt: 14128.84ms | tok/sec: 37,107 | mfu: 11.92 | total time: 113.49m
step 00502/21400 (2.35%) | loss: 3.748538 | grad norm: 0.1732 | lrm: 1.00 | dt: 13891.12ms | tok/sec: 37,742 | mfu: 12.12 | total time: 113.72m
step 00503/21400 (2.35%) | loss: 3.694789 | grad norm: 0.1790 | lrm: 1.00 | dt: 13789.10ms | tok/sec: 38,021 | mfu: 12.21 | total time: 113.95m
step 00504/21400 (2.36%) | loss: 3.708907 | grad norm: 0.1757 | lrm: 1.00 | dt: 13881.30ms | tok/sec: 37,769 | mfu: 12.13 | total time: 114.19m
step 00505/21400 (2.36%) | loss: 3.674172 | grad norm: 0.1933 | lrm: 1.00 | dt: 13807.98ms | tok/sec: 37,969 | mfu: 12.20 | total time: 114.42m
step 00506/21400 (2.36%) | loss: 3.701933 | grad norm: 0.2011 | lrm: 1.00 | dt: 13869.03ms | tok/sec: 37,802 | mfu: 12.14 | total time: 114.65m
step 00507/21400 (2.37%) | loss: 3.684770 | grad norm: 0.1916 | lrm: 1.00 | dt: 13822.68ms | tok/sec: 37,929 | mfu: 12.18 | total time: 114.88m
step 00508/21400 (2.37%) | loss: 3.664245 | grad norm: 0.1738 | lrm: 1.00 | dt: 13854.56ms | tok/sec: 37,842 | mfu: 12.16 | total time: 115.11m
step 00509/21400 (2.38%) | loss: 3.654313 | grad norm: 0.1713 | lrm: 1.00 | dt: 13848.68ms | tok/sec: 37,858 | mfu: 12.16 | total time: 115.34m
step 00510/21400 (2.38%) | loss: 3.689381 | grad norm: 0.1800 | lrm: 1.00 | dt: 13834.25ms | tok/sec: 37,897 | mfu: 12.17 | total time: 115.57m
step 00511/21400 (2.39%) | loss: 3.717461 | grad norm: 0.1835 | lrm: 1.00 | dt: 13883.81ms | tok/sec: 37,762 | mfu: 12.13 | total time: 115.80m
step 00512/21400 (2.39%) | loss: 3.687907 | grad norm: 0.1802 | lrm: 1.00 | dt: 13793.62ms | tok/sec: 38,009 | mfu: 12.21 | total time: 116.03m
step 00513/21400 (2.40%) | loss: 3.719170 | grad norm: 0.1804 | lrm: 1.00 | dt: 13905.45ms | tok/sec: 37,703 | mfu: 12.11 | total time: 116.26m
step 00514/21400 (2.40%) | loss: 3.712933 | grad norm: 0.1836 | lrm: 1.00 | dt: 13793.66ms | tok/sec: 38,009 | mfu: 12.21 | total time: 116.49m
step 00515/21400 (2.41%) | loss: 3.724436 | grad norm: 0.1759 | lrm: 1.00 | dt: 13884.01ms | tok/sec: 37,762 | mfu: 12.13 | total time: 116.72m
step 00516/21400 (2.41%) | loss: 3.701068 | grad norm: 0.2073 | lrm: 1.00 | dt: 13803.24ms | tok/sec: 37,982 | mfu: 12.20 | total time: 116.95m
step 00517/21400 (2.42%) | loss: 3.740756 | grad norm: 0.2133 | lrm: 1.00 | dt: 13876.09ms | tok/sec: 37,783 | mfu: 12.14 | total time: 117.18m
step 00518/21400 (2.42%) | loss: 3.756975 | grad norm: 0.1977 | lrm: 1.00 | dt: 13807.45ms | tok/sec: 37,971 | mfu: 12.20 | total time: 117.41m
step 00519/21400 (2.43%) | loss: 3.715005 | grad norm: 0.1975 | lrm: 1.00 | dt: 13864.93ms | tok/sec: 37,813 | mfu: 12.15 | total time: 117.65m
step 00520/21400 (2.43%) | loss: 3.695266 | grad norm: 0.1892 | lrm: 1.00 | dt: 13844.43ms | tok/sec: 37,869 | mfu: 12.17 | total time: 117.88m
step 00521/21400 (2.43%) | loss: 3.693295 | grad norm: 0.1914 | lrm: 1.00 | dt: 13858.07ms | tok/sec: 37,832 | mfu: 12.15 | total time: 118.11m
step 00522/21400 (2.44%) | loss: 3.685362 | grad norm: 0.1875 | lrm: 1.00 | dt: 13860.18ms | tok/sec: 37,826 | mfu: 12.15 | total time: 118.34m
step 00523/21400 (2.44%) | loss: 3.680314 | grad norm: 0.1677 | lrm: 1.00 | dt: 13889.26ms | tok/sec: 37,747 | mfu: 12.13 | total time: 118.57m
step 00524/21400 (2.45%) | loss: 3.675431 | grad norm: 0.1761 | lrm: 1.00 | dt: 13786.68ms | tok/sec: 38,028 | mfu: 12.22 | total time: 118.80m
step 00525/21400 (2.45%) | loss: 3.721679 | grad norm: 0.1843 | lrm: 1.00 | dt: 13822.45ms | tok/sec: 37,930 | mfu: 12.19 | total time: 119.03m
step 00526/21400 (2.46%) | loss: 3.695944 | grad norm: 0.1879 | lrm: 1.00 | dt: 14103.60ms | tok/sec: 37,174 | mfu: 11.94 | total time: 119.27m
step 00527/21400 (2.46%) | loss: 3.683333 | grad norm: 0.1958 | lrm: 1.00 | dt: 13799.72ms | tok/sec: 37,992 | mfu: 12.21 | total time: 119.50m
step 00528/21400 (2.47%) | loss: 3.640442 | grad norm: 0.1905 | lrm: 1.00 | dt: 13891.70ms | tok/sec: 37,741 | mfu: 12.12 | total time: 119.73m
step 00529/21400 (2.47%) | loss: 3.578441 | grad norm: 0.1973 | lrm: 1.00 | dt: 13792.00ms | tok/sec: 38,013 | mfu: 12.21 | total time: 119.96m
step 00530/21400 (2.48%) | loss: 3.598989 | grad norm: 0.1998 | lrm: 1.00 | dt: 13876.32ms | tok/sec: 37,782 | mfu: 12.14 | total time: 120.19m
step 00531/21400 (2.48%) | loss: 3.586475 | grad norm: 0.1905 | lrm: 1.00 | dt: 13804.26ms | tok/sec: 37,980 | mfu: 12.20 | total time: 120.42m
step 00532/21400 (2.49%) | loss: 3.576884 | grad norm: 0.1855 | lrm: 1.00 | dt: 13867.75ms | tok/sec: 37,806 | mfu: 12.15 | total time: 120.65m
step 00533/21400 (2.49%) | loss: 3.577974 | grad norm: 0.1927 | lrm: 1.00 | dt: 13817.57ms | tok/sec: 37,943 | mfu: 12.19 | total time: 120.88m
step 00534/21400 (2.50%) | loss: 3.566049 | grad norm: 0.1963 | lrm: 1.00 | dt: 13854.30ms | tok/sec: 37,842 | mfu: 12.16 | total time: 121.11m
step 00535/21400 (2.50%) | loss: 3.601146 | grad norm: 0.1796 | lrm: 1.00 | dt: 13835.94ms | tok/sec: 37,893 | mfu: 12.17 | total time: 121.34m
step 00536/21400 (2.50%) | loss: 3.577264 | grad norm: 0.1812 | lrm: 1.00 | dt: 13835.24ms | tok/sec: 37,895 | mfu: 12.17 | total time: 121.57m
step 00537/21400 (2.51%) | loss: 3.585332 | grad norm: 0.1876 | lrm: 1.00 | dt: 13852.71ms | tok/sec: 37,847 | mfu: 12.16 | total time: 121.80m
step 00538/21400 (2.51%) | loss: 3.594705 | grad norm: 0.1952 | lrm: 1.00 | dt: 13824.45ms | tok/sec: 37,924 | mfu: 12.18 | total time: 122.03m
step 00539/21400 (2.52%) | loss: 3.579232 | grad norm: 0.1928 | lrm: 1.00 | dt: 13890.70ms | tok/sec: 37,743 | mfu: 12.13 | total time: 122.26m
step 00540/21400 (2.52%) | loss: 3.608551 | grad norm: 0.1858 | lrm: 1.00 | dt: 13797.77ms | tok/sec: 37,998 | mfu: 12.21 | total time: 122.49m
step 00541/21400 (2.53%) | loss: 3.558563 | grad norm: 0.1969 | lrm: 1.00 | dt: 13883.39ms | tok/sec: 37,763 | mfu: 12.13 | total time: 122.73m
step 00542/21400 (2.53%) | loss: 3.564996 | grad norm: 0.2012 | lrm: 1.00 | dt: 13798.73ms | tok/sec: 37,995 | mfu: 12.21 | total time: 122.96m
step 00543/21400 (2.54%) | loss: 3.584624 | grad norm: 0.1846 | lrm: 1.00 | dt: 13884.80ms | tok/sec: 37,759 | mfu: 12.13 | total time: 123.19m
step 00544/21400 (2.54%) | loss: 3.559982 | grad norm: 0.1850 | lrm: 1.00 | dt: 13804.36ms | tok/sec: 37,979 | mfu: 12.20 | total time: 123.42m
step 00545/21400 (2.55%) | loss: 3.564943 | grad norm: 0.1831 | lrm: 1.00 | dt: 13879.22ms | tok/sec: 37,775 | mfu: 12.14 | total time: 123.65m
step 00546/21400 (2.55%) | loss: 3.594519 | grad norm: 0.1623 | lrm: 1.00 | dt: 13815.76ms | tok/sec: 37,948 | mfu: 12.19 | total time: 123.88m
step 00547/21400 (2.56%) | loss: 3.632096 | grad norm: 0.1631 | lrm: 1.00 | dt: 13856.33ms | tok/sec: 37,837 | mfu: 12.16 | total time: 124.11m
step 00548/21400 (2.56%) | loss: 3.624998 | grad norm: 0.1809 | lrm: 1.00 | dt: 13833.50ms | tok/sec: 37,899 | mfu: 12.18 | total time: 124.34m
step 00549/21400 (2.57%) | loss: 3.618694 | grad norm: 0.1913 | lrm: 1.00 | dt: 13850.23ms | tok/sec: 37,854 | mfu: 12.16 | total time: 124.57m
step 00550/21400 (2.57%) | loss: 3.678614 | grad norm: 0.2007 | lrm: 1.00 | dt: 13845.48ms | tok/sec: 37,867 | mfu: 12.16 | total time: 124.80m
step 00551/21400 (2.57%) | loss: 3.684397 | grad norm: 0.1930 | lrm: 1.00 | dt: 13824.77ms | tok/sec: 37,923 | mfu: 12.18 | total time: 125.03m
step 00552/21400 (2.58%) | loss: 3.683198 | grad norm: 0.1863 | lrm: 1.00 | dt: 14108.82ms | tok/sec: 37,160 | mfu: 11.94 | total time: 125.27m
step 00553/21400 (2.58%) | loss: 3.662349 | grad norm: 0.1813 | lrm: 1.00 | dt: 13775.51ms | tok/sec: 38,059 | mfu: 12.23 | total time: 125.50m
step 00554/21400 (2.59%) | loss: 3.658072 | grad norm: 0.1939 | lrm: 1.00 | dt: 13899.43ms | tok/sec: 37,720 | mfu: 12.12 | total time: 125.73m
step 00555/21400 (2.59%) | loss: 3.647202 | grad norm: 0.2227 | lrm: 1.00 | dt: 13785.63ms | tok/sec: 38,031 | mfu: 12.22 | total time: 125.96m
step 00556/21400 (2.60%) | loss: 3.642863 | grad norm: 0.1963 | lrm: 1.00 | dt: 13875.85ms | tok/sec: 37,784 | mfu: 12.14 | total time: 126.19m
step 00557/21400 (2.60%) | loss: 3.640774 | grad norm: 0.1794 | lrm: 1.00 | dt: 13809.76ms | tok/sec: 37,965 | mfu: 12.20 | total time: 126.42m
step 00558/21400 (2.61%) | loss: 3.601500 | grad norm: 0.1784 | lrm: 1.00 | dt: 13887.69ms | tok/sec: 37,751 | mfu: 12.13 | total time: 126.65m
step 00559/21400 (2.61%) | loss: 3.608926 | grad norm: 0.1876 | lrm: 1.00 | dt: 13818.25ms | tok/sec: 37,941 | mfu: 12.19 | total time: 126.88m
step 00560/21400 (2.62%) | loss: 3.609927 | grad norm: 0.2021 | lrm: 1.00 | dt: 13863.40ms | tok/sec: 37,818 | mfu: 12.15 | total time: 127.11m
step 00561/21400 (2.62%) | loss: 3.600808 | grad norm: 0.1852 | lrm: 1.00 | dt: 13832.51ms | tok/sec: 37,902 | mfu: 12.18 | total time: 127.34m
step 00562/21400 (2.63%) | loss: 3.602292 | grad norm: 0.1772 | lrm: 1.00 | dt: 13856.07ms | tok/sec: 37,838 | mfu: 12.16 | total time: 127.57m
step 00563/21400 (2.63%) | loss: 3.623693 | grad norm: 0.1871 | lrm: 1.00 | dt: 13856.87ms | tok/sec: 37,835 | mfu: 12.15 | total time: 127.81m
step 00564/21400 (2.64%) | loss: 3.619061 | grad norm: 0.1709 | lrm: 1.00 | dt: 13822.32ms | tok/sec: 37,930 | mfu: 12.19 | total time: 128.04m
step 00565/21400 (2.64%) | loss: 3.604858 | grad norm: 0.1698 | lrm: 1.00 | dt: 13893.89ms | tok/sec: 37,735 | mfu: 12.12 | total time: 128.27m
step 00566/21400 (2.64%) | loss: 3.592162 | grad norm: 0.1742 | lrm: 1.00 | dt: 13865.14ms | tok/sec: 37,813 | mfu: 12.15 | total time: 128.50m
step 00567/21400 (2.65%) | loss: 3.590599 | grad norm: 0.1623 | lrm: 1.00 | dt: 13767.70ms | tok/sec: 38,081 | mfu: 12.23 | total time: 128.73m
step 00568/21400 (2.65%) | loss: 3.592198 | grad norm: 0.1755 | lrm: 1.00 | dt: 13820.77ms | tok/sec: 37,934 | mfu: 12.19 | total time: 128.96m
step 00569/21400 (2.66%) | loss: 3.583402 | grad norm: 0.1771 | lrm: 1.00 | dt: 13880.85ms | tok/sec: 37,770 | mfu: 12.13 | total time: 129.19m
step 00570/21400 (2.66%) | loss: 3.621404 | grad norm: 0.1787 | lrm: 1.00 | dt: 13799.13ms | tok/sec: 37,994 | mfu: 12.21 | total time: 129.42m
step 00571/21400 (2.67%) | loss: 3.638165 | grad norm: 0.1930 | lrm: 1.00 | dt: 13873.86ms | tok/sec: 37,789 | mfu: 12.14 | total time: 129.65m
step 00572/21400 (2.67%) | loss: 3.576909 | grad norm: 0.1823 | lrm: 1.00 | dt: 13826.93ms | tok/sec: 37,917 | mfu: 12.18 | total time: 129.88m
step 00573/21400 (2.68%) | loss: 3.602028 | grad norm: 0.1834 | lrm: 1.00 | dt: 13865.06ms | tok/sec: 37,813 | mfu: 12.15 | total time: 130.11m
step 00574/21400 (2.68%) | loss: 3.628710 | grad norm: 0.1963 | lrm: 1.00 | dt: 13823.41ms | tok/sec: 37,927 | mfu: 12.18 | total time: 130.34m
step 00575/21400 (2.69%) | loss: 3.589432 | grad norm: 0.2055 | lrm: 1.00 | dt: 13864.77ms | tok/sec: 37,814 | mfu: 12.15 | total time: 130.57m
step 00576/21400 (2.69%) | loss: 3.571381 | grad norm: 0.1852 | lrm: 1.00 | dt: 13859.59ms | tok/sec: 37,828 | mfu: 12.15 | total time: 130.80m
step 00577/21400 (2.70%) | loss: 3.577305 | grad norm: 0.1876 | lrm: 1.00 | dt: 13827.28ms | tok/sec: 37,916 | mfu: 12.18 | total time: 131.04m
step 00578/21400 (2.70%) | loss: 3.573119 | grad norm: 0.1878 | lrm: 1.00 | dt: 13891.80ms | tok/sec: 37,740 | mfu: 12.12 | total time: 131.27m
step 00579/21400 (2.71%) | loss: 3.498494 | grad norm: 0.1787 | lrm: 1.00 | dt: 13986.68ms | tok/sec: 37,484 | mfu: 12.04 | total time: 131.50m
step 00580/21400 (2.71%) | loss: 3.505997 | grad norm: 0.1788 | lrm: 1.00 | dt: 13891.88ms | tok/sec: 37,740 | mfu: 12.12 | total time: 131.73m
step 00581/21400 (2.71%) | loss: 3.550357 | grad norm: 0.1759 | lrm: 1.00 | dt: 13793.94ms | tok/sec: 38,008 | mfu: 12.21 | total time: 131.96m
step 00582/21400 (2.72%) | loss: 3.532680 | grad norm: 0.1724 | lrm: 1.00 | dt: 13882.85ms | tok/sec: 37,765 | mfu: 12.13 | total time: 132.19m
step 00583/21400 (2.72%) | loss: 3.526834 | grad norm: 0.1811 | lrm: 1.00 | dt: 13810.52ms | tok/sec: 37,962 | mfu: 12.20 | total time: 132.42m
step 00584/21400 (2.73%) | loss: 3.523642 | grad norm: 0.1902 | lrm: 1.00 | dt: 13874.88ms | tok/sec: 37,786 | mfu: 12.14 | total time: 132.65m
step 00585/21400 (2.73%) | loss: 3.499578 | grad norm: 0.1956 | lrm: 1.00 | dt: 13813.28ms | tok/sec: 37,955 | mfu: 12.19 | total time: 132.88m
step 00586/21400 (2.74%) | loss: 3.521827 | grad norm: 0.2111 | lrm: 1.00 | dt: 13870.56ms | tok/sec: 37,798 | mfu: 12.14 | total time: 133.12m
step 00587/21400 (2.74%) | loss: 3.588150 | grad norm: 0.2136 | lrm: 1.00 | dt: 13830.98ms | tok/sec: 37,906 | mfu: 12.18 | total time: 133.35m
step 00588/21400 (2.75%) | loss: 3.613180 | grad norm: 0.2108 | lrm: 1.00 | dt: 13842.87ms | tok/sec: 37,874 | mfu: 12.17 | total time: 133.58m
step 00589/21400 (2.75%) | loss: 3.622573 | grad norm: 0.1904 | lrm: 1.00 | dt: 13866.56ms | tok/sec: 37,809 | mfu: 12.15 | total time: 133.81m
step 00590/21400 (2.76%) | loss: 3.618580 | grad norm: 0.1651 | lrm: 1.00 | dt: 13830.29ms | tok/sec: 37,908 | mfu: 12.18 | total time: 134.04m
step 00591/21400 (2.76%) | loss: 3.607362 | grad norm: 0.1645 | lrm: 1.00 | dt: 13894.76ms | tok/sec: 37,732 | mfu: 12.12 | total time: 134.27m
step 00592/21400 (2.77%) | loss: 3.569767 | grad norm: 0.1759 | lrm: 1.00 | dt: 13863.41ms | tok/sec: 37,818 | mfu: 12.15 | total time: 134.50m
step 00593/21400 (2.77%) | loss: 3.567855 | grad norm: 0.1698 | lrm: 1.00 | dt: 13765.66ms | tok/sec: 38,086 | mfu: 12.24 | total time: 134.73m
step 00594/21400 (2.78%) | loss: 3.524044 | grad norm: 0.1738 | lrm: 1.00 | dt: 13836.46ms | tok/sec: 37,891 | mfu: 12.17 | total time: 134.96m
step 00595/21400 (2.78%) | loss: 3.499434 | grad norm: 0.1773 | lrm: 1.00 | dt: 13897.84ms | tok/sec: 37,724 | mfu: 12.12 | total time: 135.19m
step 00596/21400 (2.79%) | loss: 3.443316 | grad norm: 0.2133 | lrm: 1.00 | dt: 13790.76ms | tok/sec: 38,017 | mfu: 12.21 | total time: 135.42m
step 00597/21400 (2.79%) | loss: 3.456633 | grad norm: 0.1857 | lrm: 1.00 | dt: 13864.06ms | tok/sec: 37,816 | mfu: 12.15 | total time: 135.65m
step 00598/21400 (2.79%) | loss: 3.488197 | grad norm: 0.1653 | lrm: 1.00 | dt: 13814.19ms | tok/sec: 37,952 | mfu: 12.19 | total time: 135.88m
step 00599/21400 (2.80%) | loss: 3.512629 | grad norm: 0.1877 | lrm: 1.00 | dt: 13852.98ms | tok/sec: 37,846 | mfu: 12.16 | total time: 136.11m
step 00600/21400 (2.80%) | loss: 3.502726 | grad norm: 0.1805 | lrm: 1.00 | dt: 13832.60ms | tok/sec: 37,902 | mfu: 12.18 | total time: 136.35m
step 00601/21400 (2.81%) | loss: 3.519414 | grad norm: 0.1965 | lrm: 1.00 | dt: 13840.90ms | tok/sec: 37,879 | mfu: 12.17 | total time: 136.58m
step 00602/21400 (2.81%) | loss: 3.530910 | grad norm: 0.1783 | lrm: 1.00 | dt: 13854.56ms | tok/sec: 37,842 | mfu: 12.16 | total time: 136.81m
step 00603/21400 (2.82%) | loss: 3.541929 | grad norm: 0.1804 | lrm: 1.00 | dt: 13827.46ms | tok/sec: 37,916 | mfu: 12.18 | total time: 137.04m
step 00604/21400 (2.82%) | loss: 3.526576 | grad norm: 0.1928 | lrm: 1.00 | dt: 13891.52ms | tok/sec: 37,741 | mfu: 12.12 | total time: 137.27m
step 00605/21400 (2.83%) | loss: 3.542640 | grad norm: 0.1703 | lrm: 1.00 | dt: 13791.40ms | tok/sec: 38,015 | mfu: 12.21 | total time: 137.50m
step 00606/21400 (2.83%) | loss: 3.604801 | grad norm: 0.1586 | lrm: 1.00 | dt: 14097.10ms | tok/sec: 37,191 | mfu: 11.95 | total time: 137.73m
step 00607/21400 (2.84%) | loss: 3.600449 | grad norm: 0.1767 | lrm: 1.00 | dt: 13786.24ms | tok/sec: 38,029 | mfu: 12.22 | total time: 137.96m
step 00608/21400 (2.84%) | loss: 3.618705 | grad norm: 0.1720 | lrm: 1.00 | dt: 13883.81ms | tok/sec: 37,762 | mfu: 12.13 | total time: 138.19m
step 00609/21400 (2.85%) | loss: 3.572269 | grad norm: 0.1872 | lrm: 1.00 | dt: 13803.73ms | tok/sec: 37,981 | mfu: 12.20 | total time: 138.42m
step 00610/21400 (2.85%) | loss: 3.575500 | grad norm: 0.1976 | lrm: 1.00 | dt: 13882.73ms | tok/sec: 37,765 | mfu: 12.13 | total time: 138.66m
step 00611/21400 (2.86%) | loss: 3.601005 | grad norm: 0.2158 | lrm: 1.00 | dt: 13828.77ms | tok/sec: 37,912 | mfu: 12.18 | total time: 138.89m
step 00612/21400 (2.86%) | loss: 3.597020 | grad norm: 0.2197 | lrm: 1.00 | dt: 13868.23ms | tok/sec: 37,804 | mfu: 12.14 | total time: 139.12m
step 00613/21400 (2.86%) | loss: 3.600130 | grad norm: 0.2035 | lrm: 1.00 | dt: 13820.93ms | tok/sec: 37,934 | mfu: 12.19 | total time: 139.35m
step 00614/21400 (2.87%) | loss: 3.574626 | grad norm: 0.1854 | lrm: 1.00 | dt: 13867.30ms | tok/sec: 37,807 | mfu: 12.15 | total time: 139.58m
step 00615/21400 (2.87%) | loss: 3.576009 | grad norm: 0.2093 | lrm: 1.00 | dt: 13833.54ms | tok/sec: 37,899 | mfu: 12.18 | total time: 139.81m
step 00616/21400 (2.88%) | loss: 3.613064 | grad norm: 0.2255 | lrm: 1.00 | dt: 13832.24ms | tok/sec: 37,903 | mfu: 12.18 | total time: 140.04m
step 00617/21400 (2.88%) | loss: 3.671870 | grad norm: 0.1784 | lrm: 1.00 | dt: 13898.72ms | tok/sec: 37,722 | mfu: 12.12 | total time: 140.27m
step 00618/21400 (2.89%) | loss: 3.659632 | grad norm: 0.1812 | lrm: 1.00 | dt: 13785.71ms | tok/sec: 38,031 | mfu: 12.22 | total time: 140.50m
step 00619/21400 (2.89%) | loss: 3.642549 | grad norm: 0.1744 | lrm: 1.00 | dt: 13896.76ms | tok/sec: 37,727 | mfu: 12.12 | total time: 140.73m
step 00620/21400 (2.90%) | loss: 3.619005 | grad norm: 0.1627 | lrm: 1.00 | dt: 13787.34ms | tok/sec: 38,026 | mfu: 12.22 | total time: 140.96m
step 00621/21400 (2.90%) | loss: 3.619485 | grad norm: 0.1692 | lrm: 1.00 | dt: 13893.34ms | tok/sec: 37,736 | mfu: 12.12 | total time: 141.19m
step 00622/21400 (2.91%) | loss: 3.620230 | grad norm: 0.1863 | lrm: 1.00 | dt: 13796.93ms | tok/sec: 38,000 | mfu: 12.21 | total time: 141.42m
step 00623/21400 (2.91%) | loss: 3.649756 | grad norm: 0.1789 | lrm: 1.00 | dt: 13874.34ms | tok/sec: 37,788 | mfu: 12.14 | total time: 141.66m
step 00624/21400 (2.92%) | loss: 3.665167 | grad norm: 0.1902 | lrm: 1.00 | dt: 13803.48ms | tok/sec: 37,982 | mfu: 12.20 | total time: 141.89m
step 00625/21400 (2.92%) | loss: 3.684998 | grad norm: 0.1855 | lrm: 1.00 | dt: 13874.06ms | tok/sec: 37,789 | mfu: 12.14 | total time: 142.12m
step 00626/21400 (2.93%) | loss: 3.672078 | grad norm: 0.1606 | lrm: 1.00 | dt: 13815.68ms | tok/sec: 37,948 | mfu: 12.19 | total time: 142.35m
step 00627/21400 (2.93%) | loss: 3.652428 | grad norm: 0.1579 | lrm: 1.00 | dt: 13864.37ms | tok/sec: 37,815 | mfu: 12.15 | total time: 142.58m
step 00628/21400 (2.93%) | loss: 3.723992 | grad norm: 0.1758 | lrm: 1.00 | dt: 13832.17ms | tok/sec: 37,903 | mfu: 12.18 | total time: 142.81m
step 00629/21400 (2.94%) | loss: 3.716284 | grad norm: 0.2111 | lrm: 1.00 | dt: 13835.54ms | tok/sec: 37,894 | mfu: 12.17 | total time: 143.04m
step 00630/21400 (2.94%) | loss: 3.747761 | grad norm: 0.1913 | lrm: 1.00 | dt: 13886.49ms | tok/sec: 37,755 | mfu: 12.13 | total time: 143.27m
step 00631/21400 (2.95%) | loss: 3.752373 | grad norm: 0.1772 | lrm: 1.00 | dt: 13786.27ms | tok/sec: 38,029 | mfu: 12.22 | total time: 143.50m
step 00632/21400 (2.95%) | loss: 3.721737 | grad norm: 0.1810 | lrm: 1.00 | dt: 13883.69ms | tok/sec: 37,762 | mfu: 12.13 | total time: 143.73m
step 00633/21400 (2.96%) | loss: 3.701357 | grad norm: 0.1842 | lrm: 1.00 | dt: 14006.32ms | tok/sec: 37,432 | mfu: 12.03 | total time: 143.97m
step 00634/21400 (2.96%) | loss: 3.684871 | grad norm: 0.1842 | lrm: 1.00 | dt: 13886.34ms | tok/sec: 37,755 | mfu: 12.13 | total time: 144.20m
step 00635/21400 (2.97%) | loss: 3.728079 | grad norm: 0.1862 | lrm: 1.00 | dt: 13789.84ms | tok/sec: 38,019 | mfu: 12.21 | total time: 144.43m
step 00636/21400 (2.97%) | loss: 3.755045 | grad norm: 0.1663 | lrm: 1.00 | dt: 13879.56ms | tok/sec: 37,774 | mfu: 12.14 | total time: 144.66m
step 00637/21400 (2.98%) | loss: 3.808287 | grad norm: 0.1835 | lrm: 1.00 | dt: 13796.54ms | tok/sec: 38,001 | mfu: 12.21 | total time: 144.89m
step 00638/21400 (2.98%) | loss: 3.780572 | grad norm: 0.1761 | lrm: 1.00 | dt: 13877.78ms | tok/sec: 37,778 | mfu: 12.14 | total time: 145.12m
step 00639/21400 (2.99%) | loss: 3.722896 | grad norm: 0.1848 | lrm: 1.00 | dt: 13953.50ms | tok/sec: 37,573 | mfu: 12.07 | total time: 145.35m
step 00640/21400 (2.99%) | loss: 3.688660 | grad norm: 0.2005 | lrm: 1.00 | dt: 13846.10ms | tok/sec: 37,865 | mfu: 12.16 | total time: 145.58m
step 00641/21400 (3.00%) | loss: 3.669910 | grad norm: 0.1919 | lrm: 1.00 | dt: 13822.41ms | tok/sec: 37,930 | mfu: 12.19 | total time: 145.81m
step 00642/21400 (3.00%) | loss: 3.665062 | grad norm: 0.1950 | lrm: 1.00 | dt: 13824.36ms | tok/sec: 37,924 | mfu: 12.18 | total time: 146.04m
step 00643/21400 (3.00%) | loss: 3.656533 | grad norm: 0.2123 | lrm: 1.00 | dt: 13834.88ms | tok/sec: 37,896 | mfu: 12.17 | total time: 146.27m
step 00644/21400 (3.01%) | loss: 3.635647 | grad norm: 0.1852 | lrm: 1.00 | dt: 13796.49ms | tok/sec: 38,001 | mfu: 12.21 | total time: 146.50m
step 00645/21400 (3.01%) | loss: 3.631142 | grad norm: 0.1713 | lrm: 1.00 | dt: 13878.68ms | tok/sec: 37,776 | mfu: 12.14 | total time: 146.74m
step 00646/21400 (3.02%) | loss: 3.614498 | grad norm: 0.1751 | lrm: 1.00 | dt: 13751.72ms | tok/sec: 38,125 | mfu: 12.25 | total time: 146.96m
step 00647/21400 (3.02%) | loss: 3.605606 | grad norm: 0.1630 | lrm: 1.00 | dt: 13866.10ms | tok/sec: 37,810 | mfu: 12.15 | total time: 147.20m
step 00648/21400 (3.03%) | loss: 3.577850 | grad norm: 0.1602 | lrm: 1.00 | dt: 13776.09ms | tok/sec: 38,057 | mfu: 12.23 | total time: 147.43m
step 00649/21400 (3.03%) | loss: 3.555600 | grad norm: 0.1743 | lrm: 1.00 | dt: 13857.56ms | tok/sec: 37,834 | mfu: 12.15 | total time: 147.66m
step 00650/21400 (3.04%) | loss: 3.562959 | grad norm: 0.1948 | lrm: 1.00 | dt: 13780.47ms | tok/sec: 38,045 | mfu: 12.22 | total time: 147.89m
step 00651/21400 (3.04%) | loss: 3.593279 | grad norm: 0.1964 | lrm: 1.00 | dt: 13848.57ms | tok/sec: 37,858 | mfu: 12.16 | total time: 148.12m
step 00652/21400 (3.05%) | loss: 3.575521 | grad norm: 0.1803 | lrm: 1.00 | dt: 13826.60ms | tok/sec: 37,918 | mfu: 12.18 | total time: 148.35m
step 00653/21400 (3.05%) | loss: 3.545362 | grad norm: 0.1776 | lrm: 1.00 | dt: 13827.81ms | tok/sec: 37,915 | mfu: 12.18 | total time: 148.58m
step 00654/21400 (3.06%) | loss: 3.521186 | grad norm: 0.1787 | lrm: 1.00 | dt: 13771.49ms | tok/sec: 38,070 | mfu: 12.23 | total time: 148.81m
step 00655/21400 (3.06%) | loss: 3.497033 | grad norm: 0.1695 | lrm: 1.00 | dt: 13817.42ms | tok/sec: 37,943 | mfu: 12.19 | total time: 149.04m
step 00656/21400 (3.07%) | loss: 3.485931 | grad norm: 0.2299 | lrm: 1.00 | dt: 13835.91ms | tok/sec: 37,893 | mfu: 12.17 | total time: 149.27m
step 00657/21400 (3.07%) | loss: 3.507133 | grad norm: 0.1630 | lrm: 1.00 | dt: 13792.60ms | tok/sec: 38,012 | mfu: 12.21 | total time: 149.50m
step 00658/21400 (3.07%) | loss: 3.470392 | grad norm: 0.1657 | lrm: 1.00 | dt: 13876.95ms | tok/sec: 37,781 | mfu: 12.14 | total time: 149.73m
step 00659/21400 (3.08%) | loss: 3.500984 | grad norm: 0.1849 | lrm: 1.00 | dt: 13755.71ms | tok/sec: 38,114 | mfu: 12.24 | total time: 149.96m
step 00660/21400 (3.08%) | loss: 3.497669 | grad norm: 0.1995 | lrm: 1.00 | dt: 14075.63ms | tok/sec: 37,247 | mfu: 11.97 | total time: 150.19m
step 00661/21400 (3.09%) | loss: 3.495614 | grad norm: 0.1708 | lrm: 1.00 | dt: 13768.39ms | tok/sec: 38,079 | mfu: 12.23 | total time: 150.42m
step 00662/21400 (3.09%) | loss: 3.515048 | grad norm: 0.1863 | lrm: 1.00 | dt: 13882.94ms | tok/sec: 37,764 | mfu: 12.13 | total time: 150.65m
step 00663/21400 (3.10%) | loss: 3.524435 | grad norm: 0.1975 | lrm: 1.00 | dt: 13780.34ms | tok/sec: 38,046 | mfu: 12.22 | total time: 150.88m
step 00664/21400 (3.10%) | loss: 3.512341 | grad norm: 0.1870 | lrm: 1.00 | dt: 13851.82ms | tok/sec: 37,849 | mfu: 12.16 | total time: 151.11m
step 00665/21400 (3.11%) | loss: 3.535540 | grad norm: 0.2011 | lrm: 1.00 | dt: 13782.68ms | tok/sec: 38,039 | mfu: 12.22 | total time: 151.34m
step 00666/21400 (3.11%) | loss: 3.509074 | grad norm: 0.1844 | lrm: 1.00 | dt: 13834.06ms | tok/sec: 37,898 | mfu: 12.17 | total time: 151.57m
step 00667/21400 (3.12%) | loss: 3.482260 | grad norm: 0.1666 | lrm: 1.00 | dt: 13805.86ms | tok/sec: 37,975 | mfu: 12.20 | total time: 151.80m
step 00668/21400 (3.12%) | loss: 3.499641 | grad norm: 0.1745 | lrm: 1.00 | dt: 13818.55ms | tok/sec: 37,940 | mfu: 12.19 | total time: 152.04m
step 00669/21400 (3.13%) | loss: 3.534451 | grad norm: 0.1652 | lrm: 1.00 | dt: 13825.51ms | tok/sec: 37,921 | mfu: 12.18 | total time: 152.27m
step 00670/21400 (3.13%) | loss: 3.514190 | grad norm: 0.1564 | lrm: 1.00 | dt: 13800.11ms | tok/sec: 37,991 | mfu: 12.20 | total time: 152.50m
step 00671/21400 (3.14%) | loss: 3.545116 | grad norm: 0.1743 | lrm: 1.00 | dt: 13864.64ms | tok/sec: 37,814 | mfu: 12.15 | total time: 152.73m
step 00672/21400 (3.14%) | loss: 3.525015 | grad norm: 0.1809 | lrm: 1.00 | dt: 13746.00ms | tok/sec: 38,141 | mfu: 12.25 | total time: 152.96m
step 00673/21400 (3.14%) | loss: 3.534747 | grad norm: 0.1812 | lrm: 1.00 | dt: 13868.26ms | tok/sec: 37,804 | mfu: 12.14 | total time: 153.19m
step 00674/21400 (3.15%) | loss: 3.574038 | grad norm: 0.1738 | lrm: 1.00 | dt: 13764.69ms | tok/sec: 38,089 | mfu: 12.24 | total time: 153.42m
step 00675/21400 (3.15%) | loss: 3.561539 | grad norm: 0.1668 | lrm: 1.00 | dt: 13856.61ms | tok/sec: 37,836 | mfu: 12.16 | total time: 153.65m
step 00676/21400 (3.16%) | loss: 3.552405 | grad norm: 0.1949 | lrm: 1.00 | dt: 13771.66ms | tok/sec: 38,070 | mfu: 12.23 | total time: 153.88m
step 00677/21400 (3.16%) | loss: 3.581632 | grad norm: 0.2062 | lrm: 1.00 | dt: 13847.79ms | tok/sec: 37,860 | mfu: 12.16 | total time: 154.11m
step 00678/21400 (3.17%) | loss: 3.663565 | grad norm: 0.2077 | lrm: 1.00 | dt: 13797.71ms | tok/sec: 37,998 | mfu: 12.21 | total time: 154.34m
step 00679/21400 (3.17%) | loss: 3.678084 | grad norm: 0.1876 | lrm: 1.00 | dt: 13829.92ms | tok/sec: 37,909 | mfu: 12.18 | total time: 154.57m
step 00680/21400 (3.18%) | loss: 3.652557 | grad norm: 0.1730 | lrm: 1.00 | dt: 13813.95ms | tok/sec: 37,953 | mfu: 12.19 | total time: 154.80m
step 00681/21400 (3.18%) | loss: 3.640694 | grad norm: 0.1576 | lrm: 1.00 | dt: 13829.23ms | tok/sec: 37,911 | mfu: 12.18 | total time: 155.03m
step 00682/21400 (3.19%) | loss: 3.609245 | grad norm: 0.1620 | lrm: 1.00 | dt: 13834.45ms | tok/sec: 37,897 | mfu: 12.17 | total time: 155.26m
step 00683/21400 (3.19%) | loss: 3.586334 | grad norm: 0.1707 | lrm: 1.00 | dt: 13789.31ms | tok/sec: 38,021 | mfu: 12.21 | total time: 155.49m
step 00684/21400 (3.20%) | loss: 3.572006 | grad norm: 0.1855 | lrm: 1.00 | dt: 13881.84ms | tok/sec: 37,767 | mfu: 12.13 | total time: 155.72m
step 00685/21400 (3.20%) | loss: 3.589332 | grad norm: 0.1803 | lrm: 1.00 | dt: 13755.65ms | tok/sec: 38,114 | mfu: 12.24 | total time: 155.95m
step 00686/21400 (3.21%) | loss: 3.541434 | grad norm: 0.1856 | lrm: 1.00 | dt: 13872.06ms | tok/sec: 37,794 | mfu: 12.14 | total time: 156.18m
step 00687/21400 (3.21%) | loss: 3.534429 | grad norm: 0.2012 | lrm: 1.00 | dt: 13756.42ms | tok/sec: 38,112 | mfu: 12.24 | total time: 156.41m
step 00688/21400 (3.21%) | loss: 3.522973 | grad norm: 0.1967 | lrm: 1.00 | dt: 14067.54ms | tok/sec: 37,269 | mfu: 11.97 | total time: 156.64m
step 00689/21400 (3.22%) | loss: 3.505893 | grad norm: 0.1642 | lrm: 1.00 | dt: 13767.61ms | tok/sec: 38,081 | mfu: 12.23 | total time: 156.87m
step 00690/21400 (3.22%) | loss: 3.473194 | grad norm: 0.1694 | lrm: 1.00 | dt: 13858.97ms | tok/sec: 37,830 | mfu: 12.15 | total time: 157.11m
step 00691/21400 (3.23%) | loss: 3.476421 | grad norm: 0.1885 | lrm: 1.00 | dt: 13804.01ms | tok/sec: 37,980 | mfu: 12.20 | total time: 157.34m
step 00692/21400 (3.23%) | loss: 3.453136 | grad norm: 0.1862 | lrm: 1.00 | dt: 13837.47ms | tok/sec: 37,889 | mfu: 12.17 | total time: 157.57m
step 00693/21400 (3.24%) | loss: 3.431723 | grad norm: 0.1765 | lrm: 1.00 | dt: 13804.82ms | tok/sec: 37,978 | mfu: 12.20 | total time: 157.80m
step 00694/21400 (3.24%) | loss: 3.458772 | grad norm: 0.1742 | lrm: 1.00 | dt: 13812.14ms | tok/sec: 37,958 | mfu: 12.19 | total time: 158.03m
step 00695/21400 (3.25%) | loss: 3.446223 | grad norm: 0.1822 | lrm: 1.00 | dt: 13844.97ms | tok/sec: 37,868 | mfu: 12.17 | total time: 158.26m
step 00696/21400 (3.25%) | loss: 3.443977 | grad norm: 0.1655 | lrm: 1.00 | dt: 13787.70ms | tok/sec: 38,025 | mfu: 12.22 | total time: 158.49m
step 00697/21400 (3.26%) | loss: 3.442668 | grad norm: 0.1732 | lrm: 1.00 | dt: 13880.69ms | tok/sec: 37,771 | mfu: 12.13 | total time: 158.72m
step 00698/21400 (3.26%) | loss: 3.468999 | grad norm: 0.1650 | lrm: 1.00 | dt: 13766.99ms | tok/sec: 38,082 | mfu: 12.23 | total time: 158.95m
step 00699/21400 (3.27%) | loss: 3.454751 | grad norm: 0.1543 | lrm: 1.00 | dt: 13867.94ms | tok/sec: 37,805 | mfu: 12.15 | total time: 159.18m
step 00700/21400 (3.27%) | loss: 3.473770 | grad norm: 0.1689 | lrm: 1.00 | dt: 13770.56ms | tok/sec: 38,073 | mfu: 12.23 | total time: 159.41m
step 00701/21400 (3.28%) | loss: 3.522650 | grad norm: 0.1820 | lrm: 1.00 | dt: 13860.03ms | tok/sec: 37,827 | mfu: 12.15 | total time: 159.64m
step 00702/21400 (3.28%) | loss: 3.515556 | grad norm: 0.2000 | lrm: 1.00 | dt: 13768.75ms | tok/sec: 38,078 | mfu: 12.23 | total time: 159.87m
step 00703/21400 (3.29%) | loss: 3.511709 | grad norm: 0.1950 | lrm: 1.00 | dt: 13845.60ms | tok/sec: 37,866 | mfu: 12.16 | total time: 160.10m
step 00704/21400 (3.29%) | loss: 3.509845 | grad norm: 0.1777 | lrm: 1.00 | dt: 13802.15ms | tok/sec: 37,985 | mfu: 12.20 | total time: 160.33m
step 00705/21400 (3.29%) | loss: 3.499946 | grad norm: 0.1760 | lrm: 1.00 | dt: 13850.06ms | tok/sec: 37,854 | mfu: 12.16 | total time: 160.56m
step 00706/21400 (3.30%) | loss: 3.494155 | grad norm: 0.1777 | lrm: 1.00 | dt: 13810.14ms | tok/sec: 37,963 | mfu: 12.20 | total time: 160.79m
step 00707/21400 (3.30%) | loss: 3.529359 | grad norm: 0.1724 | lrm: 1.00 | dt: 13820.58ms | tok/sec: 37,935 | mfu: 12.19 | total time: 161.02m
step 00708/21400 (3.31%) | loss: 3.546771 | grad norm: 0.1837 | lrm: 1.00 | dt: 13832.46ms | tok/sec: 37,902 | mfu: 12.18 | total time: 161.25m
step 00709/21400 (3.31%) | loss: 3.552419 | grad norm: 0.1528 | lrm: 1.00 | dt: 13800.94ms | tok/sec: 37,989 | mfu: 12.20 | total time: 161.48m
step 00710/21400 (3.32%) | loss: 3.583267 | grad norm: 0.1669 | lrm: 1.00 | dt: 13872.93ms | tok/sec: 37,792 | mfu: 12.14 | total time: 161.71m
step 00711/21400 (3.32%) | loss: 3.595272 | grad norm: 0.1911 | lrm: 1.00 | dt: 13750.02ms | tok/sec: 38,129 | mfu: 12.25 | total time: 161.94m
step 00712/21400 (3.33%) | loss: 3.555480 | grad norm: 0.1764 | lrm: 1.00 | dt: 13874.38ms | tok/sec: 37,788 | mfu: 12.14 | total time: 162.17m
step 00713/21400 (3.33%) | loss: 3.525290 | grad norm: 0.1635 | lrm: 1.00 | dt: 13771.49ms | tok/sec: 38,070 | mfu: 12.23 | total time: 162.40m
step 00714/21400 (3.34%) | loss: 3.504265 | grad norm: 0.1728 | lrm: 1.00 | dt: 13851.75ms | tok/sec: 37,849 | mfu: 12.16 | total time: 162.63m
step 00715/21400 (3.34%) | loss: 3.502216 | grad norm: 0.1596 | lrm: 1.00 | dt: 13777.60ms | tok/sec: 38,053 | mfu: 12.22 | total time: 162.86m
step 00716/21400 (3.35%) | loss: 3.542461 | grad norm: 0.1713 | lrm: 1.00 | dt: 14066.46ms | tok/sec: 37,272 | mfu: 11.97 | total time: 163.10m
step 00717/21400 (3.35%) | loss: 3.525171 | grad norm: 0.1819 | lrm: 1.00 | dt: 13792.73ms | tok/sec: 38,011 | mfu: 12.21 | total time: 163.33m
step 00718/21400 (3.36%) | loss: 3.539778 | grad norm: 0.1742 | lrm: 1.00 | dt: 13835.55ms | tok/sec: 37,894 | mfu: 12.17 | total time: 163.56m
step 00719/21400 (3.36%) | loss: 3.542393 | grad norm: 0.1613 | lrm: 1.00 | dt: 13825.27ms | tok/sec: 37,922 | mfu: 12.18 | total time: 163.79m
step 00720/21400 (3.36%) | loss: 3.583433 | grad norm: 0.1654 | lrm: 1.00 | dt: 13823.88ms | tok/sec: 37,926 | mfu: 12.18 | total time: 164.02m
step 00721/21400 (3.37%) | loss: 3.601337 | grad norm: 0.1704 | lrm: 1.00 | dt: 13831.86ms | tok/sec: 37,904 | mfu: 12.18 | total time: 164.25m
step 00722/21400 (3.37%) | loss: 3.620299 | grad norm: 0.1613 | lrm: 1.00 | dt: 13791.56ms | tok/sec: 38,015 | mfu: 12.21 | total time: 164.48m
step 00723/21400 (3.38%) | loss: 3.579318 | grad norm: 0.1649 | lrm: 1.00 | dt: 13875.93ms | tok/sec: 37,783 | mfu: 12.14 | total time: 164.71m
step 00724/21400 (3.38%) | loss: 3.580090 | grad norm: 0.1823 | lrm: 1.00 | dt: 13750.98ms | tok/sec: 38,127 | mfu: 12.25 | total time: 164.94m
step 00725/21400 (3.39%) | loss: 3.560092 | grad norm: 0.1778 | lrm: 1.00 | dt: 13866.00ms | tok/sec: 37,811 | mfu: 12.15 | total time: 165.17m
step 00726/21400 (3.39%) | loss: 3.536120 | grad norm: 0.1773 | lrm: 1.00 | dt: 13756.22ms | tok/sec: 38,112 | mfu: 12.24 | total time: 165.40m
step 00727/21400 (3.40%) | loss: 3.513155 | grad norm: 0.1997 | lrm: 1.00 | dt: 13857.19ms | tok/sec: 37,835 | mfu: 12.15 | total time: 165.63m
step 00728/21400 (3.40%) | loss: 3.516372 | grad norm: 0.1941 | lrm: 1.00 | dt: 13763.96ms | tok/sec: 38,091 | mfu: 12.24 | total time: 165.86m
step 00729/21400 (3.41%) | loss: 3.504844 | grad norm: 0.1740 | lrm: 1.00 | dt: 13852.87ms | tok/sec: 37,846 | mfu: 12.16 | total time: 166.09m
step 00730/21400 (3.41%) | loss: 3.584500 | grad norm: 0.1844 | lrm: 1.00 | dt: 13788.98ms | tok/sec: 38,022 | mfu: 12.21 | total time: 166.32m
step 00731/21400 (3.42%) | loss: 3.530025 | grad norm: 0.1693 | lrm: 1.00 | dt: 13836.84ms | tok/sec: 37,890 | mfu: 12.17 | total time: 166.55m
step 00732/21400 (3.42%) | loss: 3.549885 | grad norm: 0.1555 | lrm: 1.00 | dt: 13805.29ms | tok/sec: 37,977 | mfu: 12.20 | total time: 166.78m
step 00733/21400 (3.43%) | loss: 3.514674 | grad norm: 0.1803 | lrm: 1.00 | dt: 13801.78ms | tok/sec: 37,986 | mfu: 12.20 | total time: 167.01m
step 00734/21400 (3.43%) | loss: 3.549051 | grad norm: 0.1894 | lrm: 1.00 | dt: 13839.52ms | tok/sec: 37,883 | mfu: 12.17 | total time: 167.24m
step 00735/21400 (3.43%) | loss: 3.618030 | grad norm: 0.1878 | lrm: 1.00 | dt: 13787.85ms | tok/sec: 38,025 | mfu: 12.22 | total time: 167.47m
step 00736/21400 (3.44%) | loss: 3.628916 | grad norm: 0.1676 | lrm: 1.00 | dt: 13882.20ms | tok/sec: 37,766 | mfu: 12.13 | total time: 167.70m
step 00737/21400 (3.44%) | loss: 3.607392 | grad norm: 0.1654 | lrm: 1.00 | dt: 13751.00ms | tok/sec: 38,127 | mfu: 12.25 | total time: 167.93m
step 00738/21400 (3.45%) | loss: 3.612135 | grad norm: 0.1836 | lrm: 1.00 | dt: 13876.85ms | tok/sec: 37,781 | mfu: 12.14 | total time: 168.16m
step 00739/21400 (3.45%) | loss: 3.585889 | grad norm: 0.1689 | lrm: 1.00 | dt: 13755.69ms | tok/sec: 38,114 | mfu: 12.24 | total time: 168.39m
step 00740/21400 (3.46%) | loss: 3.537089 | grad norm: 0.1721 | lrm: 1.00 | dt: 13847.30ms | tok/sec: 37,862 | mfu: 12.16 | total time: 168.62m
step 00741/21400 (3.46%) | loss: 3.528808 | grad norm: 0.1641 | lrm: 1.00 | dt: 13771.00ms | tok/sec: 38,071 | mfu: 12.23 | total time: 168.85m
step 00742/21400 (3.47%) | loss: 3.573333 | grad norm: 0.1615 | lrm: 1.00 | dt: 13855.37ms | tok/sec: 37,840 | mfu: 12.16 | total time: 169.08m
step 00743/21400 (3.47%) | loss: 3.546694 | grad norm: 0.1796 | lrm: 1.00 | dt: 13791.76ms | tok/sec: 38,014 | mfu: 12.21 | total time: 169.31m
step 00744/21400 (3.48%) | loss: 3.561347 | grad norm: 0.1751 | lrm: 1.00 | dt: 14053.56ms | tok/sec: 37,306 | mfu: 11.98 | total time: 169.55m
step 00745/21400 (3.48%) | loss: 3.556431 | grad norm: 0.1671 | lrm: 1.00 | dt: 13800.09ms | tok/sec: 37,991 | mfu: 12.20 | total time: 169.78m
step 00746/21400 (3.49%) | loss: 3.518238 | grad norm: 0.1863 | lrm: 1.00 | dt: 13816.55ms | tok/sec: 37,946 | mfu: 12.19 | total time: 170.01m
step 00747/21400 (3.49%) | loss: 3.511139 | grad norm: 0.1784 | lrm: 1.00 | dt: 13832.40ms | tok/sec: 37,902 | mfu: 12.18 | total time: 170.24m
step 00748/21400 (3.50%) | loss: 3.506705 | grad norm: 0.1553 | lrm: 1.00 | dt: 13797.47ms | tok/sec: 37,998 | mfu: 12.21 | total time: 170.47m
step 00749/21400 (3.50%) | loss: 3.560574 | grad norm: 0.1637 | lrm: 1.00 | dt: 13891.80ms | tok/sec: 37,740 | mfu: 12.12 | total time: 170.70m
Step 00750 | Validation bpb: 1.0385
step 00750/21400 (3.50%) | loss: 3.559936 | grad norm: 0.1657 | lrm: 1.00 | dt: 13769.75ms | tok/sec: 38,075 | mfu: 12.23 | total time: 170.93m
step 00751/21400 (3.51%) | loss: 3.573875 | grad norm: 0.1598 | lrm: 1.00 | dt: 13857.88ms | tok/sec: 37,833 | mfu: 12.15 | total time: 171.16m
step 00752/21400 (3.51%) | loss: 3.622048 | grad norm: 0.1600 | lrm: 1.00 | dt: 13784.35ms | tok/sec: 38,035 | mfu: 12.22 | total time: 171.39m
step 00753/21400 (3.52%) | loss: 3.620079 | grad norm: 0.1761 | lrm: 1.00 | dt: 13849.10ms | tok/sec: 37,857 | mfu: 12.16 | total time: 171.62m
step 00754/21400 (3.52%) | loss: 3.596716 | grad norm: 0.1707 | lrm: 1.00 | dt: 13800.83ms | tok/sec: 37,989 | mfu: 12.20 | total time: 171.85m
step 00755/21400 (3.53%) | loss: 3.585832 | grad norm: 0.1765 | lrm: 1.00 | dt: 13822.60ms | tok/sec: 37,929 | mfu: 12.19 | total time: 172.08m
step 00756/21400 (3.53%) | loss: 3.586920 | grad norm: 0.2381 | lrm: 1.00 | dt: 13835.88ms | tok/sec: 37,893 | mfu: 12.17 | total time: 172.31m
step 00757/21400 (3.54%) | loss: 3.574653 | grad norm: 0.1967 | lrm: 1.00 | dt: 13794.39ms | tok/sec: 38,007 | mfu: 12.21 | total time: 172.54m
step 00758/21400 (3.54%) | loss: 3.565049 | grad norm: 0.1790 | lrm: 1.00 | dt: 13877.49ms | tok/sec: 37,779 | mfu: 12.14 | total time: 172.77m
step 00759/21400 (3.55%) | loss: 3.591813 | grad norm: 0.1924 | lrm: 1.00 | dt: 13738.13ms | tok/sec: 38,162 | mfu: 12.26 | total time: 173.00m
step 00760/21400 (3.55%) | loss: 3.577349 | grad norm: 0.1892 | lrm: 1.00 | dt: 13883.67ms | tok/sec: 37,762 | mfu: 12.13 | total time: 173.23m
step 00761/21400 (3.56%) | loss: 3.573166 | grad norm: 0.1737 | lrm: 1.00 | dt: 13756.27ms | tok/sec: 38,112 | mfu: 12.24 | total time: 173.46m
step 00762/21400 (3.56%) | loss: 3.584759 | grad norm: 0.1695 | lrm: 1.00 | dt: 13877.22ms | tok/sec: 37,780 | mfu: 12.14 | total time: 173.69m
step 00763/21400 (3.57%) | loss: 3.599324 | grad norm: 0.1780 | lrm: 1.00 | dt: 13757.56ms | tok/sec: 38,109 | mfu: 12.24 | total time: 173.92m
step 00764/21400 (3.57%) | loss: 3.559872 | grad norm: 0.1627 | lrm: 1.00 | dt: 13849.94ms | tok/sec: 37,854 | mfu: 12.16 | total time: 174.16m
step 00765/21400 (3.57%) | loss: 3.548510 | grad norm: 0.1608 | lrm: 1.00 | dt: 13777.56ms | tok/sec: 38,053 | mfu: 12.22 | total time: 174.38m
step 00766/21400 (3.58%) | loss: 3.558085 | grad norm: 0.1562 | lrm: 1.00 | dt: 13851.19ms | tok/sec: 37,851 | mfu: 12.16 | total time: 174.62m
step 00767/21400 (3.58%) | loss: 3.550806 | grad norm: 0.1709 | lrm: 1.00 | dt: 13792.16ms | tok/sec: 38,013 | mfu: 12.21 | total time: 174.85m
step 00768/21400 (3.59%) | loss: 3.539533 | grad norm: 0.1840 | lrm: 1.00 | dt: 13821.77ms | tok/sec: 37,932 | mfu: 12.19 | total time: 175.08m
step 00769/21400 (3.59%) | loss: 3.497844 | grad norm: 0.1928 | lrm: 1.00 | dt: 13824.26ms | tok/sec: 37,925 | mfu: 12.18 | total time: 175.31m
step 00770/21400 (3.60%) | loss: 3.520770 | grad norm: 0.1793 | lrm: 1.00 | dt: 14027.90ms | tok/sec: 37,374 | mfu: 12.01 | total time: 175.54m
step 00771/21400 (3.60%) | loss: 3.524512 | grad norm: 0.1768 | lrm: 1.00 | dt: 13866.00ms | tok/sec: 37,811 | mfu: 12.15 | total time: 175.77m
step 00772/21400 (3.61%) | loss: 3.533746 | grad norm: 0.1988 | lrm: 1.00 | dt: 13753.70ms | tok/sec: 38,119 | mfu: 12.25 | total time: 176.00m
step 00773/21400 (3.61%) | loss: 3.511423 | grad norm: 0.1860 | lrm: 1.00 | dt: 13882.35ms | tok/sec: 37,766 | mfu: 12.13 | total time: 176.23m
step 00774/21400 (3.62%) | loss: 3.497678 | grad norm: 0.1589 | lrm: 1.00 | dt: 13727.12ms | tok/sec: 38,193 | mfu: 12.27 | total time: 176.46m
step 00775/21400 (3.62%) | loss: 3.506815 | grad norm: 0.1686 | lrm: 1.00 | dt: 13864.57ms | tok/sec: 37,814 | mfu: 12.15 | total time: 176.69m
step 00776/21400 (3.63%) | loss: 3.521970 | grad norm: 0.1703 | lrm: 1.00 | dt: 13765.46ms | tok/sec: 38,087 | mfu: 12.24 | total time: 176.92m
step 00777/21400 (3.63%) | loss: 3.545598 | grad norm: 0.1607 | lrm: 1.00 | dt: 13850.58ms | tok/sec: 37,853 | mfu: 12.16 | total time: 177.15m
step 00778/21400 (3.64%) | loss: 3.530254 | grad norm: 0.1732 | lrm: 1.00 | dt: 13780.53ms | tok/sec: 38,045 | mfu: 12.22 | total time: 177.38m
step 00779/21400 (3.64%) | loss: 3.522744 | grad norm: 0.1640 | lrm: 1.00 | dt: 13840.85ms | tok/sec: 37,879 | mfu: 12.17 | total time: 177.61m
step 00780/21400 (3.64%) | loss: 3.547423 | grad norm: 0.1670 | lrm: 1.00 | dt: 13797.15ms | tok/sec: 37,999 | mfu: 12.21 | total time: 177.84m
step 00781/21400 (3.65%) | loss: 3.563924 | grad norm: 0.1694 | lrm: 1.00 | dt: 13837.68ms | tok/sec: 37,888 | mfu: 12.17 | total time: 178.07m
step 00782/21400 (3.65%) | loss: 3.545312 | grad norm: 0.1675 | lrm: 1.00 | dt: 13825.73ms | tok/sec: 37,921 | mfu: 12.18 | total time: 178.30m
step 00783/21400 (3.66%) | loss: 3.536163 | grad norm: 0.1714 | lrm: 1.00 | dt: 13797.71ms | tok/sec: 37,998 | mfu: 12.21 | total time: 178.53m
step 00784/21400 (3.66%) | loss: 3.517487 | grad norm: 0.1621 | lrm: 1.00 | dt: 13881.62ms | tok/sec: 37,768 | mfu: 12.13 | total time: 178.76m
step 00785/21400 (3.67%) | loss: 3.531231 | grad norm: 0.1590 | lrm: 1.00 | dt: 13749.85ms | tok/sec: 38,130 | mfu: 12.25 | total time: 178.99m
step 00786/21400 (3.67%) | loss: 3.532642 | grad norm: 0.1693 | lrm: 1.00 | dt: 13876.69ms | tok/sec: 37,781 | mfu: 12.14 | total time: 179.22m
step 00787/21400 (3.68%) | loss: 3.502519 | grad norm: 0.1839 | lrm: 1.00 | dt: 13748.86ms | tok/sec: 38,133 | mfu: 12.25 | total time: 179.45m
step 00788/21400 (3.68%) | loss: 3.539308 | grad norm: 0.1786 | lrm: 1.00 | dt: 13868.14ms | tok/sec: 37,805 | mfu: 12.15 | total time: 179.69m
step 00789/21400 (3.69%) | loss: 3.524237 | grad norm: 0.1561 | lrm: 1.00 | dt: 13766.20ms | tok/sec: 38,085 | mfu: 12.23 | total time: 179.91m
step 00790/21400 (3.69%) | loss: 3.511759 | grad norm: 0.1507 | lrm: 1.00 | dt: 13857.17ms | tok/sec: 37,835 | mfu: 12.15 | total time: 180.15m
step 00791/21400 (3.70%) | loss: 3.556895 | grad norm: 0.1606 | lrm: 1.00 | dt: 13788.06ms | tok/sec: 38,024 | mfu: 12.22 | total time: 180.38m
step 00792/21400 (3.70%) | loss: 3.518729 | grad norm: 0.1678 | lrm: 1.00 | dt: 13858.67ms | tok/sec: 37,831 | mfu: 12.15 | total time: 180.61m
step 00793/21400 (3.71%) | loss: 3.546462 | grad norm: 0.1796 | lrm: 1.00 | dt: 13795.54ms | tok/sec: 38,004 | mfu: 12.21 | total time: 180.84m
step 00794/21400 (3.71%) | loss: 3.559479 | grad norm: 0.1962 | lrm: 1.00 | dt: 13833.04ms | tok/sec: 37,901 | mfu: 12.18 | total time: 181.07m
step 00795/21400 (3.71%) | loss: 3.557427 | grad norm: 0.1984 | lrm: 1.00 | dt: 13824.07ms | tok/sec: 37,925 | mfu: 12.18 | total time: 181.30m
step 00796/21400 (3.72%) | loss: 3.536759 | grad norm: 0.1836 | lrm: 1.00 | dt: 13809.45ms | tok/sec: 37,965 | mfu: 12.20 | total time: 181.53m
step 00797/21400 (3.72%) | loss: 3.513229 | grad norm: 0.1769 | lrm: 1.00 | dt: 13876.00ms | tok/sec: 37,783 | mfu: 12.14 | total time: 181.76m
step 00798/21400 (3.73%) | loss: 3.490704 | grad norm: 0.1670 | lrm: 1.00 | dt: 13740.13ms | tok/sec: 38,157 | mfu: 12.26 | total time: 181.99m
step 00799/21400 (3.73%) | loss: 3.524271 | grad norm: 0.1633 | lrm: 1.00 | dt: 14117.74ms | tok/sec: 37,136 | mfu: 11.93 | total time: 182.22m
step 00800/21400 (3.74%) | loss: 3.490651 | grad norm: 0.2012 | lrm: 1.00 | dt: 13755.90ms | tok/sec: 38,113 | mfu: 12.24 | total time: 182.45m
step 00801/21400 (3.74%) | loss: 3.475308 | grad norm: 0.1781 | lrm: 1.00 | dt: 13862.20ms | tok/sec: 37,821 | mfu: 12.15 | total time: 182.68m
step 00802/21400 (3.75%) | loss: 3.474009 | grad norm: 0.1492 | lrm: 1.00 | dt: 13773.09ms | tok/sec: 38,066 | mfu: 12.23 | total time: 182.91m
step 00803/21400 (3.75%) | loss: 3.502432 | grad norm: 0.1624 | lrm: 1.00 | dt: 13860.21ms | tok/sec: 37,826 | mfu: 12.15 | total time: 183.14m
step 00804/21400 (3.76%) | loss: 3.567601 | grad norm: 0.1730 | lrm: 1.00 | dt: 13776.94ms | tok/sec: 38,055 | mfu: 12.23 | total time: 183.37m
step 00805/21400 (3.76%) | loss: 3.575887 | grad norm: 0.1641 | lrm: 1.00 | dt: 13840.07ms | tok/sec: 37,881 | mfu: 12.17 | total time: 183.60m
step 00806/21400 (3.77%) | loss: 3.585446 | grad norm: 0.1703 | lrm: 1.00 | dt: 13801.09ms | tok/sec: 37,988 | mfu: 12.20 | total time: 183.83m
step 00807/21400 (3.77%) | loss: 3.587531 | grad norm: 0.1640 | lrm: 1.00 | dt: 13818.01ms | tok/sec: 37,942 | mfu: 12.19 | total time: 184.06m
step 00808/21400 (3.78%) | loss: 3.572172 | grad norm: 0.1676 | lrm: 1.00 | dt: 13815.57ms | tok/sec: 37,949 | mfu: 12.19 | total time: 184.29m
step 00809/21400 (3.78%) | loss: 3.554541 | grad norm: 0.1711 | lrm: 1.00 | dt: 13810.51ms | tok/sec: 37,962 | mfu: 12.20 | total time: 184.52m
step 00810/21400 (3.79%) | loss: 3.566565 | grad norm: 0.1732 | lrm: 1.00 | dt: 13886.51ms | tok/sec: 37,755 | mfu: 12.13 | total time: 184.76m
step 00811/21400 (3.79%) | loss: 3.541287 | grad norm: 0.1605 | lrm: 1.00 | dt: 13748.71ms | tok/sec: 38,133 | mfu: 12.25 | total time: 184.99m
step 00812/21400 (3.79%) | loss: 3.555816 | grad norm: 0.1540 | lrm: 1.00 | dt: 13884.03ms | tok/sec: 37,761 | mfu: 12.13 | total time: 185.22m
step 00813/21400 (3.80%) | loss: 3.555806 | grad norm: 0.1519 | lrm: 1.00 | dt: 13751.15ms | tok/sec: 38,126 | mfu: 12.25 | total time: 185.45m
step 00814/21400 (3.80%) | loss: 3.555958 | grad norm: 0.1548 | lrm: 1.00 | dt: 13862.91ms | tok/sec: 37,819 | mfu: 12.15 | total time: 185.68m
step 00815/21400 (3.81%) | loss: 3.552754 | grad norm: 0.1622 | lrm: 1.00 | dt: 13768.73ms | tok/sec: 38,078 | mfu: 12.23 | total time: 185.91m
step 00816/21400 (3.81%) | loss: 3.502961 | grad norm: 0.1576 | lrm: 1.00 | dt: 13869.66ms | tok/sec: 37,801 | mfu: 12.14 | total time: 186.14m
step 00817/21400 (3.82%) | loss: 3.486600 | grad norm: 0.1620 | lrm: 1.00 | dt: 13775.27ms | tok/sec: 38,060 | mfu: 12.23 | total time: 186.37m
step 00818/21400 (3.82%) | loss: 3.470136 | grad norm: 0.1649 | lrm: 1.00 | dt: 13863.63ms | tok/sec: 37,817 | mfu: 12.15 | total time: 186.60m
step 00819/21400 (3.83%) | loss: 3.471794 | grad norm: 0.1633 | lrm: 1.00 | dt: 13781.74ms | tok/sec: 38,042 | mfu: 12.22 | total time: 186.83m
step 00820/21400 (3.83%) | loss: 3.481814 | grad norm: 0.1513 | lrm: 1.00 | dt: 13827.81ms | tok/sec: 37,915 | mfu: 12.18 | total time: 187.06m
step 00821/21400 (3.84%) | loss: 3.453314 | grad norm: 0.1488 | lrm: 1.00 | dt: 13827.36ms | tok/sec: 37,916 | mfu: 12.18 | total time: 187.29m
step 00822/21400 (3.84%) | loss: 3.474101 | grad norm: 0.1614 | lrm: 1.00 | dt: 13798.27ms | tok/sec: 37,996 | mfu: 12.21 | total time: 187.52m
step 00823/21400 (3.85%) | loss: 3.486347 | grad norm: 0.1647 | lrm: 1.00 | dt: 13885.68ms | tok/sec: 37,757 | mfu: 12.13 | total time: 187.75m
step 00824/21400 (3.85%) | loss: 3.450237 | grad norm: 0.1589 | lrm: 1.00 | dt: 13744.81ms | tok/sec: 38,144 | mfu: 12.25 | total time: 187.98m
step 00825/21400 (3.86%) | loss: 3.476358 | grad norm: 0.1717 | lrm: 1.00 | dt: 13893.11ms | tok/sec: 37,737 | mfu: 12.12 | total time: 188.21m
step 00826/21400 (3.86%) | loss: 3.447351 | grad norm: 0.1792 | lrm: 1.00 | dt: 13752.51ms | tok/sec: 38,123 | mfu: 12.25 | total time: 188.44m
step 00827/21400 (3.86%) | loss: 3.455063 | grad norm: 0.1352 | lrm: 1.00 | dt: 13884.05ms | tok/sec: 37,761 | mfu: 12.13 | total time: 188.67m
step 00828/21400 (3.87%) | loss: 3.441050 | grad norm: 0.1550 | lrm: 1.00 | dt: 13760.01ms | tok/sec: 38,102 | mfu: 12.24 | total time: 188.90m
step 00829/21400 (3.87%) | loss: 3.476582 | grad norm: 0.1832 | lrm: 1.00 | dt: 14084.76ms | tok/sec: 37,223 | mfu: 11.96 | total time: 189.14m
step 00830/21400 (3.88%) | loss: 3.472203 | grad norm: 0.1665 | lrm: 1.00 | dt: 13770.95ms | tok/sec: 38,072 | mfu: 12.23 | total time: 189.37m
step 00831/21400 (3.88%) | loss: 3.504258 | grad norm: 0.1781 | lrm: 1.00 | dt: 13842.65ms | tok/sec: 37,874 | mfu: 12.17 | total time: 189.60m
step 00832/21400 (3.89%) | loss: 3.547215 | grad norm: 0.2031 | lrm: 1.00 | dt: 13786.57ms | tok/sec: 38,028 | mfu: 12.22 | total time: 189.83m
step 00833/21400 (3.89%) | loss: 3.574026 | grad norm: 0.1663 | lrm: 1.00 | dt: 13828.51ms | tok/sec: 37,913 | mfu: 12.18 | total time: 190.06m
step 00834/21400 (3.90%) | loss: 3.561384 | grad norm: 0.1547 | lrm: 1.00 | dt: 13810.43ms | tok/sec: 37,963 | mfu: 12.20 | total time: 190.29m
step 00835/21400 (3.90%) | loss: 3.524456 | grad norm: 0.1657 | lrm: 1.00 | dt: 13790.78ms | tok/sec: 38,017 | mfu: 12.21 | total time: 190.52m
step 00836/21400 (3.91%) | loss: 3.537045 | grad norm: 0.1694 | lrm: 1.00 | dt: 13884.63ms | tok/sec: 37,760 | mfu: 12.13 | total time: 190.75m
step 00837/21400 (3.91%) | loss: 3.516294 | grad norm: 0.1728 | lrm: 1.00 | dt: 13726.02ms | tok/sec: 38,196 | mfu: 12.27 | total time: 190.98m
step 00838/21400 (3.92%) | loss: 3.503397 | grad norm: 0.1609 | lrm: 1.00 | dt: 13867.71ms | tok/sec: 37,806 | mfu: 12.15 | total time: 191.21m
step 00839/21400 (3.92%) | loss: 3.510992 | grad norm: 0.1531 | lrm: 1.00 | dt: 13744.85ms | tok/sec: 38,144 | mfu: 12.25 | total time: 191.44m
step 00840/21400 (3.93%) | loss: 3.495784 | grad norm: 0.1511 | lrm: 1.00 | dt: 13858.07ms | tok/sec: 37,832 | mfu: 12.15 | total time: 191.67m
step 00841/21400 (3.93%) | loss: 3.545478 | grad norm: 0.1517 | lrm: 1.00 | dt: 13752.08ms | tok/sec: 38,124 | mfu: 12.25 | total time: 191.90m
step 00842/21400 (3.93%) | loss: 3.577424 | grad norm: 0.1618 | lrm: 1.00 | dt: 13858.49ms | tok/sec: 37,831 | mfu: 12.15 | total time: 192.13m
step 00843/21400 (3.94%) | loss: 3.502232 | grad norm: 0.1621 | lrm: 1.00 | dt: 13752.71ms | tok/sec: 38,122 | mfu: 12.25 | total time: 192.36m
step 00844/21400 (3.94%) | loss: 3.492740 | grad norm: 0.1659 | lrm: 1.00 | dt: 13838.24ms | tok/sec: 37,886 | mfu: 12.17 | total time: 192.59m
step 00845/21400 (3.95%) | loss: 3.497741 | grad norm: 0.1627 | lrm: 1.00 | dt: 13800.40ms | tok/sec: 37,990 | mfu: 12.20 | total time: 192.82m
step 00846/21400 (3.95%) | loss: 3.524669 | grad norm: 0.1727 | lrm: 1.00 | dt: 13808.31ms | tok/sec: 37,969 | mfu: 12.20 | total time: 193.05m
step 00847/21400 (3.96%) | loss: 3.455340 | grad norm: 0.1776 | lrm: 1.00 | dt: 13814.70ms | tok/sec: 37,951 | mfu: 12.19 | total time: 193.28m
step 00848/21400 (3.96%) | loss: 3.469310 | grad norm: 0.1779 | lrm: 1.00 | dt: 13791.47ms | tok/sec: 38,015 | mfu: 12.21 | total time: 193.51m
step 00849/21400 (3.97%) | loss: 3.495955 | grad norm: 0.1648 | lrm: 1.00 | dt: 13872.32ms | tok/sec: 37,793 | mfu: 12.14 | total time: 193.74m
step 00850/21400 (3.97%) | loss: 3.491771 | grad norm: 0.1806 | lrm: 1.00 | dt: 13733.84ms | tok/sec: 38,174 | mfu: 12.26 | total time: 193.97m
step 00851/21400 (3.98%) | loss: 3.491524 | grad norm: 0.1650 | lrm: 1.00 | dt: 13865.10ms | tok/sec: 37,813 | mfu: 12.15 | total time: 194.20m
step 00852/21400 (3.98%) | loss: 3.487310 | grad norm: 0.1682 | lrm: 1.00 | dt: 13741.06ms | tok/sec: 38,154 | mfu: 12.26 | total time: 194.43m
step 00853/21400 (3.99%) | loss: 3.495988 | grad norm: 0.1723 | lrm: 1.00 | dt: 13865.69ms | tok/sec: 37,811 | mfu: 12.15 | total time: 194.66m
step 00854/21400 (3.99%) | loss: 3.467663 | grad norm: 0.1731 | lrm: 1.00 | dt: 13745.10ms | tok/sec: 38,143 | mfu: 12.25 | total time: 194.89m
step 00855/21400 (4.00%) | loss: 3.444793 | grad norm: 0.1610 | lrm: 1.00 | dt: 13850.90ms | tok/sec: 37,852 | mfu: 12.16 | total time: 195.12m
step 00856/21400 (4.00%) | loss: 3.425807 | grad norm: 0.1545 | lrm: 1.00 | dt: 13772.78ms | tok/sec: 38,066 | mfu: 12.23 | total time: 195.35m
step 00857/21400 (4.00%) | loss: 3.422742 | grad norm: 0.1585 | lrm: 1.00 | dt: 13841.57ms | tok/sec: 37,877 | mfu: 12.17 | total time: 195.58m
step 00858/21400 (4.01%) | loss: 3.383320 | grad norm: 0.1579 | lrm: 1.00 | dt: 13781.14ms | tok/sec: 38,043 | mfu: 12.22 | total time: 195.81m
step 00859/21400 (4.01%) | loss: 3.388565 | grad norm: 0.1677 | lrm: 1.00 | dt: 14055.19ms | tok/sec: 37,302 | mfu: 11.98 | total time: 196.04m
step 00860/21400 (4.02%) | loss: 3.385820 | grad norm: 0.1688 | lrm: 1.00 | dt: 13808.82ms | tok/sec: 37,967 | mfu: 12.20 | total time: 196.27m
step 00861/21400 (4.02%) | loss: 3.386658 | grad norm: 0.1592 | lrm: 1.00 | dt: 13802.92ms | tok/sec: 37,983 | mfu: 12.20 | total time: 196.50m
step 00862/21400 (4.03%) | loss: 3.360865 | grad norm: 0.1709 | lrm: 1.00 | dt: 13898.74ms | tok/sec: 37,721 | mfu: 12.12 | total time: 196.74m
step 00863/21400 (4.03%) | loss: 3.385628 | grad norm: 0.1827 | lrm: 1.00 | dt: 13737.51ms | tok/sec: 38,164 | mfu: 12.26 | total time: 196.96m
step 00864/21400 (4.04%) | loss: 3.396630 | grad norm: 0.1825 | lrm: 1.00 | dt: 13870.69ms | tok/sec: 37,798 | mfu: 12.14 | total time: 197.20m
step 00865/21400 (4.04%) | loss: 3.395891 | grad norm: 0.1584 | lrm: 1.00 | dt: 13757.39ms | tok/sec: 38,109 | mfu: 12.24 | total time: 197.42m
step 00866/21400 (4.05%) | loss: 3.409798 | grad norm: 0.1488 | lrm: 1.00 | dt: 13864.58ms | tok/sec: 37,814 | mfu: 12.15 | total time: 197.66m
step 00867/21400 (4.05%) | loss: 3.382021 | grad norm: 0.1494 | lrm: 1.00 | dt: 13750.46ms | tok/sec: 38,128 | mfu: 12.25 | total time: 197.88m
step 00868/21400 (4.06%) | loss: 3.338633 | grad norm: 0.1268 | lrm: 1.00 | dt: 13863.13ms | tok/sec: 37,818 | mfu: 12.15 | total time: 198.12m
step 00869/21400 (4.06%) | loss: 3.304613 | grad norm: 0.1344 | lrm: 1.00 | dt: 13769.07ms | tok/sec: 38,077 | mfu: 12.23 | total time: 198.35m
step 00870/21400 (4.07%) | loss: 3.303501 | grad norm: 0.1797 | lrm: 1.00 | dt: 13847.21ms | tok/sec: 37,862 | mfu: 12.16 | total time: 198.58m
step 00871/21400 (4.07%) | loss: 3.317309 | grad norm: 0.1754 | lrm: 1.00 | dt: 13796.43ms | tok/sec: 38,001 | mfu: 12.21 | total time: 198.81m
step 00872/21400 (4.07%) | loss: 3.320852 | grad norm: 0.1572 | lrm: 1.00 | dt: 13826.40ms | tok/sec: 37,919 | mfu: 12.18 | total time: 199.04m
step 00873/21400 (4.08%) | loss: 3.402116 | grad norm: 0.1556 | lrm: 1.00 | dt: 13818.37ms | tok/sec: 37,941 | mfu: 12.19 | total time: 199.27m
step 00874/21400 (4.08%) | loss: 3.461473 | grad norm: 0.1676 | lrm: 1.00 | dt: 13789.45ms | tok/sec: 38,020 | mfu: 12.21 | total time: 199.50m
step 00875/21400 (4.09%) | loss: 3.460130 | grad norm: 0.1769 | lrm: 1.00 | dt: 13887.90ms | tok/sec: 37,751 | mfu: 12.13 | total time: 199.73m
step 00876/21400 (4.09%) | loss: 3.502094 | grad norm: 0.1672 | lrm: 1.00 | dt: 13724.91ms | tok/sec: 38,199 | mfu: 12.27 | total time: 199.96m
step 00877/21400 (4.10%) | loss: 3.507069 | grad norm: 0.1527 | lrm: 1.00 | dt: 13870.32ms | tok/sec: 37,799 | mfu: 12.14 | total time: 200.19m
step 00878/21400 (4.10%) | loss: 3.466367 | grad norm: 0.1651 | lrm: 1.00 | dt: 13733.32ms | tok/sec: 38,176 | mfu: 12.26 | total time: 200.42m
step 00879/21400 (4.11%) | loss: 3.463594 | grad norm: 0.1715 | lrm: 1.00 | dt: 13860.55ms | tok/sec: 37,825 | mfu: 12.15 | total time: 200.65m
step 00880/21400 (4.11%) | loss: 3.465712 | grad norm: 0.1619 | lrm: 1.00 | dt: 13932.71ms | tok/sec: 37,630 | mfu: 12.09 | total time: 200.88m
step 00881/21400 (4.12%) | loss: 3.550240 | grad norm: 0.1670 | lrm: 1.00 | dt: 13967.70ms | tok/sec: 37,535 | mfu: 12.06 | total time: 201.11m
step 00882/21400 (4.12%) | loss: 3.547737 | grad norm: 0.1656 | lrm: 1.00 | dt: 13870.71ms | tok/sec: 37,798 | mfu: 12.14 | total time: 201.34m
step 00883/21400 (4.13%) | loss: 3.536964 | grad norm: 0.1596 | lrm: 1.00 | dt: 14169.10ms | tok/sec: 37,002 | mfu: 11.89 | total time: 201.58m
step 00884/21400 (4.13%) | loss: 3.556314 | grad norm: 0.1493 | lrm: 1.00 | dt: 13992.49ms | tok/sec: 37,469 | mfu: 12.04 | total time: 201.81m
step 00885/21400 (4.14%) | loss: 3.527638 | grad norm: 0.1534 | lrm: 1.00 | dt: 14117.31ms | tok/sec: 37,137 | mfu: 11.93 | total time: 202.05m
step 00886/21400 (4.14%) | loss: 3.505437 | grad norm: 0.1583 | lrm: 1.00 | dt: 14012.26ms | tok/sec: 37,416 | mfu: 12.02 | total time: 202.28m
step 00887/21400 (4.14%) | loss: 3.463664 | grad norm: 0.1644 | lrm: 1.00 | dt: 14094.03ms | tok/sec: 37,199 | mfu: 11.95 | total time: 202.52m
step 00888/21400 (4.15%) | loss: 3.479021 | grad norm: 0.1756 | lrm: 1.00 | dt: 14062.19ms | tok/sec: 37,283 | mfu: 11.98 | total time: 202.75m
step 00889/21400 (4.15%) | loss: 3.501103 | grad norm: 0.1615 | lrm: 1.00 | dt: 14678.19ms | tok/sec: 35,718 | mfu: 11.47 | total time: 203.00m
step 00890/21400 (4.16%) | loss: 3.507422 | grad norm: 0.1544 | lrm: 1.00 | dt: 14052.88ms | tok/sec: 37,308 | mfu: 11.99 | total time: 203.23m
step 00891/21400 (4.16%) | loss: 3.495277 | grad norm: 0.1517 | lrm: 1.00 | dt: 14064.22ms | tok/sec: 37,278 | mfu: 11.98 | total time: 203.46m
step 00892/21400 (4.17%) | loss: 3.495471 | grad norm: 0.1489 | lrm: 1.00 | dt: 14291.16ms | tok/sec: 36,686 | mfu: 11.79 | total time: 203.70m
step 00893/21400 (4.17%) | loss: 3.468156 | grad norm: 0.1507 | lrm: 1.00 | dt: 14149.22ms | tok/sec: 37,054 | mfu: 11.90 | total time: 203.94m
step 00894/21400 (4.18%) | loss: 3.448166 | grad norm: 0.1503 | lrm: 1.00 | dt: 14183.65ms | tok/sec: 36,964 | mfu: 11.87 | total time: 204.18m
step 00895/21400 (4.18%) | loss: 3.393836 | grad norm: 0.1415 | lrm: 1.00 | dt: 14029.30ms | tok/sec: 37,370 | mfu: 12.01 | total time: 204.41m
step 00896/21400 (4.19%) | loss: 3.400587 | grad norm: 0.1441 | lrm: 1.00 | dt: 14209.11ms | tok/sec: 36,898 | mfu: 11.85 | total time: 204.65m
step 00897/21400 (4.19%) | loss: 3.454352 | grad norm: 0.1566 | lrm: 1.00 | dt: 14050.68ms | tok/sec: 37,314 | mfu: 11.99 | total time: 204.88m
step 00898/21400 (4.20%) | loss: 3.438143 | grad norm: 0.1597 | lrm: 1.00 | dt: 14191.27ms | tok/sec: 36,944 | mfu: 11.87 | total time: 205.12m
step 00899/21400 (4.20%) | loss: 3.434928 | grad norm: 0.1913 | lrm: 1.00 | dt: 14044.00ms | tok/sec: 37,331 | mfu: 11.99 | total time: 205.35m
step 00900/21400 (4.21%) | loss: 3.438064 | grad norm: 0.1755 | lrm: 1.00 | dt: 14146.99ms | tok/sec: 37,060 | mfu: 11.91 | total time: 205.59m
step 00901/21400 (4.21%) | loss: 3.466317 | grad norm: 0.1803 | lrm: 1.00 | dt: 14051.11ms | tok/sec: 37,312 | mfu: 11.99 | total time: 205.82m
step 00902/21400 (4.21%) | loss: 3.494776 | grad norm: 0.1660 | lrm: 1.00 | dt: 14189.75ms | tok/sec: 36,948 | mfu: 11.87 | total time: 206.06m
step 00903/21400 (4.22%) | loss: 3.475861 | grad norm: 0.1714 | lrm: 1.00 | dt: 13925.36ms | tok/sec: 37,649 | mfu: 12.10 | total time: 206.29m
step 00904/21400 (4.22%) | loss: 3.460118 | grad norm: 0.1771 | lrm: 1.00 | dt: 14254.67ms | tok/sec: 36,780 | mfu: 11.82 | total time: 206.53m
step 00905/21400 (4.23%) | loss: 3.486577 | grad norm: 0.1687 | lrm: 1.00 | dt: 13898.77ms | tok/sec: 37,721 | mfu: 12.12 | total time: 206.76m
step 00906/21400 (4.23%) | loss: 3.448120 | grad norm: 0.1562 | lrm: 1.00 | dt: 13845.49ms | tok/sec: 37,867 | mfu: 12.16 | total time: 206.99m
step 00907/21400 (4.24%) | loss: 3.446282 | grad norm: 0.1451 | lrm: 1.00 | dt: 13916.99ms | tok/sec: 37,672 | mfu: 12.10 | total time: 207.22m
step 00908/21400 (4.24%) | loss: 3.482841 | grad norm: 0.1414 | lrm: 1.00 | dt: 13809.37ms | tok/sec: 37,966 | mfu: 12.20 | total time: 207.45m
step 00909/21400 (4.25%) | loss: 3.457720 | grad norm: 0.1499 | lrm: 1.00 | dt: 14072.79ms | tok/sec: 37,255 | mfu: 11.97 | total time: 207.69m
step 00910/21400 (4.25%) | loss: 3.465966 | grad norm: 0.1468 | lrm: 1.00 | dt: 13903.44ms | tok/sec: 37,709 | mfu: 12.11 | total time: 207.92m
step 00911/21400 (4.26%) | loss: 3.475948 | grad norm: 0.1522 | lrm: 1.00 | dt: 14024.11ms | tok/sec: 37,384 | mfu: 12.01 | total time: 208.15m
step 00912/21400 (4.26%) | loss: 3.431690 | grad norm: 0.1576 | lrm: 1.00 | dt: 14004.45ms | tok/sec: 37,437 | mfu: 12.03 | total time: 208.38m
step 00913/21400 (4.27%) | loss: 3.386026 | grad norm: 0.1562 | lrm: 1.00 | dt: 14071.87ms | tok/sec: 37,257 | mfu: 11.97 | total time: 208.62m
step 00914/21400 (4.27%) | loss: 3.413233 | grad norm: 0.1620 | lrm: 1.00 | dt: 14082.54ms | tok/sec: 37,229 | mfu: 11.96 | total time: 208.85m
step 00915/21400 (4.28%) | loss: 3.404847 | grad norm: 0.1532 | lrm: 1.00 | dt: 13954.89ms | tok/sec: 37,570 | mfu: 12.07 | total time: 209.09m
step 00916/21400 (4.28%) | loss: 3.421808 | grad norm: 0.1819 | lrm: 1.00 | dt: 13870.21ms | tok/sec: 37,799 | mfu: 12.14 | total time: 209.32m
step 00917/21400 (4.29%) | loss: 3.380794 | grad norm: 0.1908 | lrm: 1.00 | dt: 13879.07ms | tok/sec: 37,775 | mfu: 12.14 | total time: 209.55m
step 00918/21400 (4.29%) | loss: 3.381806 | grad norm: 0.1721 | lrm: 1.00 | dt: 13943.66ms | tok/sec: 37,600 | mfu: 12.08 | total time: 209.78m
step 00919/21400 (4.29%) | loss: 3.408114 | grad norm: 0.1598 | lrm: 1.00 | dt: 13844.02ms | tok/sec: 37,871 | mfu: 12.17 | total time: 210.01m
step 00920/21400 (4.30%) | loss: 3.392269 | grad norm: 0.1622 | lrm: 1.00 | dt: 14968.26ms | tok/sec: 35,026 | mfu: 11.25 | total time: 210.26m
step 00921/21400 (4.30%) | loss: 3.406135 | grad norm: 0.1534 | lrm: 1.00 | dt: 13809.42ms | tok/sec: 37,965 | mfu: 12.20 | total time: 210.49m
step 00922/21400 (4.31%) | loss: 3.388942 | grad norm: 0.1408 | lrm: 1.00 | dt: 14002.99ms | tok/sec: 37,441 | mfu: 12.03 | total time: 210.73m
step 00923/21400 (4.31%) | loss: 3.410096 | grad norm: 0.1659 | lrm: 1.00 | dt: 13903.21ms | tok/sec: 37,709 | mfu: 12.11 | total time: 210.96m
step 00924/21400 (4.32%) | loss: 3.410235 | grad norm: 0.1700 | lrm: 1.00 | dt: 14145.29ms | tok/sec: 37,064 | mfu: 11.91 | total time: 211.19m
step 00925/21400 (4.32%) | loss: 3.418370 | grad norm: 0.1676 | lrm: 1.00 | dt: 13979.33ms | tok/sec: 37,504 | mfu: 12.05 | total time: 211.43m
step 00926/21400 (4.33%) | loss: 3.404015 | grad norm: 0.1764 | lrm: 1.00 | dt: 13960.67ms | tok/sec: 37,554 | mfu: 12.06 | total time: 211.66m
step 00927/21400 (4.33%) | loss: 3.376090 | grad norm: 0.1740 | lrm: 1.00 | dt: 13848.42ms | tok/sec: 37,859 | mfu: 12.16 | total time: 211.89m
step 00928/21400 (4.34%) | loss: 3.355884 | grad norm: 0.1657 | lrm: 1.00 | dt: 13956.37ms | tok/sec: 37,566 | mfu: 12.07 | total time: 212.12m
step 00929/21400 (4.34%) | loss: 3.387696 | grad norm: 0.1630 | lrm: 1.00 | dt: 13836.32ms | tok/sec: 37,892 | mfu: 12.17 | total time: 212.35m
step 00930/21400 (4.35%) | loss: 3.382580 | grad norm: 0.1542 | lrm: 1.00 | dt: 13926.50ms | tok/sec: 37,646 | mfu: 12.09 | total time: 212.58m
step 00931/21400 (4.35%) | loss: 3.426709 | grad norm: 0.1581 | lrm: 1.00 | dt: 13889.73ms | tok/sec: 37,746 | mfu: 12.13 | total time: 212.82m
step 00932/21400 (4.36%) | loss: 3.446198 | grad norm: 0.1562 | lrm: 1.00 | dt: 13853.89ms | tok/sec: 37,844 | mfu: 12.16 | total time: 213.05m
step 00933/21400 (4.36%) | loss: 3.409681 | grad norm: 0.1645 | lrm: 1.00 | dt: 13971.44ms | tok/sec: 37,525 | mfu: 12.06 | total time: 213.28m
step 00934/21400 (4.36%) | loss: 3.433824 | grad norm: 0.1467 | lrm: 1.00 | dt: 13827.80ms | tok/sec: 37,915 | mfu: 12.18 | total time: 213.51m
step 00935/21400 (4.37%) | loss: 3.406063 | grad norm: 0.1400 | lrm: 1.00 | dt: 13926.32ms | tok/sec: 37,647 | mfu: 12.09 | total time: 213.74m
step 00936/21400 (4.37%) | loss: 3.408487 | grad norm: 0.1443 | lrm: 1.00 | dt: 13847.05ms | tok/sec: 37,862 | mfu: 12.16 | total time: 213.97m
step 00937/21400 (4.38%) | loss: 3.398699 | grad norm: 0.1430 | lrm: 1.00 | dt: 13977.04ms | tok/sec: 37,510 | mfu: 12.05 | total time: 214.21m
step 00938/21400 (4.38%) | loss: 3.449245 | grad norm: 0.1484 | lrm: 1.00 | dt: 13811.95ms | tok/sec: 37,959 | mfu: 12.19 | total time: 214.44m
step 00939/21400 (4.39%) | loss: 3.422653 | grad norm: 0.1568 | lrm: 1.00 | dt: 13970.68ms | tok/sec: 37,527 | mfu: 12.06 | total time: 214.67m
step 00940/21400 (4.39%) | loss: 3.422477 | grad norm: 0.1575 | lrm: 1.00 | dt: 13814.64ms | tok/sec: 37,951 | mfu: 12.19 | total time: 214.90m
step 00941/21400 (4.40%) | loss: 3.431357 | grad norm: 0.1688 | lrm: 1.00 | dt: 13972.71ms | tok/sec: 37,522 | mfu: 12.05 | total time: 215.13m
step 00942/21400 (4.40%) | loss: 3.429967 | grad norm: 0.1539 | lrm: 1.00 | dt: 13826.61ms | tok/sec: 37,918 | mfu: 12.18 | total time: 215.36m
step 00943/21400 (4.41%) | loss: 3.434623 | grad norm: 0.1497 | lrm: 1.00 | dt: 13934.52ms | tok/sec: 37,625 | mfu: 12.09 | total time: 215.59m
step 00944/21400 (4.41%) | loss: 3.413888 | grad norm: 0.1658 | lrm: 1.00 | dt: 13862.23ms | tok/sec: 37,821 | mfu: 12.15 | total time: 215.83m
step 00945/21400 (4.42%) | loss: 3.380298 | grad norm: 0.1500 | lrm: 1.00 | dt: 13865.98ms | tok/sec: 37,811 | mfu: 12.15 | total time: 216.06m
step 00946/21400 (4.42%) | loss: 3.377521 | grad norm: 0.1577 | lrm: 1.00 | dt: 13942.12ms | tok/sec: 37,604 | mfu: 12.08 | total time: 216.29m
step 00947/21400 (4.43%) | loss: 3.395920 | grad norm: 0.1650 | lrm: 1.00 | dt: 13852.41ms | tok/sec: 37,848 | mfu: 12.16 | total time: 216.52m
step 00948/21400 (4.43%) | loss: 3.409171 | grad norm: 0.1663 | lrm: 1.00 | dt: 13927.45ms | tok/sec: 37,644 | mfu: 12.09 | total time: 216.75m
step 00949/21400 (4.43%) | loss: 3.385989 | grad norm: 0.1656 | lrm: 1.00 | dt: 13843.27ms | tok/sec: 37,873 | mfu: 12.17 | total time: 216.98m
step 00950/21400 (4.44%) | loss: 3.387606 | grad norm: 0.1677 | lrm: 1.00 | dt: 13939.44ms | tok/sec: 37,611 | mfu: 12.08 | total time: 217.22m
step 00951/21400 (4.44%) | loss: 3.365203 | grad norm: 0.1622 | lrm: 1.00 | dt: 14620.54ms | tok/sec: 35,859 | mfu: 11.52 | total time: 217.46m
step 00952/21400 (4.45%) | loss: 3.369716 | grad norm: 0.1450 | lrm: 1.00 | dt: 14230.07ms | tok/sec: 36,843 | mfu: 11.84 | total time: 217.70m
step 00953/21400 (4.45%) | loss: 3.368651 | grad norm: 0.1479 | lrm: 1.00 | dt: 13821.82ms | tok/sec: 37,931 | mfu: 12.19 | total time: 217.93m
step 00954/21400 (4.46%) | loss: 3.355462 | grad norm: 0.1600 | lrm: 1.00 | dt: 13971.19ms | tok/sec: 37,526 | mfu: 12.06 | total time: 218.16m
step 00955/21400 (4.46%) | loss: 3.377410 | grad norm: 0.1608 | lrm: 1.00 | dt: 13813.04ms | tok/sec: 37,956 | mfu: 12.19 | total time: 218.39m
step 00956/21400 (4.47%) | loss: 3.381304 | grad norm: 0.1588 | lrm: 1.00 | dt: 13963.06ms | tok/sec: 37,548 | mfu: 12.06 | total time: 218.62m
step 00957/21400 (4.47%) | loss: 3.315217 | grad norm: 0.1486 | lrm: 1.00 | dt: 13830.73ms | tok/sec: 37,907 | mfu: 12.18 | total time: 218.85m
step 00958/21400 (4.48%) | loss: 3.346317 | grad norm: 0.1496 | lrm: 1.00 | dt: 13914.76ms | tok/sec: 37,678 | mfu: 12.10 | total time: 219.08m
step 00959/21400 (4.48%) | loss: 3.376549 | grad norm: 0.1550 | lrm: 1.00 | dt: 13869.64ms | tok/sec: 37,801 | mfu: 12.14 | total time: 219.32m
step 00960/21400 (4.49%) | loss: 3.416105 | grad norm: 0.1593 | lrm: 1.00 | dt: 13861.45ms | tok/sec: 37,823 | mfu: 12.15 | total time: 219.55m
step 00961/21400 (4.49%) | loss: 3.421320 | grad norm: 0.1651 | lrm: 1.00 | dt: 13941.61ms | tok/sec: 37,605 | mfu: 12.08 | total time: 219.78m
step 00962/21400 (4.50%) | loss: 3.466339 | grad norm: 0.1622 | lrm: 1.00 | dt: 13829.36ms | tok/sec: 37,911 | mfu: 12.18 | total time: 220.01m
step 00963/21400 (4.50%) | loss: 3.467238 | grad norm: 0.1491 | lrm: 1.00 | dt: 13936.09ms | tok/sec: 37,620 | mfu: 12.09 | total time: 220.24m
step 00964/21400 (4.50%) | loss: 3.428448 | grad norm: 0.1618 | lrm: 1.00 | dt: 13869.59ms | tok/sec: 37,801 | mfu: 12.14 | total time: 220.47m
step 00965/21400 (4.51%) | loss: 3.484109 | grad norm: 0.1602 | lrm: 1.00 | dt: 13975.09ms | tok/sec: 37,515 | mfu: 12.05 | total time: 220.71m
step 00966/21400 (4.51%) | loss: 3.498859 | grad norm: 0.1456 | lrm: 1.00 | dt: 13836.26ms | tok/sec: 37,892 | mfu: 12.17 | total time: 220.94m
step 00967/21400 (4.52%) | loss: 3.479625 | grad norm: 0.1594 | lrm: 1.00 | dt: 13982.04ms | tok/sec: 37,497 | mfu: 12.05 | total time: 221.17m
step 00968/21400 (4.52%) | loss: 3.435998 | grad norm: 0.1692 | lrm: 1.00 | dt: 13830.22ms | tok/sec: 37,908 | mfu: 12.18 | total time: 221.40m
step 00969/21400 (4.53%) | loss: 3.485498 | grad norm: 0.1642 | lrm: 1.00 | dt: 13974.22ms | tok/sec: 37,518 | mfu: 12.05 | total time: 221.63m
step 00970/21400 (4.53%) | loss: 3.472896 | grad norm: 0.1638 | lrm: 1.00 | dt: 13830.97ms | tok/sec: 37,906 | mfu: 12.18 | total time: 221.86m
step 00971/21400 (4.54%) | loss: 3.452988 | grad norm: 0.1642 | lrm: 1.00 | dt: 13956.92ms | tok/sec: 37,564 | mfu: 12.07 | total time: 222.10m
step 00972/21400 (4.54%) | loss: 3.455633 | grad norm: 0.1654 | lrm: 1.00 | dt: 13835.77ms | tok/sec: 37,893 | mfu: 12.17 | total time: 222.33m
step 00973/21400 (4.55%) | loss: 3.443774 | grad norm: 0.1624 | lrm: 1.00 | dt: 13933.35ms | tok/sec: 37,628 | mfu: 12.09 | total time: 222.56m
step 00974/21400 (4.55%) | loss: 3.426397 | grad norm: 0.1530 | lrm: 1.00 | dt: 13927.29ms | tok/sec: 37,644 | mfu: 12.09 | total time: 222.79m
step 00975/21400 (4.56%) | loss: 3.409562 | grad norm: 0.1641 | lrm: 1.00 | dt: 13903.71ms | tok/sec: 37,708 | mfu: 12.11 | total time: 223.02m
step 00976/21400 (4.56%) | loss: 3.427507 | grad norm: 0.1524 | lrm: 1.00 | dt: 13790.67ms | tok/sec: 38,017 | mfu: 12.21 | total time: 223.25m
step 00977/21400 (4.57%) | loss: 3.477632 | grad norm: 0.1425 | lrm: 1.00 | dt: 13872.82ms | tok/sec: 37,792 | mfu: 12.14 | total time: 223.48m
step 00978/21400 (4.57%) | loss: 3.463716 | grad norm: 0.1608 | lrm: 1.00 | dt: 13929.14ms | tok/sec: 37,639 | mfu: 12.09 | total time: 223.72m
step 00979/21400 (4.57%) | loss: 3.457102 | grad norm: 0.1609 | lrm: 1.00 | dt: 13946.34ms | tok/sec: 37,593 | mfu: 12.08 | total time: 223.95m
step 00980/21400 (4.58%) | loss: 3.451156 | grad norm: 0.1632 | lrm: 1.00 | dt: 13910.99ms | tok/sec: 37,688 | mfu: 12.11 | total time: 224.18m
step 00981/21400 (4.58%) | loss: 3.456800 | grad norm: 0.1733 | lrm: 1.00 | dt: 13822.23ms | tok/sec: 37,930 | mfu: 12.19 | total time: 224.41m
step 00982/21400 (4.59%) | loss: 3.449336 | grad norm: 0.1539 | lrm: 1.00 | dt: 13969.93ms | tok/sec: 37,529 | mfu: 12.06 | total time: 224.64m
step 00983/21400 (4.59%) | loss: 3.450008 | grad norm: 0.1468 | lrm: 1.00 | dt: 14110.37ms | tok/sec: 37,156 | mfu: 11.94 | total time: 224.88m
step 00984/21400 (4.60%) | loss: 3.477634 | grad norm: 0.1539 | lrm: 1.00 | dt: 13972.52ms | tok/sec: 37,522 | mfu: 12.05 | total time: 225.11m
step 00985/21400 (4.60%) | loss: 3.461179 | grad norm: 0.1662 | lrm: 1.00 | dt: 13816.61ms | tok/sec: 37,946 | mfu: 12.19 | total time: 225.34m
step 00986/21400 (4.61%) | loss: 3.405519 | grad norm: 0.1709 | lrm: 1.00 | dt: 13943.76ms | tok/sec: 37,600 | mfu: 12.08 | total time: 225.57m
step 00987/21400 (4.61%) | loss: 3.401122 | grad norm: 0.1683 | lrm: 1.00 | dt: 13842.21ms | tok/sec: 37,876 | mfu: 12.17 | total time: 225.81m
step 00988/21400 (4.62%) | loss: 3.430989 | grad norm: 0.1546 | lrm: 1.00 | dt: 13926.00ms | tok/sec: 37,648 | mfu: 12.09 | total time: 226.04m
step 00989/21400 (4.62%) | loss: 3.414058 | grad norm: 0.1645 | lrm: 1.00 | dt: 13830.17ms | tok/sec: 37,909 | mfu: 12.18 | total time: 226.27m
step 00990/21400 (4.63%) | loss: 3.407617 | grad norm: 0.1561 | lrm: 1.00 | dt: 13931.60ms | tok/sec: 37,632 | mfu: 12.09 | total time: 226.50m
step 00991/21400 (4.63%) | loss: 3.430456 | grad norm: 0.1656 | lrm: 1.00 | dt: 13819.44ms | tok/sec: 37,938 | mfu: 12.19 | total time: 226.73m
step 00992/21400 (4.64%) | loss: 3.446927 | grad norm: 0.1645 | lrm: 1.00 | dt: 13972.22ms | tok/sec: 37,523 | mfu: 12.05 | total time: 226.96m
step 00993/21400 (4.64%) | loss: 3.399078 | grad norm: 0.1529 | lrm: 1.00 | dt: 13901.75ms | tok/sec: 37,713 | mfu: 12.12 | total time: 227.19m
step 00994/21400 (4.64%) | loss: 3.404914 | grad norm: 0.1577 | lrm: 1.00 | dt: 13826.11ms | tok/sec: 37,920 | mfu: 12.18 | total time: 227.43m
step 00995/21400 (4.65%) | loss: 3.412869 | grad norm: 0.1782 | lrm: 1.00 | dt: 13928.45ms | tok/sec: 37,641 | mfu: 12.09 | total time: 227.66m
step 00996/21400 (4.65%) | loss: 3.414542 | grad norm: 0.1652 | lrm: 1.00 | dt: 13819.15ms | tok/sec: 37,939 | mfu: 12.19 | total time: 227.89m
step 00997/21400 (4.66%) | loss: 3.405869 | grad norm: 0.1507 | lrm: 1.00 | dt: 13951.23ms | tok/sec: 37,580 | mfu: 12.07 | total time: 228.12m
step 00998/21400 (4.66%) | loss: 3.405307 | grad norm: 0.1508 | lrm: 1.00 | dt: 13819.17ms | tok/sec: 37,939 | mfu: 12.19 | total time: 228.35m
step 00999/21400 (4.67%) | loss: 3.426444 | grad norm: 0.1771 | lrm: 1.00 | dt: 13950.02ms | tok/sec: 37,583 | mfu: 12.07 | total time: 228.58m
Step 01000 | Validation bpb: 1.0116
step 01000/21400 (4.67%) | loss: 3.438976 | grad norm: 0.1778 | lrm: 1.00 | dt: 13851.83ms | tok/sec: 37,849 | mfu: 12.16 | total time: 228.81m
2025-11-11 02:59:27,804 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model file to: /home/henny/.cache/nanochat/base_checkpoints/d20/model_001000.pt
2025-11-11 02:59:32,185 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer file to: /home/henny/.cache/nanochat/base_checkpoints/d20/optim_001000.pt
2025-11-11 02:59:32,186 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata file to: /home/henny/.cache/nanochat/base_checkpoints/d20/meta_001000.json
step 01001/21400 (4.68%) | loss: 3.470731 | grad norm: 0.1568 | lrm: 1.00 | dt: 14712.95ms | tok/sec: 35,634 | mfu: 11.45 | total time: 229.06m
step 01002/21400 (4.68%) | loss: 3.537247 | grad norm: 0.1451 | lrm: 1.00 | dt: 13790.77ms | tok/sec: 38,017 | mfu: 12.21 | total time: 229.29m
step 01003/21400 (4.69%) | loss: 3.533053 | grad norm: 0.1678 | lrm: 1.00 | dt: 13952.03ms | tok/sec: 37,577 | mfu: 12.07 | total time: 229.52m
step 01004/21400 (4.69%) | loss: 3.513641 | grad norm: 0.1725 | lrm: 1.00 | dt: 13809.84ms | tok/sec: 37,964 | mfu: 12.20 | total time: 229.75m
step 01005/21400 (4.70%) | loss: 3.540881 | grad norm: 0.1751 | lrm: 1.00 | dt: 13945.41ms | tok/sec: 37,595 | mfu: 12.08 | total time: 229.98m
step 01006/21400 (4.70%) | loss: 3.548117 | grad norm: 0.1653 | lrm: 1.00 | dt: 13825.64ms | tok/sec: 37,921 | mfu: 12.18 | total time: 230.21m
step 01007/21400 (4.71%) | loss: 3.507560 | grad norm: 0.1496 | lrm: 1.00 | dt: 13937.11ms | tok/sec: 37,618 | mfu: 12.08 | total time: 230.45m
step 01008/21400 (4.71%) | loss: 3.510266 | grad norm: 0.1498 | lrm: 1.00 | dt: 13812.22ms | tok/sec: 37,958 | mfu: 12.19 | total time: 230.68m
step 01009/21400 (4.71%) | loss: 3.487903 | grad norm: 0.1494 | lrm: 1.00 | dt: 13958.06ms | tok/sec: 37,561 | mfu: 12.07 | total time: 230.91m
step 01010/21400 (4.72%) | loss: 3.473471 | grad norm: 0.1475 | lrm: 1.00 | dt: 13779.35ms | tok/sec: 38,048 | mfu: 12.22 | total time: 231.14m
step 01011/21400 (4.72%) | loss: 3.434338 | grad norm: 0.1497 | lrm: 1.00 | dt: 13979.86ms | tok/sec: 37,503 | mfu: 12.05 | total time: 231.37m
step 01012/21400 (4.73%) | loss: 3.425489 | grad norm: 0.1547 | lrm: 1.00 | dt: 14127.56ms | tok/sec: 37,111 | mfu: 11.92 | total time: 231.61m
step 01013/21400 (4.73%) | loss: 3.418622 | grad norm: 0.1563 | lrm: 1.00 | dt: 13926.70ms | tok/sec: 37,646 | mfu: 12.09 | total time: 231.84m
step 01014/21400 (4.74%) | loss: 3.485149 | grad norm: 0.1528 | lrm: 1.00 | dt: 13928.95ms | tok/sec: 37,640 | mfu: 12.09 | total time: 232.07m
step 01015/21400 (4.74%) | loss: 3.445632 | grad norm: 0.1512 | lrm: 1.00 | dt: 13795.70ms | tok/sec: 38,003 | mfu: 12.21 | total time: 232.30m
step 01016/21400 (4.75%) | loss: 3.426690 | grad norm: 0.1583 | lrm: 1.00 | dt: 13977.30ms | tok/sec: 37,509 | mfu: 12.05 | total time: 232.53m
step 01017/21400 (4.75%) | loss: 3.418287 | grad norm: 0.1310 | lrm: 1.00 | dt: 13804.50ms | tok/sec: 37,979 | mfu: 12.20 | total time: 232.76m
step 01018/21400 (4.76%) | loss: 3.466439 | grad norm: 0.1313 | lrm: 1.00 | dt: 13958.11ms | tok/sec: 37,561 | mfu: 12.07 | total time: 233.00m
step 01019/21400 (4.76%) | loss: 3.457599 | grad norm: 0.1463 | lrm: 1.00 | dt: 13821.42ms | tok/sec: 37,932 | mfu: 12.19 | total time: 233.23m
step 01020/21400 (4.77%) | loss: 3.453424 | grad norm: 0.1565 | lrm: 1.00 | dt: 13954.20ms | tok/sec: 37,572 | mfu: 12.07 | total time: 233.46m
step 01021/21400 (4.77%) | loss: 3.429788 | grad norm: 0.1435 | lrm: 1.00 | dt: 13847.25ms | tok/sec: 37,862 | mfu: 12.16 | total time: 233.69m
step 01022/21400 (4.78%) | loss: 3.380967 | grad norm: 0.1394 | lrm: 1.00 | dt: 13942.78ms | tok/sec: 37,602 | mfu: 12.08 | total time: 233.92m
step 01023/21400 (4.78%) | loss: 3.368217 | grad norm: 0.1503 | lrm: 1.00 | dt: 13780.66ms | tok/sec: 38,045 | mfu: 12.22 | total time: 234.15m
step 01024/21400 (4.79%) | loss: 3.388677 | grad norm: 0.1540 | lrm: 1.00 | dt: 13988.99ms | tok/sec: 37,478 | mfu: 12.04 | total time: 234.39m
step 01025/21400 (4.79%) | loss: 3.390817 | grad norm: 0.1466 | lrm: 1.00 | dt: 13803.47ms | tok/sec: 37,982 | mfu: 12.20 | total time: 234.62m
step 01026/21400 (4.79%) | loss: 3.385060 | grad norm: 0.1443 | lrm: 1.00 | dt: 13935.86ms | tok/sec: 37,621 | mfu: 12.09 | total time: 234.85m
step 01027/21400 (4.80%) | loss: 3.403554 | grad norm: 0.1548 | lrm: 1.00 | dt: 13863.50ms | tok/sec: 37,817 | mfu: 12.15 | total time: 235.08m
step 01028/21400 (4.80%) | loss: 3.406749 | grad norm: 0.1603 | lrm: 1.00 | dt: 13988.15ms | tok/sec: 37,480 | mfu: 12.04 | total time: 235.31m
step 01029/21400 (4.81%) | loss: 3.410196 | grad norm: 0.1600 | lrm: 1.00 | dt: 13777.15ms | tok/sec: 38,054 | mfu: 12.23 | total time: 235.54m
step 01030/21400 (4.81%) | loss: 3.415652 | grad norm: 0.1845 | lrm: 1.00 | dt: 13869.15ms | tok/sec: 37,802 | mfu: 12.14 | total time: 235.77m
step 01031/21400 (4.82%) | loss: 3.432600 | grad norm: 0.1561 | lrm: 1.00 | dt: 13962.63ms | tok/sec: 37,549 | mfu: 12.06 | total time: 236.01m
step 01032/21400 (4.82%) | loss: 3.401136 | grad norm: 0.1331 | lrm: 1.00 | dt: 13820.57ms | tok/sec: 37,935 | mfu: 12.19 | total time: 236.24m
step 01033/21400 (4.83%) | loss: 3.395986 | grad norm: 0.1385 | lrm: 1.00 | dt: 13945.10ms | tok/sec: 37,596 | mfu: 12.08 | total time: 236.47m
step 01034/21400 (4.83%) | loss: 3.382721 | grad norm: 0.1534 | lrm: 1.00 | dt: 13827.36ms | tok/sec: 37,916 | mfu: 12.18 | total time: 236.70m
step 01035/21400 (4.84%) | loss: 3.368402 | grad norm: 0.1692 | lrm: 1.00 | dt: 13954.22ms | tok/sec: 37,572 | mfu: 12.07 | total time: 236.93m
step 01036/21400 (4.84%) | loss: 3.323118 | grad norm: 0.1609 | lrm: 1.00 | dt: 13972.18ms | tok/sec: 37,523 | mfu: 12.05 | total time: 237.17m
step 01037/21400 (4.85%) | loss: 3.329834 | grad norm: 0.1600 | lrm: 1.00 | dt: 14104.19ms | tok/sec: 37,172 | mfu: 11.94 | total time: 237.40m
step 01038/21400 (4.85%) | loss: 3.299514 | grad norm: 0.1547 | lrm: 1.00 | dt: 13952.27ms | tok/sec: 37,577 | mfu: 12.07 | total time: 237.63m
step 01039/21400 (4.86%) | loss: 3.326075 | grad norm: 0.1481 | lrm: 1.00 | dt: 14062.54ms | tok/sec: 37,282 | mfu: 11.98 | total time: 237.87m
step 01040/21400 (4.86%) | loss: 3.325134 | grad norm: 0.1438 | lrm: 1.00 | dt: 14019.76ms | tok/sec: 37,396 | mfu: 12.01 | total time: 238.10m
step 01041/21400 (4.86%) | loss: 3.313166 | grad norm: 0.1329 | lrm: 1.00 | dt: 14049.25ms | tok/sec: 37,317 | mfu: 11.99 | total time: 238.33m
step 01042/21400 (4.87%) | loss: 3.355312 | grad norm: 0.1469 | lrm: 1.00 | dt: 13980.79ms | tok/sec: 37,500 | mfu: 12.05 | total time: 238.57m
step 01043/21400 (4.87%) | loss: 3.358512 | grad norm: 0.1380 | lrm: 1.00 | dt: 13944.18ms | tok/sec: 37,599 | mfu: 12.08 | total time: 238.80m
step 01044/21400 (4.88%) | loss: 3.350653 | grad norm: 0.1255 | lrm: 1.00 | dt: 13931.59ms | tok/sec: 37,633 | mfu: 12.09 | total time: 239.03m
step 01045/21400 (4.88%) | loss: 3.387471 | grad norm: 0.1331 | lrm: 1.00 | dt: 14071.90ms | tok/sec: 37,257 | mfu: 11.97 | total time: 239.27m
step 01046/21400 (4.89%) | loss: 3.369794 | grad norm: 0.1436 | lrm: 1.00 | dt: 13907.91ms | tok/sec: 37,697 | mfu: 12.11 | total time: 239.50m
step 01047/21400 (4.89%) | loss: 3.365613 | grad norm: 0.1423 | lrm: 1.00 | dt: 13774.92ms | tok/sec: 38,061 | mfu: 12.23 | total time: 239.73m
step 01048/21400 (4.90%) | loss: 3.372673 | grad norm: 0.1441 | lrm: 1.00 | dt: 13910.08ms | tok/sec: 37,691 | mfu: 12.11 | total time: 239.96m
step 01049/21400 (4.90%) | loss: 3.364315 | grad norm: 0.1436 | lrm: 1.00 | dt: 13781.71ms | tok/sec: 38,042 | mfu: 12.22 | total time: 240.19m
step 01050/21400 (4.91%) | loss: 3.364604 | grad norm: 0.1452 | lrm: 1.00 | dt: 13898.75ms | tok/sec: 37,721 | mfu: 12.12 | total time: 240.42m
step 01051/21400 (4.91%) | loss: 3.370204 | grad norm: 0.1374 | lrm: 1.00 | dt: 13786.41ms | tok/sec: 38,029 | mfu: 12.22 | total time: 240.65m
step 01052/21400 (4.92%) | loss: 3.404937 | grad norm: 0.1469 | lrm: 1.00 | dt: 13905.93ms | tok/sec: 37,702 | mfu: 12.11 | total time: 240.88m
step 01053/21400 (4.92%) | loss: 3.425474 | grad norm: 0.1572 | lrm: 1.00 | dt: 13822.72ms | tok/sec: 37,929 | mfu: 12.18 | total time: 241.11m
step 01054/21400 (4.93%) | loss: 3.358341 | grad norm: 0.1591 | lrm: 1.00 | dt: 13866.97ms | tok/sec: 37,808 | mfu: 12.15 | total time: 241.34m
step 01055/21400 (4.93%) | loss: 3.370342 | grad norm: 0.1776 | lrm: 1.00 | dt: 13856.23ms | tok/sec: 37,837 | mfu: 12.16 | total time: 241.58m
step 01056/21400 (4.93%) | loss: 3.363504 | grad norm: 0.1535 | lrm: 1.00 | dt: 13849.51ms | tok/sec: 37,856 | mfu: 12.16 | total time: 241.81m
step 01057/21400 (4.94%) | loss: 3.377609 | grad norm: 0.1322 | lrm: 1.00 | dt: 13923.53ms | tok/sec: 37,654 | mfu: 12.10 | total time: 242.04m
step 01058/21400 (4.94%) | loss: 3.359349 | grad norm: 0.1464 | lrm: 1.00 | dt: 13772.01ms | tok/sec: 38,069 | mfu: 12.23 | total time: 242.27m
step 01059/21400 (4.95%) | loss: 3.372483 | grad norm: 0.1431 | lrm: 1.00 | dt: 13912.00ms | tok/sec: 37,686 | mfu: 12.11 | total time: 242.50m
step 01060/21400 (4.95%) | loss: 3.362987 | grad norm: 0.1390 | lrm: 1.00 | dt: 13778.95ms | tok/sec: 38,049 | mfu: 12.22 | total time: 242.73m
step 01061/21400 (4.96%) | loss: 3.362977 | grad norm: 0.1364 | lrm: 1.00 | dt: 13926.00ms | tok/sec: 37,648 | mfu: 12.09 | total time: 242.96m
step 01062/21400 (4.96%) | loss: 3.369352 | grad norm: 0.1469 | lrm: 1.00 | dt: 13775.61ms | tok/sec: 38,059 | mfu: 12.23 | total time: 243.19m
step 01063/21400 (4.97%) | loss: 3.397794 | grad norm: 0.1567 | lrm: 1.00 | dt: 13921.20ms | tok/sec: 37,661 | mfu: 12.10 | total time: 243.42m
step 01064/21400 (4.97%) | loss: 3.444548 | grad norm: 0.1564 | lrm: 1.00 | dt: 13795.04ms | tok/sec: 38,005 | mfu: 12.21 | total time: 243.65m
step 01065/21400 (4.98%) | loss: 3.437646 | grad norm: 0.1453 | lrm: 1.00 | dt: 13894.76ms | tok/sec: 37,732 | mfu: 12.12 | total time: 243.88m
step 01066/21400 (4.98%) | loss: 3.418373 | grad norm: 0.1366 | lrm: 1.00 | dt: 13815.50ms | tok/sec: 37,949 | mfu: 12.19 | total time: 244.11m
step 01067/21400 (4.99%) | loss: 3.418953 | grad norm: 0.1363 | lrm: 1.00 | dt: 13883.98ms | tok/sec: 37,762 | mfu: 12.13 | total time: 244.35m
step 01068/21400 (4.99%) | loss: 3.419070 | grad norm: 0.1399 | lrm: 1.00 | dt: 13844.08ms | tok/sec: 37,870 | mfu: 12.17 | total time: 244.58m
step 01069/21400 (5.00%) | loss: 3.435310 | grad norm: 0.1468 | lrm: 1.00 | dt: 13846.94ms | tok/sec: 37,863 | mfu: 12.16 | total time: 244.81m
step 01070/21400 (5.00%) | loss: 3.479329 | grad norm: 0.1586 | lrm: 1.00 | dt: 13931.64ms | tok/sec: 37,632 | mfu: 12.09 | total time: 245.04m
step 01071/21400 (5.00%) | loss: 3.450789 | grad norm: 0.1527 | lrm: 1.00 | dt: 13784.09ms | tok/sec: 38,035 | mfu: 12.22 | total time: 245.27m
step 01072/21400 (5.01%) | loss: 3.453296 | grad norm: 0.1388 | lrm: 1.00 | dt: 13930.71ms | tok/sec: 37,635 | mfu: 12.09 | total time: 245.50m
step 01073/21400 (5.01%) | loss: 3.449940 | grad norm: 0.1524 | lrm: 1.00 | dt: 13772.91ms | tok/sec: 38,066 | mfu: 12.23 | total time: 245.73m
step 01074/21400 (5.02%) | loss: 3.420741 | grad norm: 0.1802 | lrm: 1.00 | dt: 13914.14ms | tok/sec: 37,680 | mfu: 12.10 | total time: 245.96m
step 01075/21400 (5.02%) | loss: 3.399863 | grad norm: 0.1699 | lrm: 1.00 | dt: 13780.44ms | tok/sec: 38,045 | mfu: 12.22 | total time: 246.19m
step 01076/21400 (5.03%) | loss: 3.367021 | grad norm: 0.1698 | lrm: 1.00 | dt: 13914.97ms | tok/sec: 37,677 | mfu: 12.10 | total time: 246.43m
step 01077/21400 (5.03%) | loss: 3.382982 | grad norm: 0.1627 | lrm: 1.00 | dt: 13780.99ms | tok/sec: 38,044 | mfu: 12.22 | total time: 246.65m
step 01078/21400 (5.04%) | loss: 3.389557 | grad norm: 0.1573 | lrm: 1.00 | dt: 14268.02ms | tok/sec: 36,745 | mfu: 11.80 | total time: 246.89m
step 01079/21400 (5.04%) | loss: 3.382103 | grad norm: 0.1462 | lrm: 1.00 | dt: 13805.62ms | tok/sec: 37,976 | mfu: 12.20 | total time: 247.12m
step 01080/21400 (5.05%) | loss: 3.387749 | grad norm: 0.1549 | lrm: 1.00 | dt: 13872.85ms | tok/sec: 37,792 | mfu: 12.14 | total time: 247.35m
step 01081/21400 (5.05%) | loss: 3.381066 | grad norm: 0.1502 | lrm: 1.00 | dt: 13822.83ms | tok/sec: 37,929 | mfu: 12.18 | total time: 247.58m
step 01082/21400 (5.06%) | loss: 3.377430 | grad norm: 0.1324 | lrm: 1.00 | dt: 13862.29ms | tok/sec: 37,821 | mfu: 12.15 | total time: 247.82m
step 01083/21400 (5.06%) | loss: 3.359611 | grad norm: 0.1284 | lrm: 1.00 | dt: 13872.91ms | tok/sec: 37,792 | mfu: 12.14 | total time: 248.05m
step 01084/21400 (5.07%) | loss: 3.360054 | grad norm: 0.1541 | lrm: 1.00 | dt: 13834.65ms | tok/sec: 37,896 | mfu: 12.17 | total time: 248.28m
step 01085/21400 (5.07%) | loss: 3.341742 | grad norm: 0.1426 | lrm: 1.00 | dt: 13935.99ms | tok/sec: 37,621 | mfu: 12.09 | total time: 248.51m
step 01086/21400 (5.07%) | loss: 3.348160 | grad norm: 0.1522 | lrm: 1.00 | dt: 13762.64ms | tok/sec: 38,095 | mfu: 12.24 | total time: 248.74m
step 01087/21400 (5.08%) | loss: 3.346934 | grad norm: 0.1587 | lrm: 1.00 | dt: 13930.39ms | tok/sec: 37,636 | mfu: 12.09 | total time: 248.97m
step 01088/21400 (5.08%) | loss: 3.335331 | grad norm: 0.1455 | lrm: 1.00 | dt: 13776.37ms | tok/sec: 38,057 | mfu: 12.23 | total time: 249.20m
step 01089/21400 (5.09%) | loss: 3.336557 | grad norm: 0.1389 | lrm: 1.00 | dt: 13918.93ms | tok/sec: 37,667 | mfu: 12.10 | total time: 249.43m
step 01090/21400 (5.09%) | loss: 3.326815 | grad norm: 0.1540 | lrm: 1.00 | dt: 13818.62ms | tok/sec: 37,940 | mfu: 12.19 | total time: 249.66m
step 01091/21400 (5.10%) | loss: 3.324983 | grad norm: 0.1503 | lrm: 1.00 | dt: 13892.56ms | tok/sec: 37,738 | mfu: 12.12 | total time: 249.89m
step 01092/21400 (5.10%) | loss: 3.438951 | grad norm: 0.1414 | lrm: 1.00 | dt: 13797.63ms | tok/sec: 37,998 | mfu: 12.21 | total time: 250.12m
step 01093/21400 (5.11%) | loss: 3.414703 | grad norm: 0.1564 | lrm: 1.00 | dt: 13891.19ms | tok/sec: 37,742 | mfu: 12.12 | total time: 250.36m
step 01094/21400 (5.11%) | loss: 3.397634 | grad norm: 0.1582 | lrm: 1.00 | dt: 13821.01ms | tok/sec: 37,934 | mfu: 12.19 | total time: 250.59m
step 01095/21400 (5.12%) | loss: 3.407563 | grad norm: 0.1560 | lrm: 1.00 | dt: 13868.68ms | tok/sec: 37,803 | mfu: 12.14 | total time: 250.82m
step 01096/21400 (5.12%) | loss: 3.340504 | grad norm: 0.1593 | lrm: 1.00 | dt: 13863.16ms | tok/sec: 37,818 | mfu: 12.15 | total time: 251.05m
step 01097/21400 (5.13%) | loss: 3.352304 | grad norm: 0.1519 | lrm: 1.00 | dt: 13834.86ms | tok/sec: 37,896 | mfu: 12.17 | total time: 251.28m
step 01098/21400 (5.13%) | loss: 3.346085 | grad norm: 0.1463 | lrm: 1.00 | dt: 13945.93ms | tok/sec: 37,594 | mfu: 12.08 | total time: 251.51m
step 01099/21400 (5.14%) | loss: 3.355653 | grad norm: 0.1498 | lrm: 1.00 | dt: 13759.05ms | tok/sec: 38,104 | mfu: 12.24 | total time: 251.74m
step 01100/21400 (5.14%) | loss: 3.376814 | grad norm: 0.1529 | lrm: 1.00 | dt: 13922.50ms | tok/sec: 37,657 | mfu: 12.10 | total time: 251.97m
step 01101/21400 (5.14%) | loss: 3.320888 | grad norm: 0.1383 | lrm: 1.00 | dt: 13772.90ms | tok/sec: 38,066 | mfu: 12.23 | total time: 252.20m
step 01102/21400 (5.15%) | loss: 3.307740 | grad norm: 0.1334 | lrm: 1.00 | dt: 13925.37ms | tok/sec: 37,649 | mfu: 12.10 | total time: 252.43m
step 01103/21400 (5.15%) | loss: 3.315972 | grad norm: 0.1493 | lrm: 1.00 | dt: 13791.28ms | tok/sec: 38,015 | mfu: 12.21 | total time: 252.66m
step 01104/21400 (5.16%) | loss: 3.333718 | grad norm: 0.1491 | lrm: 1.00 | dt: 13895.37ms | tok/sec: 37,731 | mfu: 12.12 | total time: 252.90m
step 01105/21400 (5.16%) | loss: 3.331827 | grad norm: 0.1374 | lrm: 1.00 | dt: 13792.00ms | tok/sec: 38,013 | mfu: 12.21 | total time: 253.13m
step 01106/21400 (5.17%) | loss: 3.336341 | grad norm: 0.1349 | lrm: 1.00 | dt: 13907.40ms | tok/sec: 37,698 | mfu: 12.11 | total time: 253.36m
step 01107/21400 (5.17%) | loss: 3.268441 | grad norm: 0.1501 | lrm: 1.00 | dt: 13829.31ms | tok/sec: 37,911 | mfu: 12.18 | total time: 253.59m
step 01108/21400 (5.18%) | loss: 3.279395 | grad norm: 0.1469 | lrm: 1.00 | dt: 13870.27ms | tok/sec: 37,799 | mfu: 12.14 | total time: 253.82m
step 01109/21400 (5.18%) | loss: 3.253302 | grad norm: 0.1379 | lrm: 1.00 | dt: 13865.21ms | tok/sec: 37,813 | mfu: 12.15 | total time: 254.05m
step 01110/21400 (5.19%) | loss: 3.227293 | grad norm: 0.1384 | lrm: 1.00 | dt: 13837.78ms | tok/sec: 37,888 | mfu: 12.17 | total time: 254.28m
step 01111/21400 (5.19%) | loss: 3.233729 | grad norm: 0.1546 | lrm: 1.00 | dt: 14235.69ms | tok/sec: 36,829 | mfu: 11.83 | total time: 254.52m
step 01112/21400 (5.20%) | loss: 3.246029 | grad norm: 0.1458 | lrm: 1.00 | dt: 13767.44ms | tok/sec: 38,081 | mfu: 12.23 | total time: 254.75m
step 01113/21400 (5.20%) | loss: 3.235795 | grad norm: 0.1494 | lrm: 1.00 | dt: 13922.70ms | tok/sec: 37,657 | mfu: 12.10 | total time: 254.98m
step 01114/21400 (5.21%) | loss: 3.247139 | grad norm: 0.1506 | lrm: 1.00 | dt: 13776.43ms | tok/sec: 38,056 | mfu: 12.23 | total time: 255.21m
step 01115/21400 (5.21%) | loss: 3.261742 | grad norm: 0.1417 | lrm: 1.00 | dt: 13930.09ms | tok/sec: 37,637 | mfu: 12.09 | total time: 255.44m
step 01116/21400 (5.21%) | loss: 3.301567 | grad norm: 0.1371 | lrm: 1.00 | dt: 13755.53ms | tok/sec: 38,114 | mfu: 12.24 | total time: 255.67m
step 01117/21400 (5.22%) | loss: 3.268679 | grad norm: 0.1372 | lrm: 1.00 | dt: 13902.21ms | tok/sec: 37,712 | mfu: 12.12 | total time: 255.90m
step 01118/21400 (5.22%) | loss: 3.259881 | grad norm: 0.1402 | lrm: 1.00 | dt: 13779.30ms | tok/sec: 38,048 | mfu: 12.22 | total time: 256.13m
step 01119/21400 (5.23%) | loss: 3.318954 | grad norm: 0.1508 | lrm: 1.00 | dt: 13904.62ms | tok/sec: 37,706 | mfu: 12.11 | total time: 256.36m
step 01120/21400 (5.23%) | loss: 3.329103 | grad norm: 0.1414 | lrm: 1.00 | dt: 13798.25ms | tok/sec: 37,996 | mfu: 12.21 | total time: 256.59m
step 01121/21400 (5.24%) | loss: 3.354364 | grad norm: 0.1397 | lrm: 1.00 | dt: 13892.02ms | tok/sec: 37,740 | mfu: 12.12 | total time: 256.83m
step 01122/21400 (5.24%) | loss: 3.349321 | grad norm: 0.1504 | lrm: 1.00 | dt: 13830.24ms | tok/sec: 37,908 | mfu: 12.18 | total time: 257.06m
step 01123/21400 (5.25%) | loss: 3.377008 | grad norm: 0.1624 | lrm: 1.00 | dt: 13848.51ms | tok/sec: 37,858 | mfu: 12.16 | total time: 257.29m
step 01124/21400 (5.25%) | loss: 3.386485 | grad norm: 0.1459 | lrm: 1.00 | dt: 13928.47ms | tok/sec: 37,641 | mfu: 12.09 | total time: 257.52m
step 01125/21400 (5.26%) | loss: 3.428336 | grad norm: 0.1627 | lrm: 1.00 | dt: 13756.53ms | tok/sec: 38,111 | mfu: 12.24 | total time: 257.75m
step 01126/21400 (5.26%) | loss: 3.386219 | grad norm: 0.1561 | lrm: 1.00 | dt: 13933.76ms | tok/sec: 37,627 | mfu: 12.09 | total time: 257.98m
step 01127/21400 (5.27%) | loss: 3.357229 | grad norm: 0.1244 | lrm: 1.00 | dt: 13768.43ms | tok/sec: 38,078 | mfu: 12.23 | total time: 258.21m
step 01128/21400 (5.27%) | loss: 3.376772 | grad norm: 0.1275 | lrm: 1.00 | dt: 13920.47ms | tok/sec: 37,663 | mfu: 12.10 | total time: 258.44m
step 01129/21400 (5.28%) | loss: 3.402276 | grad norm: 0.1534 | lrm: 1.00 | dt: 13777.08ms | tok/sec: 38,055 | mfu: 12.23 | total time: 258.67m
step 01130/21400 (5.28%) | loss: 3.396990 | grad norm: 0.1519 | lrm: 1.00 | dt: 13909.25ms | tok/sec: 37,693 | mfu: 12.11 | total time: 258.90m
step 01131/21400 (5.29%) | loss: 3.311740 | grad norm: 0.1362 | lrm: 1.00 | dt: 13781.44ms | tok/sec: 38,043 | mfu: 12.22 | total time: 259.13m
step 01132/21400 (5.29%) | loss: 3.380691 | grad norm: 0.1378 | lrm: 1.00 | dt: 13899.99ms | tok/sec: 37,718 | mfu: 12.12 | total time: 259.36m
step 01133/21400 (5.29%) | loss: 3.371275 | grad norm: 0.1528 | lrm: 1.00 | dt: 13808.49ms | tok/sec: 37,968 | mfu: 12.20 | total time: 259.59m
step 01134/21400 (5.30%) | loss: 3.384760 | grad norm: 0.1393 | lrm: 1.00 | dt: 13886.43ms | tok/sec: 37,755 | mfu: 12.13 | total time: 259.83m
step 01135/21400 (5.30%) | loss: 3.380231 | grad norm: 0.1462 | lrm: 1.00 | dt: 13839.68ms | tok/sec: 37,882 | mfu: 12.17 | total time: 260.06m
step 01136/21400 (5.31%) | loss: 3.382742 | grad norm: 0.1592 | lrm: 1.00 | dt: 13852.33ms | tok/sec: 37,848 | mfu: 12.16 | total time: 260.29m
step 01137/21400 (5.31%) | loss: 3.356260 | grad norm: 0.1483 | lrm: 1.00 | dt: 13856.45ms | tok/sec: 37,837 | mfu: 12.16 | total time: 260.52m
step 01138/21400 (5.32%) | loss: 3.338983 | grad norm: 0.1461 | lrm: 1.00 | dt: 13825.33ms | tok/sec: 37,922 | mfu: 12.18 | total time: 260.75m
step 01139/21400 (5.32%) | loss: 3.318440 | grad norm: 0.1523 | lrm: 1.00 | dt: 13941.34ms | tok/sec: 37,606 | mfu: 12.08 | total time: 260.98m
step 01140/21400 (5.33%) | loss: 3.318276 | grad norm: 0.1701 | lrm: 1.00 | dt: 13755.30ms | tok/sec: 38,115 | mfu: 12.24 | total time: 261.21m
step 01141/21400 (5.33%) | loss: 3.321180 | grad norm: 0.1705 | lrm: 1.00 | dt: 13939.46ms | tok/sec: 37,611 | mfu: 12.08 | total time: 261.44m
step 01142/21400 (5.34%) | loss: 3.294133 | grad norm: 0.1493 | lrm: 1.00 | dt: 13757.87ms | tok/sec: 38,108 | mfu: 12.24 | total time: 261.67m
step 01143/21400 (5.34%) | loss: 3.291855 | grad norm: 0.1424 | lrm: 1.00 | dt: 13906.67ms | tok/sec: 37,700 | mfu: 12.11 | total time: 261.90m
step 01144/21400 (5.35%) | loss: 3.337132 | grad norm: 0.1479 | lrm: 1.00 | dt: 14123.96ms | tok/sec: 37,120 | mfu: 11.93 | total time: 262.14m
step 01145/21400 (5.35%) | loss: 3.343253 | grad norm: 0.1608 | lrm: 1.00 | dt: 13906.54ms | tok/sec: 37,700 | mfu: 12.11 | total time: 262.37m
step 01146/21400 (5.36%) | loss: 3.337302 | grad norm: 0.1417 | lrm: 1.00 | dt: 13808.83ms | tok/sec: 37,967 | mfu: 12.20 | total time: 262.60m
step 01147/21400 (5.36%) | loss: 3.343471 | grad norm: 0.1426 | lrm: 1.00 | dt: 13894.06ms | tok/sec: 37,734 | mfu: 12.12 | total time: 262.83m
step 01148/21400 (5.36%) | loss: 3.330624 | grad norm: 0.1475 | lrm: 1.00 | dt: 13831.70ms | tok/sec: 37,904 | mfu: 12.18 | total time: 263.06m
step 01149/21400 (5.37%) | loss: 3.338863 | grad norm: 0.1479 | lrm: 1.00 | dt: 13863.64ms | tok/sec: 37,817 | mfu: 12.15 | total time: 263.29m
step 01150/21400 (5.37%) | loss: 3.358897 | grad norm: 0.1503 | lrm: 1.00 | dt: 13845.09ms | tok/sec: 37,868 | mfu: 12.17 | total time: 263.53m
step 01151/21400 (5.38%) | loss: 3.370372 | grad norm: 0.1472 | lrm: 1.00 | dt: 13845.60ms | tok/sec: 37,866 | mfu: 12.16 | total time: 263.76m
step 01152/21400 (5.38%) | loss: 3.319954 | grad norm: 0.1479 | lrm: 1.00 | dt: 13929.68ms | tok/sec: 37,638 | mfu: 12.09 | total time: 263.99m
step 01153/21400 (5.39%) | loss: 3.303611 | grad norm: 0.1363 | lrm: 1.00 | dt: 13761.74ms | tok/sec: 38,097 | mfu: 12.24 | total time: 264.22m
step 01154/21400 (5.39%) | loss: 3.296393 | grad norm: 0.1363 | lrm: 1.00 | dt: 13928.92ms | tok/sec: 37,640 | mfu: 12.09 | total time: 264.45m
step 01155/21400 (5.40%) | loss: 3.285513 | grad norm: 0.1233 | lrm: 1.00 | dt: 13771.56ms | tok/sec: 38,070 | mfu: 12.23 | total time: 264.68m
step 01156/21400 (5.40%) | loss: 3.336550 | grad norm: 0.1334 | lrm: 1.00 | dt: 13918.33ms | tok/sec: 37,668 | mfu: 12.10 | total time: 264.91m
step 01157/21400 (5.41%) | loss: 3.274957 | grad norm: 0.1345 | lrm: 1.00 | dt: 13772.85ms | tok/sec: 38,066 | mfu: 12.23 | total time: 265.14m
step 01158/21400 (5.41%) | loss: 3.263327 | grad norm: 0.1397 | lrm: 1.00 | dt: 13923.65ms | tok/sec: 37,654 | mfu: 12.10 | total time: 265.37m
step 01159/21400 (5.42%) | loss: 3.246521 | grad norm: 0.1326 | lrm: 1.00 | dt: 13785.22ms | tok/sec: 38,032 | mfu: 12.22 | total time: 265.60m
step 01160/21400 (5.42%) | loss: 3.257346 | grad norm: 0.1312 | lrm: 1.00 | dt: 13901.85ms | tok/sec: 37,713 | mfu: 12.12 | total time: 265.83m
step 01161/21400 (5.43%) | loss: 3.341121 | grad norm: 0.1412 | lrm: 1.00 | dt: 13823.47ms | tok/sec: 37,927 | mfu: 12.18 | total time: 266.06m
step 01162/21400 (5.43%) | loss: 3.356882 | grad norm: 0.1512 | lrm: 1.00 | dt: 13864.91ms | tok/sec: 37,814 | mfu: 12.15 | total time: 266.30m
step 01163/21400 (5.43%) | loss: 3.351879 | grad norm: 0.1452 | lrm: 1.00 | dt: 13857.97ms | tok/sec: 37,832 | mfu: 12.15 | total time: 266.53m
step 01164/21400 (5.44%) | loss: 3.341509 | grad norm: 0.1325 | lrm: 1.00 | dt: 13841.58ms | tok/sec: 37,877 | mfu: 12.17 | total time: 266.76m
step 01165/21400 (5.44%) | loss: 3.320369 | grad norm: 0.1404 | lrm: 1.00 | dt: 13950.45ms | tok/sec: 37,582 | mfu: 12.07 | total time: 266.99m
step 01166/21400 (5.45%) | loss: 3.292106 | grad norm: 0.1434 | lrm: 1.00 | dt: 13748.33ms | tok/sec: 38,134 | mfu: 12.25 | total time: 267.22m
step 01167/21400 (5.45%) | loss: 3.284102 | grad norm: 0.1440 | lrm: 1.00 | dt: 13936.56ms | tok/sec: 37,619 | mfu: 12.09 | total time: 267.45m
step 01168/21400 (5.46%) | loss: 3.294390 | grad norm: 0.1454 | lrm: 1.00 | dt: 13768.95ms | tok/sec: 38,077 | mfu: 12.23 | total time: 267.68m
step 01169/21400 (5.46%) | loss: 3.300112 | grad norm: 0.1509 | lrm: 1.00 | dt: 13922.90ms | tok/sec: 37,656 | mfu: 12.10 | total time: 267.91m
step 01170/21400 (5.47%) | loss: 3.343372 | grad norm: 0.1506 | lrm: 1.00 | dt: 13778.90ms | tok/sec: 38,050 | mfu: 12.22 | total time: 268.14m
step 01171/21400 (5.47%) | loss: 3.338777 | grad norm: 0.1424 | lrm: 1.00 | dt: 13928.67ms | tok/sec: 37,640 | mfu: 12.09 | total time: 268.37m
step 01172/21400 (5.48%) | loss: 3.364209 | grad norm: 0.1525 | lrm: 1.00 | dt: 13779.32ms | tok/sec: 38,048 | mfu: 12.22 | total time: 268.60m
step 01173/21400 (5.48%) | loss: 3.312519 | grad norm: 0.1567 | lrm: 1.00 | dt: 13903.08ms | tok/sec: 37,710 | mfu: 12.11 | total time: 268.84m
step 01174/21400 (5.49%) | loss: 3.329796 | grad norm: 0.1453 | lrm: 1.00 | dt: 13825.93ms | tok/sec: 37,920 | mfu: 12.18 | total time: 269.07m
step 01175/21400 (5.49%) | loss: 3.343331 | grad norm: 0.1441 | lrm: 1.00 | dt: 13863.57ms | tok/sec: 37,817 | mfu: 12.15 | total time: 269.30m
step 01176/21400 (5.50%) | loss: 3.336329 | grad norm: 0.1440 | lrm: 1.00 | dt: 13841.94ms | tok/sec: 37,876 | mfu: 12.17 | total time: 269.53m
step 01177/21400 (5.50%) | loss: 3.288356 | grad norm: 0.1337 | lrm: 1.00 | dt: 13829.67ms | tok/sec: 37,910 | mfu: 12.18 | total time: 269.76m
step 01178/21400 (5.50%) | loss: 3.299409 | grad norm: 0.1307 | lrm: 1.00 | dt: 13932.39ms | tok/sec: 37,630 | mfu: 12.09 | total time: 269.99m
step 01179/21400 (5.51%) | loss: 3.320990 | grad norm: 0.1460 | lrm: 1.00 | dt: 14098.91ms | tok/sec: 37,186 | mfu: 11.95 | total time: 270.23m
step 01180/21400 (5.51%) | loss: 3.327222 | grad norm: 0.1477 | lrm: 1.00 | dt: 13927.60ms | tok/sec: 37,643 | mfu: 12.09 | total time: 270.46m
step 01181/21400 (5.52%) | loss: 3.294444 | grad norm: 0.1419 | lrm: 1.00 | dt: 13743.96ms | tok/sec: 38,146 | mfu: 12.25 | total time: 270.69m
step 01182/21400 (5.52%) | loss: 3.242550 | grad norm: 0.1430 | lrm: 1.00 | dt: 13911.25ms | tok/sec: 37,688 | mfu: 12.11 | total time: 270.92m
step 01183/21400 (5.53%) | loss: 3.199406 | grad norm: 0.1451 | lrm: 1.00 | dt: 13760.95ms | tok/sec: 38,099 | mfu: 12.24 | total time: 271.15m
step 01184/21400 (5.53%) | loss: 3.216265 | grad norm: 0.1635 | lrm: 1.00 | dt: 13901.79ms | tok/sec: 37,713 | mfu: 12.12 | total time: 271.38m
step 01185/21400 (5.54%) | loss: 3.192625 | grad norm: 0.1264 | lrm: 1.00 | dt: 13765.95ms | tok/sec: 38,085 | mfu: 12.24 | total time: 271.61m
step 01186/21400 (5.54%) | loss: 3.174452 | grad norm: 0.1351 | lrm: 1.00 | dt: 13891.56ms | tok/sec: 37,741 | mfu: 12.12 | total time: 271.84m
step 01187/21400 (5.55%) | loss: 3.196046 | grad norm: 0.1386 | lrm: 1.00 | dt: 13786.02ms | tok/sec: 38,030 | mfu: 12.22 | total time: 272.07m
step 01188/21400 (5.55%) | loss: 3.235517 | grad norm: 0.1414 | lrm: 1.00 | dt: 13880.40ms | tok/sec: 37,771 | mfu: 12.13 | total time: 272.30m
step 01189/21400 (5.56%) | loss: 3.326221 | grad norm: 0.1378 | lrm: 1.00 | dt: 13817.38ms | tok/sec: 37,944 | mfu: 12.19 | total time: 272.53m
step 01190/21400 (5.56%) | loss: 3.333150 | grad norm: 0.1391 | lrm: 1.00 | dt: 13851.49ms | tok/sec: 37,850 | mfu: 12.16 | total time: 272.76m
step 01191/21400 (5.57%) | loss: 3.316468 | grad norm: 0.1424 | lrm: 1.00 | dt: 13865.33ms | tok/sec: 37,812 | mfu: 12.15 | total time: 272.99m
step 01192/21400 (5.57%) | loss: 3.293910 | grad norm: 0.1387 | lrm: 1.00 | dt: 13802.21ms | tok/sec: 37,985 | mfu: 12.20 | total time: 273.22m
step 01193/21400 (5.57%) | loss: 3.311645 | grad norm: 0.1493 | lrm: 1.00 | dt: 13921.54ms | tok/sec: 37,660 | mfu: 12.10 | total time: 273.46m
step 01194/21400 (5.58%) | loss: 3.288038 | grad norm: 0.1379 | lrm: 1.00 | dt: 13753.92ms | tok/sec: 38,119 | mfu: 12.25 | total time: 273.69m
step 01195/21400 (5.58%) | loss: 3.305026 | grad norm: 0.1290 | lrm: 1.00 | dt: 13930.35ms | tok/sec: 37,636 | mfu: 12.09 | total time: 273.92m
step 01196/21400 (5.59%) | loss: 3.268874 | grad norm: 0.1471 | lrm: 1.00 | dt: 13747.08ms | tok/sec: 38,138 | mfu: 12.25 | total time: 274.15m
step 01197/21400 (5.59%) | loss: 3.278884 | grad norm: 0.1443 | lrm: 1.00 | dt: 13912.49ms | tok/sec: 37,684 | mfu: 12.11 | total time: 274.38m
step 01198/21400 (5.60%) | loss: 3.265806 | grad norm: 0.1579 | lrm: 1.00 | dt: 13761.54ms | tok/sec: 38,098 | mfu: 12.24 | total time: 274.61m
step 01199/21400 (5.60%) | loss: 3.274451 | grad norm: 0.1547 | lrm: 1.00 | dt: 13878.82ms | tok/sec: 37,776 | mfu: 12.14 | total time: 274.84m
step 01200/21400 (5.61%) | loss: 3.249453 | grad norm: 0.1321 | lrm: 1.00 | dt: 13794.33ms | tok/sec: 38,007 | mfu: 12.21 | total time: 275.07m
step 01201/21400 (5.61%) | loss: 3.294651 | grad norm: 0.1385 | lrm: 1.00 | dt: 13871.39ms | tok/sec: 37,796 | mfu: 12.14 | total time: 275.30m
step 01202/21400 (5.62%) | loss: 3.283588 | grad norm: 0.1533 | lrm: 1.00 | dt: 13814.57ms | tok/sec: 37,951 | mfu: 12.19 | total time: 275.53m
step 01203/21400 (5.62%) | loss: 3.275327 | grad norm: 0.1494 | lrm: 1.00 | dt: 13842.72ms | tok/sec: 37,874 | mfu: 12.17 | total time: 275.76m
step 01204/21400 (5.63%) | loss: 3.275306 | grad norm: 0.1519 | lrm: 1.00 | dt: 13867.21ms | tok/sec: 37,807 | mfu: 12.15 | total time: 275.99m
step 01205/21400 (5.63%) | loss: 3.246695 | grad norm: 0.1551 | lrm: 1.00 | dt: 13801.19ms | tok/sec: 37,988 | mfu: 12.20 | total time: 276.22m
step 01206/21400 (5.64%) | loss: 3.242552 | grad norm: 0.1466 | lrm: 1.00 | dt: 13923.38ms | tok/sec: 37,655 | mfu: 12.10 | total time: 276.45m
step 01207/21400 (5.64%) | loss: 3.245890 | grad norm: 0.1460 | lrm: 1.00 | dt: 13752.02ms | tok/sec: 38,124 | mfu: 12.25 | total time: 276.68m
step 01208/21400 (5.64%) | loss: 3.277907 | grad norm: 0.1531 | lrm: 1.00 | dt: 13914.52ms | tok/sec: 37,679 | mfu: 12.10 | total time: 276.92m
step 01209/21400 (5.65%) | loss: 3.297682 | grad norm: 0.1488 | lrm: 1.00 | dt: 13746.66ms | tok/sec: 38,139 | mfu: 12.25 | total time: 277.14m
step 01210/21400 (5.65%) | loss: 3.289473 | grad norm: 0.1454 | lrm: 1.00 | dt: 13907.55ms | tok/sec: 37,698 | mfu: 12.11 | total time: 277.38m
step 01211/21400 (5.66%) | loss: 3.332701 | grad norm: 0.1416 | lrm: 1.00 | dt: 13772.08ms | tok/sec: 38,068 | mfu: 12.23 | total time: 277.61m
step 01212/21400 (5.66%) | loss: 3.274133 | grad norm: 0.1517 | lrm: 1.00 | dt: 13905.91ms | tok/sec: 37,702 | mfu: 12.11 | total time: 277.84m
step 01213/21400 (5.67%) | loss: 3.265909 | grad norm: 0.1516 | lrm: 1.00 | dt: 14116.35ms | tok/sec: 37,140 | mfu: 11.93 | total time: 278.07m
step 01214/21400 (5.67%) | loss: 3.293254 | grad norm: 0.1389 | lrm: 1.00 | dt: 13863.96ms | tok/sec: 37,816 | mfu: 12.15 | total time: 278.30m
step 01215/21400 (5.68%) | loss: 3.308561 | grad norm: 0.1452 | lrm: 1.00 | dt: 13800.67ms | tok/sec: 37,990 | mfu: 12.20 | total time: 278.53m
step 01216/21400 (5.68%) | loss: 3.307927 | grad norm: 0.1419 | lrm: 1.00 | dt: 13856.25ms | tok/sec: 37,837 | mfu: 12.16 | total time: 278.77m
step 01217/21400 (5.69%) | loss: 3.305061 | grad norm: 0.1442 | lrm: 1.00 | dt: 13869.72ms | tok/sec: 37,800 | mfu: 12.14 | total time: 279.00m
step 01218/21400 (5.69%) | loss: 3.320627 | grad norm: 0.1418 | lrm: 1.00 | dt: 13930.52ms | tok/sec: 37,635 | mfu: 12.09 | total time: 279.23m
step 01219/21400 (5.70%) | loss: 3.321223 | grad norm: 0.1441 | lrm: 1.00 | dt: 13690.21ms | tok/sec: 38,296 | mfu: 12.30 | total time: 279.46m
step 01220/21400 (5.70%) | loss: 3.293519 | grad norm: 0.1544 | lrm: 1.00 | dt: 13821.46ms | tok/sec: 37,932 | mfu: 12.19 | total time: 279.69m
step 01221/21400 (5.71%) | loss: 3.304823 | grad norm: 0.1406 | lrm: 1.00 | dt: 13920.00ms | tok/sec: 37,664 | mfu: 12.10 | total time: 279.92m
step 01222/21400 (5.71%) | loss: 3.303535 | grad norm: 0.1373 | lrm: 1.00 | dt: 13748.74ms | tok/sec: 38,133 | mfu: 12.25 | total time: 280.15m
step 01223/21400 (5.71%) | loss: 3.284379 | grad norm: 0.1435 | lrm: 1.00 | dt: 13914.54ms | tok/sec: 37,679 | mfu: 12.10 | total time: 280.38m
step 01224/21400 (5.72%) | loss: 3.281654 | grad norm: 0.1389 | lrm: 1.00 | dt: 13751.43ms | tok/sec: 38,126 | mfu: 12.25 | total time: 280.61m
step 01225/21400 (5.72%) | loss: 3.293140 | grad norm: 0.1363 | lrm: 1.00 | dt: 13891.29ms | tok/sec: 37,742 | mfu: 12.12 | total time: 280.84m
step 01226/21400 (5.73%) | loss: 3.321579 | grad norm: 0.1504 | lrm: 1.00 | dt: 13778.58ms | tok/sec: 38,050 | mfu: 12.22 | total time: 281.07m
step 01227/21400 (5.73%) | loss: 3.298633 | grad norm: 0.1548 | lrm: 1.00 | dt: 13889.70ms | tok/sec: 37,746 | mfu: 12.13 | total time: 281.30m
step 01228/21400 (5.74%) | loss: 3.277003 | grad norm: 0.1406 | lrm: 1.00 | dt: 13801.78ms | tok/sec: 37,986 | mfu: 12.20 | total time: 281.53m
step 01229/21400 (5.74%) | loss: 3.260992 | grad norm: 0.1457 | lrm: 1.00 | dt: 13853.80ms | tok/sec: 37,844 | mfu: 12.16 | total time: 281.76m
step 01230/21400 (5.75%) | loss: 3.241912 | grad norm: 0.1638 | lrm: 1.00 | dt: 13838.98ms | tok/sec: 37,884 | mfu: 12.17 | total time: 281.99m
step 01231/21400 (5.75%) | loss: 3.263310 | grad norm: 0.1453 | lrm: 1.00 | dt: 13813.42ms | tok/sec: 37,954 | mfu: 12.19 | total time: 282.22m
step 01232/21400 (5.76%) | loss: 3.244611 | grad norm: 0.1301 | lrm: 1.00 | dt: 13932.15ms | tok/sec: 37,631 | mfu: 12.09 | total time: 282.46m
step 01233/21400 (5.76%) | loss: 3.272224 | grad norm: 0.1264 | lrm: 1.00 | dt: 13738.66ms | tok/sec: 38,161 | mfu: 12.26 | total time: 282.68m
step 01234/21400 (5.77%) | loss: 3.305589 | grad norm: 0.1314 | lrm: 1.00 | dt: 13927.88ms | tok/sec: 37,643 | mfu: 12.09 | total time: 282.92m
step 01235/21400 (5.77%) | loss: 3.312916 | grad norm: 0.1353 | lrm: 1.00 | dt: 13748.17ms | tok/sec: 38,135 | mfu: 12.25 | total time: 283.15m
step 01236/21400 (5.78%) | loss: 3.307523 | grad norm: 0.1416 | lrm: 1.00 | dt: 13918.34ms | tok/sec: 37,668 | mfu: 12.10 | total time: 283.38m
step 01237/21400 (5.78%) | loss: 3.307196 | grad norm: 0.1300 | lrm: 1.00 | dt: 13754.12ms | tok/sec: 38,118 | mfu: 12.25 | total time: 283.61m
step 01238/21400 (5.79%) | loss: 3.292375 | grad norm: 0.1373 | lrm: 1.00 | dt: 13914.55ms | tok/sec: 37,679 | mfu: 12.10 | total time: 283.84m
step 01239/21400 (5.79%) | loss: 3.282662 | grad norm: 0.1291 | lrm: 1.00 | dt: 13781.52ms | tok/sec: 38,042 | mfu: 12.22 | total time: 284.07m
step 01240/21400 (5.79%) | loss: 3.320307 | grad norm: 0.1398 | lrm: 1.00 | dt: 13876.18ms | tok/sec: 37,783 | mfu: 12.14 | total time: 284.30m
step 01241/21400 (5.80%) | loss: 3.367608 | grad norm: 0.1361 | lrm: 1.00 | dt: 13805.70ms | tok/sec: 37,976 | mfu: 12.20 | total time: 284.53m
step 01242/21400 (5.80%) | loss: 3.368685 | grad norm: 0.1434 | lrm: 1.00 | dt: 13850.37ms | tok/sec: 37,853 | mfu: 12.16 | total time: 284.76m
step 01243/21400 (5.81%) | loss: 3.351928 | grad norm: 0.1435 | lrm: 1.00 | dt: 13849.35ms | tok/sec: 37,856 | mfu: 12.16 | total time: 284.99m
step 01244/21400 (5.81%) | loss: 3.371856 | grad norm: 0.1398 | lrm: 1.00 | dt: 13816.34ms | tok/sec: 37,946 | mfu: 12.19 | total time: 285.22m
step 01245/21400 (5.82%) | loss: 3.381882 | grad norm: 0.1373 | lrm: 1.00 | dt: 13932.52ms | tok/sec: 37,630 | mfu: 12.09 | total time: 285.45m
step 01246/21400 (5.82%) | loss: 3.395317 | grad norm: 0.1409 | lrm: 1.00 | dt: 13735.80ms | tok/sec: 38,169 | mfu: 12.26 | total time: 285.68m
step 01247/21400 (5.83%) | loss: 3.404295 | grad norm: 0.1331 | lrm: 1.00 | dt: 13933.03ms | tok/sec: 37,629 | mfu: 12.09 | total time: 285.92m
step 01248/21400 (5.83%) | loss: 3.327867 | grad norm: 0.1385 | lrm: 1.00 | dt: 14116.61ms | tok/sec: 37,139 | mfu: 11.93 | total time: 286.15m
step 01249/21400 (5.84%) | loss: 3.320943 | grad norm: 0.1422 | lrm: 1.00 | dt: 13931.70ms | tok/sec: 37,632 | mfu: 12.09 | total time: 286.38m
Step 01250 | Validation bpb: 0.9923
step 01250/21400 (5.84%) | loss: 3.317092 | grad norm: 0.1451 | lrm: 1.00 | dt: 13781.23ms | tok/sec: 38,043 | mfu: 12.22 | total time: 286.61m
step 01251/21400 (5.85%) | loss: 3.301348 | grad norm: 0.1330 | lrm: 1.00 | dt: 13884.37ms | tok/sec: 37,761 | mfu: 12.13 | total time: 286.84m
step 01252/21400 (5.85%) | loss: 3.308145 | grad norm: 0.1262 | lrm: 1.00 | dt: 13807.87ms | tok/sec: 37,970 | mfu: 12.20 | total time: 287.07m
step 01253/21400 (5.86%) | loss: 3.305608 | grad norm: 0.1256 | lrm: 1.00 | dt: 13858.26ms | tok/sec: 37,832 | mfu: 12.15 | total time: 287.31m
step 01254/21400 (5.86%) | loss: 3.320458 | grad norm: 0.1251 | lrm: 1.00 | dt: 13839.20ms | tok/sec: 37,884 | mfu: 12.17 | total time: 287.54m
step 01255/21400 (5.86%) | loss: 3.306020 | grad norm: 0.1222 | lrm: 1.00 | dt: 13831.83ms | tok/sec: 37,904 | mfu: 12.18 | total time: 287.77m
step 01256/21400 (5.87%) | loss: 3.285805 | grad norm: 0.1326 | lrm: 1.00 | dt: 13941.67ms | tok/sec: 37,605 | mfu: 12.08 | total time: 288.00m
step 01257/21400 (5.87%) | loss: 3.298148 | grad norm: 0.1557 | lrm: 1.00 | dt: 13732.56ms | tok/sec: 38,178 | mfu: 12.26 | total time: 288.23m
step 01258/21400 (5.88%) | loss: 3.305816 | grad norm: 0.1552 | lrm: 1.00 | dt: 13924.50ms | tok/sec: 37,652 | mfu: 12.10 | total time: 288.46m
step 01259/21400 (5.88%) | loss: 3.319702 | grad norm: 0.1561 | lrm: 1.00 | dt: 13751.71ms | tok/sec: 38,125 | mfu: 12.25 | total time: 288.69m
step 01260/21400 (5.89%) | loss: 3.327276 | grad norm: 0.1395 | lrm: 1.00 | dt: 13921.87ms | tok/sec: 37,659 | mfu: 12.10 | total time: 288.92m
step 01261/21400 (5.89%) | loss: 3.317487 | grad norm: 0.1360 | lrm: 1.00 | dt: 13775.34ms | tok/sec: 38,059 | mfu: 12.23 | total time: 289.15m
step 01262/21400 (5.90%) | loss: 3.372128 | grad norm: 0.1436 | lrm: 1.00 | dt: 13895.72ms | tok/sec: 37,730 | mfu: 12.12 | total time: 289.38m
step 01263/21400 (5.90%) | loss: 3.354740 | grad norm: 0.1378 | lrm: 1.00 | dt: 13774.31ms | tok/sec: 38,062 | mfu: 12.23 | total time: 289.61m
step 01264/21400 (5.91%) | loss: 3.368377 | grad norm: 0.1384 | lrm: 1.00 | dt: 13885.46ms | tok/sec: 37,758 | mfu: 12.13 | total time: 289.84m
step 01265/21400 (5.91%) | loss: 3.411335 | grad norm: 0.1300 | lrm: 1.00 | dt: 13807.58ms | tok/sec: 37,971 | mfu: 12.20 | total time: 290.07m
step 01266/21400 (5.92%) | loss: 3.389563 | grad norm: 0.1322 | lrm: 1.00 | dt: 13852.25ms | tok/sec: 37,848 | mfu: 12.16 | total time: 290.30m
step 01267/21400 (5.92%) | loss: 3.390832 | grad norm: 0.1471 | lrm: 1.00 | dt: 13834.03ms | tok/sec: 37,898 | mfu: 12.17 | total time: 290.53m
step 01268/21400 (5.93%) | loss: 3.378721 | grad norm: 0.1767 | lrm: 1.00 | dt: 13823.47ms | tok/sec: 37,927 | mfu: 12.18 | total time: 290.77m
step 01269/21400 (5.93%) | loss: 3.361708 | grad norm: 0.1496 | lrm: 1.00 | dt: 13935.90ms | tok/sec: 37,621 | mfu: 12.09 | total time: 291.00m
step 01270/21400 (5.93%) | loss: 3.343098 | grad norm: 0.1405 | lrm: 1.00 | dt: 13734.12ms | tok/sec: 38,174 | mfu: 12.26 | total time: 291.23m
step 01271/21400 (5.94%) | loss: 3.366649 | grad norm: 0.1660 | lrm: 1.00 | dt: 13963.09ms | tok/sec: 37,548 | mfu: 12.06 | total time: 291.46m
step 01272/21400 (5.94%) | loss: 3.368216 | grad norm: 0.1542 | lrm: 1.00 | dt: 13744.49ms | tok/sec: 38,145 | mfu: 12.25 | total time: 291.69m
step 01273/21400 (5.95%) | loss: 3.322681 | grad norm: 0.1392 | lrm: 1.00 | dt: 13922.53ms | tok/sec: 37,657 | mfu: 12.10 | total time: 291.92m
step 01274/21400 (5.95%) | loss: 3.288421 | grad norm: 0.1415 | lrm: 1.00 | dt: 13760.01ms | tok/sec: 38,102 | mfu: 12.24 | total time: 292.15m
step 01275/21400 (5.96%) | loss: 3.290945 | grad norm: 0.1358 | lrm: 1.00 | dt: 13903.65ms | tok/sec: 37,708 | mfu: 12.11 | total time: 292.38m
step 01276/21400 (5.96%) | loss: 3.291224 | grad norm: 0.1384 | lrm: 1.00 | dt: 13767.46ms | tok/sec: 38,081 | mfu: 12.23 | total time: 292.61m
step 01277/21400 (5.97%) | loss: 3.293852 | grad norm: 0.1474 | lrm: 1.00 | dt: 13904.03ms | tok/sec: 37,707 | mfu: 12.11 | total time: 292.84m
step 01278/21400 (5.97%) | loss: 3.307939 | grad norm: 0.1351 | lrm: 1.00 | dt: 13805.42ms | tok/sec: 37,976 | mfu: 12.20 | total time: 293.07m
step 01279/21400 (5.98%) | loss: 3.303784 | grad norm: 0.1321 | lrm: 1.00 | dt: 13859.34ms | tok/sec: 37,829 | mfu: 12.15 | total time: 293.30m
step 01280/21400 (5.98%) | loss: 3.330112 | grad norm: 0.1486 | lrm: 1.00 | dt: 14178.79ms | tok/sec: 36,976 | mfu: 11.88 | total time: 293.54m
step 01281/21400 (5.99%) | loss: 3.333431 | grad norm: 0.1486 | lrm: 1.00 | dt: 13855.60ms | tok/sec: 37,839 | mfu: 12.16 | total time: 293.77m
step 01282/21400 (5.99%) | loss: 3.299380 | grad norm: 0.1382 | lrm: 1.00 | dt: 13850.68ms | tok/sec: 37,852 | mfu: 12.16 | total time: 294.00m
step 01283/21400 (6.00%) | loss: 3.292944 | grad norm: 0.1216 | lrm: 1.00 | dt: 13803.72ms | tok/sec: 37,981 | mfu: 12.20 | total time: 294.23m
step 01284/21400 (6.00%) | loss: 3.263900 | grad norm: 0.1241 | lrm: 1.00 | dt: 13931.39ms | tok/sec: 37,633 | mfu: 12.09 | total time: 294.46m
step 01285/21400 (6.00%) | loss: 3.279484 | grad norm: 0.1268 | lrm: 1.00 | dt: 13727.27ms | tok/sec: 38,193 | mfu: 12.27 | total time: 294.69m
step 01286/21400 (6.01%) | loss: 3.311287 | grad norm: 0.1349 | lrm: 1.00 | dt: 13919.92ms | tok/sec: 37,664 | mfu: 12.10 | total time: 294.92m
step 01287/21400 (6.01%) | loss: 3.303344 | grad norm: 0.1380 | lrm: 1.00 | dt: 13741.71ms | tok/sec: 38,153 | mfu: 12.26 | total time: 295.15m
step 01288/21400 (6.02%) | loss: 3.314682 | grad norm: 0.1338 | lrm: 1.00 | dt: 13928.07ms | tok/sec: 37,642 | mfu: 12.09 | total time: 295.39m
step 01289/21400 (6.02%) | loss: 3.310335 | grad norm: 0.1383 | lrm: 1.00 | dt: 13759.69ms | tok/sec: 38,103 | mfu: 12.24 | total time: 295.62m
step 01290/21400 (6.03%) | loss: 3.326485 | grad norm: 0.1360 | lrm: 1.00 | dt: 13890.54ms | tok/sec: 37,744 | mfu: 12.13 | total time: 295.85m
step 01291/21400 (6.03%) | loss: 3.332789 | grad norm: 0.1178 | lrm: 1.00 | dt: 13779.19ms | tok/sec: 38,049 | mfu: 12.22 | total time: 296.08m
step 01292/21400 (6.04%) | loss: 3.302193 | grad norm: 0.1330 | lrm: 1.00 | dt: 13891.39ms | tok/sec: 37,741 | mfu: 12.12 | total time: 296.31m
step 01293/21400 (6.04%) | loss: 3.347503 | grad norm: 0.1485 | lrm: 1.00 | dt: 13820.37ms | tok/sec: 37,935 | mfu: 12.19 | total time: 296.54m
step 01294/21400 (6.05%) | loss: 3.359754 | grad norm: 0.1464 | lrm: 1.00 | dt: 13846.09ms | tok/sec: 37,865 | mfu: 12.16 | total time: 296.77m
step 01295/21400 (6.05%) | loss: 3.361952 | grad norm: 0.1277 | lrm: 1.00 | dt: 13856.92ms | tok/sec: 37,835 | mfu: 12.15 | total time: 297.00m
step 01296/21400 (6.06%) | loss: 3.328074 | grad norm: 0.1322 | lrm: 1.00 | dt: 13819.93ms | tok/sec: 37,937 | mfu: 12.19 | total time: 297.23m
step 01297/21400 (6.06%) | loss: 3.332166 | grad norm: 0.1408 | lrm: 1.00 | dt: 14140.04ms | tok/sec: 37,078 | mfu: 11.91 | total time: 297.47m
step 01298/21400 (6.07%) | loss: 3.372826 | grad norm: 0.1359 | lrm: 1.00 | dt: 13704.33ms | tok/sec: 38,257 | mfu: 12.29 | total time: 297.69m
step 01299/21400 (6.07%) | loss: 3.407257 | grad norm: 0.1345 | lrm: 1.00 | dt: 13890.93ms | tok/sec: 37,743 | mfu: 12.13 | total time: 297.93m
step 01300/21400 (6.07%) | loss: 3.376346 | grad norm: 0.1278 | lrm: 1.00 | dt: 13718.58ms | tok/sec: 38,217 | mfu: 12.28 | total time: 298.15m
step 01301/21400 (6.08%) | loss: 3.362594 | grad norm: 0.1348 | lrm: 1.00 | dt: 13883.29ms | tok/sec: 37,763 | mfu: 12.13 | total time: 298.39m
step 01302/21400 (6.08%) | loss: 3.337553 | grad norm: 0.1494 | lrm: 1.00 | dt: 13731.71ms | tok/sec: 38,180 | mfu: 12.27 | total time: 298.61m
step 01303/21400 (6.09%) | loss: 3.384419 | grad norm: 0.1368 | lrm: 1.00 | dt: 13870.96ms | tok/sec: 37,797 | mfu: 12.14 | total time: 298.85m
step 01304/21400 (6.09%) | loss: 3.369884 | grad norm: 0.1228 | lrm: 1.00 | dt: 13743.07ms | tok/sec: 38,149 | mfu: 12.26 | total time: 299.07m
step 01305/21400 (6.10%) | loss: 3.316557 | grad norm: 0.1359 | lrm: 1.00 | dt: 13837.68ms | tok/sec: 37,888 | mfu: 12.17 | total time: 299.31m
step 01306/21400 (6.10%) | loss: 3.304614 | grad norm: 0.1381 | lrm: 1.00 | dt: 13777.22ms | tok/sec: 38,054 | mfu: 12.23 | total time: 299.54m
step 01307/21400 (6.11%) | loss: 3.306360 | grad norm: 0.1363 | lrm: 1.00 | dt: 13824.76ms | tok/sec: 37,923 | mfu: 12.18 | total time: 299.77m
step 01308/21400 (6.11%) | loss: 3.373838 | grad norm: 0.1429 | lrm: 1.00 | dt: 13810.09ms | tok/sec: 37,964 | mfu: 12.20 | total time: 300.00m
step 01309/21400 (6.12%) | loss: 3.350873 | grad norm: 0.1467 | lrm: 1.00 | dt: 13790.68ms | tok/sec: 38,017 | mfu: 12.21 | total time: 300.23m
step 01310/21400 (6.12%) | loss: 3.344327 | grad norm: 0.2098 | lrm: 1.00 | dt: 13908.34ms | tok/sec: 37,695 | mfu: 12.11 | total time: 300.46m
step 01311/21400 (6.13%) | loss: 3.370065 | grad norm: 0.1253 | lrm: 1.00 | dt: 13695.38ms | tok/sec: 38,282 | mfu: 12.30 | total time: 300.69m
step 01312/21400 (6.13%) | loss: 3.399217 | grad norm: 0.1529 | lrm: 1.00 | dt: 13902.34ms | tok/sec: 37,712 | mfu: 12.12 | total time: 300.92m
step 01313/21400 (6.14%) | loss: 3.365479 | grad norm: 0.1418 | lrm: 1.00 | dt: 13706.80ms | tok/sec: 38,250 | mfu: 12.29 | total time: 301.15m
step 01314/21400 (6.14%) | loss: 3.328851 | grad norm: 0.1361 | lrm: 1.00 | dt: 13884.34ms | tok/sec: 37,761 | mfu: 12.13 | total time: 301.38m
step 01315/21400 (6.14%) | loss: 3.358421 | grad norm: 0.1382 | lrm: 1.00 | dt: 13728.79ms | tok/sec: 38,188 | mfu: 12.27 | total time: 301.61m
step 01316/21400 (6.15%) | loss: 3.352026 | grad norm: 0.1308 | lrm: 1.00 | dt: 13869.20ms | tok/sec: 37,802 | mfu: 12.14 | total time: 301.84m
step 01317/21400 (6.15%) | loss: 3.328439 | grad norm: 0.1270 | lrm: 1.00 | dt: 14104.09ms | tok/sec: 37,172 | mfu: 11.94 | total time: 302.07m
step 01318/21400 (6.16%) | loss: 3.326720 | grad norm: 0.1320 | lrm: 1.00 | dt: 13852.58ms | tok/sec: 37,847 | mfu: 12.16 | total time: 302.30m
step 01319/21400 (6.16%) | loss: 3.336533 | grad norm: 0.1363 | lrm: 1.00 | dt: 13774.12ms | tok/sec: 38,063 | mfu: 12.23 | total time: 302.53m
step 01320/21400 (6.17%) | loss: 3.336777 | grad norm: 0.1403 | lrm: 1.00 | dt: 13824.60ms | tok/sec: 37,924 | mfu: 12.18 | total time: 302.76m
step 01321/21400 (6.17%) | loss: 3.327263 | grad norm: 0.1318 | lrm: 1.00 | dt: 13812.99ms | tok/sec: 37,956 | mfu: 12.19 | total time: 302.99m
step 01322/21400 (6.18%) | loss: 3.302699 | grad norm: 0.1340 | lrm: 1.00 | dt: 13797.25ms | tok/sec: 37,999 | mfu: 12.21 | total time: 303.22m
step 01323/21400 (6.18%) | loss: 3.292202 | grad norm: 0.1439 | lrm: 1.00 | dt: 13909.38ms | tok/sec: 37,693 | mfu: 12.11 | total time: 303.46m
step 01324/21400 (6.19%) | loss: 3.275885 | grad norm: 0.1474 | lrm: 1.00 | dt: 13698.92ms | tok/sec: 38,272 | mfu: 12.30 | total time: 303.68m
step 01325/21400 (6.19%) | loss: 3.280263 | grad norm: 0.1309 | lrm: 1.00 | dt: 13890.02ms | tok/sec: 37,745 | mfu: 12.13 | total time: 303.91m
step 01326/21400 (6.20%) | loss: 3.349111 | grad norm: 0.1381 | lrm: 1.00 | dt: 13707.62ms | tok/sec: 38,247 | mfu: 12.29 | total time: 304.14m
step 01327/21400 (6.20%) | loss: 3.351556 | grad norm: 0.1343 | lrm: 1.00 | dt: 13890.72ms | tok/sec: 37,743 | mfu: 12.13 | total time: 304.37m
step 01328/21400 (6.21%) | loss: 3.351689 | grad norm: 0.1325 | lrm: 1.00 | dt: 13727.18ms | tok/sec: 38,193 | mfu: 12.27 | total time: 304.60m
step 01329/21400 (6.21%) | loss: 3.372025 | grad norm: 0.1368 | lrm: 1.00 | dt: 13881.70ms | tok/sec: 37,768 | mfu: 12.13 | total time: 304.84m
step 01330/21400 (6.21%) | loss: 3.373876 | grad norm: 0.1387 | lrm: 1.00 | dt: 13754.22ms | tok/sec: 38,118 | mfu: 12.25 | total time: 305.06m
step 01331/21400 (6.22%) | loss: 3.394651 | grad norm: 0.1298 | lrm: 1.00 | dt: 13855.49ms | tok/sec: 37,839 | mfu: 12.16 | total time: 305.30m
step 01332/21400 (6.22%) | loss: 3.382922 | grad norm: 0.1361 | lrm: 1.00 | dt: 13773.77ms | tok/sec: 38,064 | mfu: 12.23 | total time: 305.52m
step 01333/21400 (6.23%) | loss: 3.375204 | grad norm: 0.1351 | lrm: 1.00 | dt: 13813.07ms | tok/sec: 37,955 | mfu: 12.19 | total time: 305.75m
step 01334/21400 (6.23%) | loss: 3.349119 | grad norm: 0.1340 | lrm: 1.00 | dt: 13824.36ms | tok/sec: 37,924 | mfu: 12.18 | total time: 305.99m
step 01335/21400 (6.24%) | loss: 3.346309 | grad norm: 0.1369 | lrm: 1.00 | dt: 13782.02ms | tok/sec: 38,041 | mfu: 12.22 | total time: 306.22m
step 01336/21400 (6.24%) | loss: 3.333194 | grad norm: 0.1368 | lrm: 1.00 | dt: 13919.93ms | tok/sec: 37,664 | mfu: 12.10 | total time: 306.45m
step 01337/21400 (6.25%) | loss: 3.345972 | grad norm: 0.1390 | lrm: 1.00 | dt: 13690.47ms | tok/sec: 38,295 | mfu: 12.30 | total time: 306.68m
step 01338/21400 (6.25%) | loss: 3.345909 | grad norm: 0.1298 | lrm: 1.00 | dt: 13896.04ms | tok/sec: 37,729 | mfu: 12.12 | total time: 306.91m
step 01339/21400 (6.26%) | loss: 3.365855 | grad norm: 0.1414 | lrm: 1.00 | dt: 13725.29ms | tok/sec: 38,198 | mfu: 12.27 | total time: 307.14m
step 01340/21400 (6.26%) | loss: 3.374467 | grad norm: 0.1434 | lrm: 1.00 | dt: 13886.69ms | tok/sec: 37,754 | mfu: 12.13 | total time: 307.37m
step 01341/21400 (6.27%) | loss: 3.342630 | grad norm: 0.1341 | lrm: 1.00 | dt: 13728.33ms | tok/sec: 38,190 | mfu: 12.27 | total time: 307.60m
step 01342/21400 (6.27%) | loss: 3.370664 | grad norm: 0.1470 | lrm: 1.00 | dt: 13888.33ms | tok/sec: 37,750 | mfu: 12.13 | total time: 307.83m
step 01343/21400 (6.28%) | loss: 3.415523 | grad norm: 0.1576 | lrm: 1.00 | dt: 13751.82ms | tok/sec: 38,124 | mfu: 12.25 | total time: 308.06m
step 01344/21400 (6.28%) | loss: 3.412367 | grad norm: 0.1327 | lrm: 1.00 | dt: 13865.53ms | tok/sec: 37,812 | mfu: 12.15 | total time: 308.29m
step 01345/21400 (6.29%) | loss: 3.413254 | grad norm: 0.1519 | lrm: 1.00 | dt: 13781.70ms | tok/sec: 38,042 | mfu: 12.22 | total time: 308.52m
step 01346/21400 (6.29%) | loss: 3.375148 | grad norm: 0.1669 | lrm: 1.00 | dt: 13829.40ms | tok/sec: 37,911 | mfu: 12.18 | total time: 308.75m
step 01347/21400 (6.29%) | loss: 3.372551 | grad norm: 0.1509 | lrm: 1.00 | dt: 13821.66ms | tok/sec: 37,932 | mfu: 12.19 | total time: 308.98m
step 01348/21400 (6.30%) | loss: 3.332616 | grad norm: 0.1340 | lrm: 1.00 | dt: 13792.86ms | tok/sec: 38,011 | mfu: 12.21 | total time: 309.21m
step 01349/21400 (6.30%) | loss: 3.318660 | grad norm: 0.1314 | lrm: 1.00 | dt: 13905.40ms | tok/sec: 37,703 | mfu: 12.11 | total time: 309.44m
step 01350/21400 (6.31%) | loss: 3.290443 | grad norm: 0.1243 | lrm: 1.00 | dt: 13702.30ms | tok/sec: 38,262 | mfu: 12.29 | total time: 309.67m
step 01351/21400 (6.31%) | loss: 3.277246 | grad norm: 0.1359 | lrm: 1.00 | dt: 13914.68ms | tok/sec: 37,678 | mfu: 12.10 | total time: 309.90m
step 01352/21400 (6.32%) | loss: 3.259414 | grad norm: 0.1420 | lrm: 1.00 | dt: 13723.66ms | tok/sec: 38,203 | mfu: 12.27 | total time: 310.13m
step 01353/21400 (6.32%) | loss: 3.225121 | grad norm: 0.1469 | lrm: 1.00 | dt: 14256.91ms | tok/sec: 36,774 | mfu: 11.81 | total time: 310.37m
step 01354/21400 (6.33%) | loss: 3.234540 | grad norm: 0.1500 | lrm: 1.00 | dt: 13717.56ms | tok/sec: 38,220 | mfu: 12.28 | total time: 310.60m
step 01355/21400 (6.33%) | loss: 3.239116 | grad norm: 0.1355 | lrm: 1.00 | dt: 13874.14ms | tok/sec: 37,788 | mfu: 12.14 | total time: 310.83m
step 01356/21400 (6.34%) | loss: 3.228301 | grad norm: 0.1302 | lrm: 1.00 | dt: 13733.62ms | tok/sec: 38,175 | mfu: 12.26 | total time: 311.06m
step 01357/21400 (6.34%) | loss: 3.235202 | grad norm: 0.1281 | lrm: 1.00 | dt: 13851.45ms | tok/sec: 37,850 | mfu: 12.16 | total time: 311.29m
step 01358/21400 (6.35%) | loss: 3.247299 | grad norm: 0.1267 | lrm: 1.00 | dt: 13766.58ms | tok/sec: 38,084 | mfu: 12.23 | total time: 311.52m
step 01359/21400 (6.35%) | loss: 3.285004 | grad norm: 0.1265 | lrm: 1.00 | dt: 13822.87ms | tok/sec: 37,929 | mfu: 12.18 | total time: 311.75m
step 01360/21400 (6.36%) | loss: 3.305489 | grad norm: 0.1327 | lrm: 1.00 | dt: 13810.61ms | tok/sec: 37,962 | mfu: 12.20 | total time: 311.98m
step 01361/21400 (6.36%) | loss: 3.297341 | grad norm: 0.1595 | lrm: 1.00 | dt: 13783.88ms | tok/sec: 38,036 | mfu: 12.22 | total time: 312.21m
step 01362/21400 (6.36%) | loss: 3.342997 | grad norm: 0.1355 | lrm: 1.00 | dt: 13914.69ms | tok/sec: 37,678 | mfu: 12.10 | total time: 312.44m
step 01363/21400 (6.37%) | loss: 3.368674 | grad norm: 0.1202 | lrm: 1.00 | dt: 13697.91ms | tok/sec: 38,275 | mfu: 12.30 | total time: 312.67m
step 01364/21400 (6.37%) | loss: 3.379393 | grad norm: 0.1185 | lrm: 1.00 | dt: 13894.82ms | tok/sec: 37,732 | mfu: 12.12 | total time: 312.90m
step 01365/21400 (6.38%) | loss: 3.383586 | grad norm: 0.1203 | lrm: 1.00 | dt: 13697.47ms | tok/sec: 38,276 | mfu: 12.30 | total time: 313.13m
step 01366/21400 (6.38%) | loss: 3.382172 | grad norm: 0.1233 | lrm: 1.00 | dt: 13893.68ms | tok/sec: 37,735 | mfu: 12.12 | total time: 313.36m
step 01367/21400 (6.39%) | loss: 3.364245 | grad norm: 0.1270 | lrm: 1.00 | dt: 13715.73ms | tok/sec: 38,225 | mfu: 12.28 | total time: 313.59m
step 01368/21400 (6.39%) | loss: 3.368964 | grad norm: 0.1398 | lrm: 1.00 | dt: 13880.04ms | tok/sec: 37,772 | mfu: 12.13 | total time: 313.82m
step 01369/21400 (6.40%) | loss: 3.342680 | grad norm: 0.1514 | lrm: 1.00 | dt: 13738.24ms | tok/sec: 38,162 | mfu: 12.26 | total time: 314.05m
step 01370/21400 (6.40%) | loss: 3.358349 | grad norm: 0.1451 | lrm: 1.00 | dt: 13862.72ms | tok/sec: 37,819 | mfu: 12.15 | total time: 314.28m
step 01371/21400 (6.41%) | loss: 3.334809 | grad norm: 0.1433 | lrm: 1.00 | dt: 13763.85ms | tok/sec: 38,091 | mfu: 12.24 | total time: 314.51m
step 01372/21400 (6.41%) | loss: 3.311494 | grad norm: 0.1427 | lrm: 1.00 | dt: 13837.22ms | tok/sec: 37,889 | mfu: 12.17 | total time: 314.74m
step 01373/21400 (6.42%) | loss: 3.307200 | grad norm: 0.1393 | lrm: 1.00 | dt: 13814.45ms | tok/sec: 37,952 | mfu: 12.19 | total time: 314.97m
step 01374/21400 (6.42%) | loss: 3.338790 | grad norm: 0.1267 | lrm: 1.00 | dt: 13793.52ms | tok/sec: 38,009 | mfu: 12.21 | total time: 315.20m
step 01375/21400 (6.43%) | loss: 3.313686 | grad norm: 0.1330 | lrm: 1.00 | dt: 13909.87ms | tok/sec: 37,691 | mfu: 12.11 | total time: 315.43m
step 01376/21400 (6.43%) | loss: 3.330357 | grad norm: 0.1225 | lrm: 1.00 | dt: 13690.37ms | tok/sec: 38,296 | mfu: 12.30 | total time: 315.66m
step 01377/21400 (6.43%) | loss: 3.297365 | grad norm: 0.1203 | lrm: 1.00 | dt: 13910.87ms | tok/sec: 37,689 | mfu: 12.11 | total time: 315.89m
step 01378/21400 (6.44%) | loss: 3.314734 | grad norm: 0.1287 | lrm: 1.00 | dt: 13708.73ms | tok/sec: 38,244 | mfu: 12.29 | total time: 316.12m
step 01379/21400 (6.44%) | loss: 3.327464 | grad norm: 0.1434 | lrm: 1.00 | dt: 13884.44ms | tok/sec: 37,760 | mfu: 12.13 | total time: 316.35m
step 01380/21400 (6.45%) | loss: 3.292225 | grad norm: 0.1314 | lrm: 1.00 | dt: 13721.82ms | tok/sec: 38,208 | mfu: 12.27 | total time: 316.58m
step 01381/21400 (6.45%) | loss: 3.268453 | grad norm: 0.1190 | lrm: 1.00 | dt: 13878.01ms | tok/sec: 37,778 | mfu: 12.14 | total time: 316.81m
step 01382/21400 (6.46%) | loss: 3.257570 | grad norm: 0.1534 | lrm: 1.00 | dt: 13736.51ms | tok/sec: 38,167 | mfu: 12.26 | total time: 317.04m
step 01383/21400 (6.46%) | loss: 3.253568 | grad norm: 0.1552 | lrm: 1.00 | dt: 13855.22ms | tok/sec: 37,840 | mfu: 12.16 | total time: 317.27m
step 01384/21400 (6.47%) | loss: 3.255929 | grad norm: 0.1303 | lrm: 1.00 | dt: 13790.50ms | tok/sec: 38,018 | mfu: 12.21 | total time: 317.50m
step 01385/21400 (6.47%) | loss: 3.254638 | grad norm: 0.1293 | lrm: 1.00 | dt: 13824.39ms | tok/sec: 37,924 | mfu: 12.18 | total time: 317.73m
step 01386/21400 (6.48%) | loss: 3.244144 | grad norm: 0.1266 | lrm: 1.00 | dt: 13813.13ms | tok/sec: 37,955 | mfu: 12.19 | total time: 317.96m
step 01387/21400 (6.48%) | loss: 3.252571 | grad norm: 0.1420 | lrm: 1.00 | dt: 13786.38ms | tok/sec: 38,029 | mfu: 12.22 | total time: 318.19m
step 01388/21400 (6.49%) | loss: 3.257244 | grad norm: 0.1361 | lrm: 1.00 | dt: 13903.80ms | tok/sec: 37,708 | mfu: 12.11 | total time: 318.42m
step 01389/21400 (6.49%) | loss: 3.267957 | grad norm: 0.1350 | lrm: 1.00 | dt: 13685.47ms | tok/sec: 38,309 | mfu: 12.31 | total time: 318.65m
step 01390/21400 (6.50%) | loss: 3.287438 | grad norm: 0.1225 | lrm: 1.00 | dt: 14252.08ms | tok/sec: 36,786 | mfu: 11.82 | total time: 318.89m
step 01391/21400 (6.50%) | loss: 3.354696 | grad norm: 0.1251 | lrm: 1.00 | dt: 13701.40ms | tok/sec: 38,265 | mfu: 12.29 | total time: 319.12m
step 01392/21400 (6.50%) | loss: 3.252495 | grad norm: 0.1323 | lrm: 1.00 | dt: 13891.10ms | tok/sec: 37,742 | mfu: 12.12 | total time: 319.35m
step 01393/21400 (6.51%) | loss: 3.256351 | grad norm: 0.1207 | lrm: 1.00 | dt: 13701.93ms | tok/sec: 38,263 | mfu: 12.29 | total time: 319.57m
step 01394/21400 (6.51%) | loss: 3.251984 | grad norm: 0.1226 | lrm: 1.00 | dt: 13881.24ms | tok/sec: 37,769 | mfu: 12.13 | total time: 319.81m
step 01395/21400 (6.52%) | loss: 3.294394 | grad norm: 0.1212 | lrm: 1.00 | dt: 13733.18ms | tok/sec: 38,176 | mfu: 12.26 | total time: 320.04m
step 01396/21400 (6.52%) | loss: 3.281491 | grad norm: 0.1216 | lrm: 1.00 | dt: 13866.63ms | tok/sec: 37,809 | mfu: 12.15 | total time: 320.27m
step 01397/21400 (6.53%) | loss: 3.256922 | grad norm: 0.1450 | lrm: 1.00 | dt: 13772.46ms | tok/sec: 38,067 | mfu: 12.23 | total time: 320.50m
step 01398/21400 (6.53%) | loss: 3.295026 | grad norm: 0.1431 | lrm: 1.00 | dt: 13835.52ms | tok/sec: 37,894 | mfu: 12.17 | total time: 320.73m
step 01399/21400 (6.54%) | loss: 3.282633 | grad norm: 0.1268 | lrm: 1.00 | dt: 13797.72ms | tok/sec: 37,998 | mfu: 12.21 | total time: 320.96m
step 01400/21400 (6.54%) | loss: 3.274116 | grad norm: 0.1228 | lrm: 1.00 | dt: 13813.56ms | tok/sec: 37,954 | mfu: 12.19 | total time: 321.19m
step 01401/21400 (6.55%) | loss: 3.302850 | grad norm: 0.1380 | lrm: 1.00 | dt: 13917.37ms | tok/sec: 37,671 | mfu: 12.10 | total time: 321.42m
step 01402/21400 (6.55%) | loss: 3.270058 | grad norm: 0.1457 | lrm: 1.00 | dt: 13691.21ms | tok/sec: 38,293 | mfu: 12.30 | total time: 321.65m
step 01403/21400 (6.56%) | loss: 3.245443 | grad norm: 0.1489 | lrm: 1.00 | dt: 13901.57ms | tok/sec: 37,714 | mfu: 12.12 | total time: 321.88m
step 01404/21400 (6.56%) | loss: 3.299929 | grad norm: 0.1515 | lrm: 1.00 | dt: 13706.70ms | tok/sec: 38,250 | mfu: 12.29 | total time: 322.11m
step 01405/21400 (6.57%) | loss: 3.276280 | grad norm: 0.1394 | lrm: 1.00 | dt: 13909.88ms | tok/sec: 37,691 | mfu: 12.11 | total time: 322.34m
step 01406/21400 (6.57%) | loss: 3.243986 | grad norm: 0.1460 | lrm: 1.00 | dt: 13717.34ms | tok/sec: 38,220 | mfu: 12.28 | total time: 322.57m
step 01407/21400 (6.57%) | loss: 3.244360 | grad norm: 0.1368 | lrm: 1.00 | dt: 13881.99ms | tok/sec: 37,767 | mfu: 12.13 | total time: 322.80m
step 01408/21400 (6.58%) | loss: 3.289548 | grad norm: 0.1261 | lrm: 1.00 | dt: 13745.64ms | tok/sec: 38,142 | mfu: 12.25 | total time: 323.03m
step 01409/21400 (6.58%) | loss: 3.293470 | grad norm: 0.1211 | lrm: 1.00 | dt: 13865.02ms | tok/sec: 37,813 | mfu: 12.15 | total time: 323.26m
step 01410/21400 (6.59%) | loss: 3.290534 | grad norm: 0.1296 | lrm: 1.00 | dt: 13768.36ms | tok/sec: 38,079 | mfu: 12.23 | total time: 323.49m
step 01411/21400 (6.59%) | loss: 3.384242 | grad norm: 0.1465 | lrm: 1.00 | dt: 13847.42ms | tok/sec: 37,861 | mfu: 12.16 | total time: 323.72m
step 01412/21400 (6.60%) | loss: 3.395237 | grad norm: 0.1236 | lrm: 1.00 | dt: 13811.05ms | tok/sec: 37,961 | mfu: 12.20 | total time: 323.95m
step 01413/21400 (6.60%) | loss: 3.381016 | grad norm: 0.1129 | lrm: 1.00 | dt: 13804.17ms | tok/sec: 37,980 | mfu: 12.20 | total time: 324.18m
step 01414/21400 (6.61%) | loss: 3.373320 | grad norm: 0.1299 | lrm: 1.00 | dt: 13924.55ms | tok/sec: 37,652 | mfu: 12.10 | total time: 324.41m
step 01415/21400 (6.61%) | loss: 3.368535 | grad norm: 0.1399 | lrm: 1.00 | dt: 13687.22ms | tok/sec: 38,304 | mfu: 12.31 | total time: 324.64m
step 01416/21400 (6.62%) | loss: 3.356585 | grad norm: 0.1320 | lrm: 1.00 | dt: 13912.14ms | tok/sec: 37,685 | mfu: 12.11 | total time: 324.87m
step 01417/21400 (6.62%) | loss: 3.310097 | grad norm: 0.1295 | lrm: 1.00 | dt: 13710.61ms | tok/sec: 38,239 | mfu: 12.28 | total time: 325.10m
step 01418/21400 (6.63%) | loss: 3.316706 | grad norm: 0.1313 | lrm: 1.00 | dt: 13902.15ms | tok/sec: 37,712 | mfu: 12.12 | total time: 325.33m
step 01419/21400 (6.63%) | loss: 3.307010 | grad norm: 0.1277 | lrm: 1.00 | dt: 13720.58ms | tok/sec: 38,211 | mfu: 12.28 | total time: 325.56m
step 01420/21400 (6.64%) | loss: 3.267672 | grad norm: 0.1335 | lrm: 1.00 | dt: 13881.38ms | tok/sec: 37,769 | mfu: 12.13 | total time: 325.79m
step 01421/21400 (6.64%) | loss: 3.246440 | grad norm: 0.1419 | lrm: 1.00 | dt: 13730.47ms | tok/sec: 38,184 | mfu: 12.27 | total time: 326.02m
step 01422/21400 (6.64%) | loss: 3.295249 | grad norm: 0.1394 | lrm: 1.00 | dt: 13868.34ms | tok/sec: 37,804 | mfu: 12.14 | total time: 326.25m
step 01423/21400 (6.65%) | loss: 3.296235 | grad norm: 0.1784 | lrm: 1.00 | dt: 13754.60ms | tok/sec: 38,117 | mfu: 12.25 | total time: 326.48m
step 01424/21400 (6.65%) | loss: 3.288487 | grad norm: 0.1431 | lrm: 1.00 | dt: 13849.19ms | tok/sec: 37,856 | mfu: 12.16 | total time: 326.71m
step 01425/21400 (6.66%) | loss: 3.251200 | grad norm: 0.1258 | lrm: 1.00 | dt: 13793.51ms | tok/sec: 38,009 | mfu: 12.21 | total time: 326.94m
step 01426/21400 (6.66%) | loss: 3.225928 | grad norm: 0.1301 | lrm: 1.00 | dt: 13801.58ms | tok/sec: 37,987 | mfu: 12.20 | total time: 327.17m
step 01427/21400 (6.67%) | loss: 3.251910 | grad norm: 0.1243 | lrm: 1.00 | dt: 14234.81ms | tok/sec: 36,831 | mfu: 11.83 | total time: 327.41m
step 01428/21400 (6.67%) | loss: 3.262040 | grad norm: 0.1185 | lrm: 1.00 | dt: 13763.83ms | tok/sec: 38,091 | mfu: 12.24 | total time: 327.64m
step 01429/21400 (6.68%) | loss: 3.254831 | grad norm: 0.1246 | lrm: 1.00 | dt: 13915.68ms | tok/sec: 37,676 | mfu: 12.10 | total time: 327.87m
step 01430/21400 (6.68%) | loss: 3.262529 | grad norm: 0.1243 | lrm: 1.00 | dt: 13690.43ms | tok/sec: 38,295 | mfu: 12.30 | total time: 328.10m
step 01431/21400 (6.69%) | loss: 3.267938 | grad norm: 0.1337 | lrm: 1.00 | dt: 13903.23ms | tok/sec: 37,709 | mfu: 12.11 | total time: 328.33m
step 01432/21400 (6.69%) | loss: 3.270787 | grad norm: 0.1348 | lrm: 1.00 | dt: 13704.77ms | tok/sec: 38,255 | mfu: 12.29 | total time: 328.56m
step 01433/21400 (6.70%) | loss: 3.263099 | grad norm: 0.1370 | lrm: 1.00 | dt: 13880.41ms | tok/sec: 37,771 | mfu: 12.13 | total time: 328.79m
step 01434/21400 (6.70%) | loss: 3.318550 | grad norm: 0.1414 | lrm: 1.00 | dt: 13746.97ms | tok/sec: 38,138 | mfu: 12.25 | total time: 329.02m
step 01435/21400 (6.71%) | loss: 3.319215 | grad norm: 0.1425 | lrm: 1.00 | dt: 13866.84ms | tok/sec: 37,808 | mfu: 12.15 | total time: 329.25m
step 01436/21400 (6.71%) | loss: 3.321674 | grad norm: 0.1364 | lrm: 1.00 | dt: 13758.90ms | tok/sec: 38,105 | mfu: 12.24 | total time: 329.48m
step 01437/21400 (6.71%) | loss: 3.333542 | grad norm: 0.1283 | lrm: 1.00 | dt: 13841.58ms | tok/sec: 37,877 | mfu: 12.17 | total time: 329.71m
step 01438/21400 (6.72%) | loss: 3.321816 | grad norm: 0.1233 | lrm: 1.00 | dt: 13789.80ms | tok/sec: 38,019 | mfu: 12.21 | total time: 329.94m
step 01439/21400 (6.72%) | loss: 3.294189 | grad norm: 0.1178 | lrm: 1.00 | dt: 13799.05ms | tok/sec: 37,994 | mfu: 12.21 | total time: 330.17m
step 01440/21400 (6.73%) | loss: 3.281088 | grad norm: 0.1265 | lrm: 1.00 | dt: 13837.63ms | tok/sec: 37,888 | mfu: 12.17 | total time: 330.40m
step 01441/21400 (6.73%) | loss: 3.266867 | grad norm: 0.1266 | lrm: 1.00 | dt: 13765.15ms | tok/sec: 38,088 | mfu: 12.24 | total time: 330.63m
step 01442/21400 (6.74%) | loss: 3.305511 | grad norm: 0.1219 | lrm: 1.00 | dt: 13904.42ms | tok/sec: 37,706 | mfu: 12.11 | total time: 330.86m
step 01443/21400 (6.74%) | loss: 3.348775 | grad norm: 0.1222 | lrm: 1.00 | dt: 13696.89ms | tok/sec: 38,277 | mfu: 12.30 | total time: 331.09m
step 01444/21400 (6.75%) | loss: 3.357295 | grad norm: 0.1267 | lrm: 1.00 | dt: 13899.56ms | tok/sec: 37,719 | mfu: 12.12 | total time: 331.32m
step 01445/21400 (6.75%) | loss: 3.419375 | grad norm: 0.1411 | lrm: 1.00 | dt: 13704.21ms | tok/sec: 38,257 | mfu: 12.29 | total time: 331.55m
step 01446/21400 (6.76%) | loss: 3.361468 | grad norm: 0.1408 | lrm: 1.00 | dt: 13880.65ms | tok/sec: 37,771 | mfu: 12.13 | total time: 331.78m
step 01447/21400 (6.76%) | loss: 3.326628 | grad norm: 0.1416 | lrm: 1.00 | dt: 13734.03ms | tok/sec: 38,174 | mfu: 12.26 | total time: 332.01m
step 01448/21400 (6.77%) | loss: 3.300801 | grad norm: 0.1392 | lrm: 1.00 | dt: 13862.01ms | tok/sec: 37,821 | mfu: 12.15 | total time: 332.24m
step 01449/21400 (6.77%) | loss: 3.269710 | grad norm: 0.1350 | lrm: 1.00 | dt: 13746.19ms | tok/sec: 38,140 | mfu: 12.25 | total time: 332.47m
step 01450/21400 (6.78%) | loss: 3.266441 | grad norm: 0.1355 | lrm: 1.00 | dt: 13847.33ms | tok/sec: 37,862 | mfu: 12.16 | total time: 332.70m
step 01451/21400 (6.78%) | loss: 3.309451 | grad norm: 0.1315 | lrm: 1.00 | dt: 13782.69ms | tok/sec: 38,039 | mfu: 12.22 | total time: 332.93m
step 01452/21400 (6.79%) | loss: 3.348395 | grad norm: 0.1556 | lrm: 1.00 | dt: 13809.70ms | tok/sec: 37,965 | mfu: 12.20 | total time: 333.16m
step 01453/21400 (6.79%) | loss: 3.365583 | grad norm: 0.1311 | lrm: 1.00 | dt: 13830.99ms | tok/sec: 37,906 | mfu: 12.18 | total time: 333.39m
step 01454/21400 (6.79%) | loss: 3.382980 | grad norm: 0.1180 | lrm: 1.00 | dt: 13833.51ms | tok/sec: 37,899 | mfu: 12.18 | total time: 333.62m
step 01455/21400 (6.80%) | loss: 3.375006 | grad norm: 0.1277 | lrm: 1.00 | dt: 13913.37ms | tok/sec: 37,682 | mfu: 12.11 | total time: 333.85m
step 01456/21400 (6.80%) | loss: 3.351692 | grad norm: 0.1314 | lrm: 1.00 | dt: 13694.13ms | tok/sec: 38,285 | mfu: 12.30 | total time: 334.08m
step 01457/21400 (6.81%) | loss: 3.320192 | grad norm: 0.1364 | lrm: 1.00 | dt: 13902.78ms | tok/sec: 37,711 | mfu: 12.11 | total time: 334.31m
step 01458/21400 (6.81%) | loss: 3.343376 | grad norm: 0.1369 | lrm: 1.00 | dt: 13714.63ms | tok/sec: 38,228 | mfu: 12.28 | total time: 334.54m
step 01459/21400 (6.82%) | loss: 3.304164 | grad norm: 0.1275 | lrm: 1.00 | dt: 13884.93ms | tok/sec: 37,759 | mfu: 12.13 | total time: 334.77m
step 01460/21400 (6.82%) | loss: 3.315169 | grad norm: 0.1198 | lrm: 1.00 | dt: 13737.11ms | tok/sec: 38,165 | mfu: 12.26 | total time: 335.00m
step 01461/21400 (6.83%) | loss: 3.336713 | grad norm: 0.1219 | lrm: 1.00 | dt: 13877.23ms | tok/sec: 37,780 | mfu: 12.14 | total time: 335.23m
step 01462/21400 (6.83%) | loss: 3.321064 | grad norm: 0.1201 | lrm: 1.00 | dt: 13756.79ms | tok/sec: 38,111 | mfu: 12.24 | total time: 335.46m
step 01463/21400 (6.84%) | loss: 3.300351 | grad norm: 0.1369 | lrm: 1.00 | dt: 13850.18ms | tok/sec: 37,854 | mfu: 12.16 | total time: 335.69m
step 01464/21400 (6.84%) | loss: 3.277608 | grad norm: 0.1250 | lrm: 1.00 | dt: 13799.93ms | tok/sec: 37,992 | mfu: 12.21 | total time: 335.92m
step 01465/21400 (6.85%) | loss: 3.268912 | grad norm: 0.1137 | lrm: 1.00 | dt: 14187.94ms | tok/sec: 36,953 | mfu: 11.87 | total time: 336.16m
step 01466/21400 (6.85%) | loss: 3.214262 | grad norm: 0.1262 | lrm: 1.00 | dt: 13834.74ms | tok/sec: 37,896 | mfu: 12.17 | total time: 336.39m
step 01467/21400 (6.86%) | loss: 3.201633 | grad norm: 0.1227 | lrm: 1.00 | dt: 13760.62ms | tok/sec: 38,100 | mfu: 12.24 | total time: 336.62m
step 01468/21400 (6.86%) | loss: 3.182206 | grad norm: 0.1343 | lrm: 1.00 | dt: 13908.95ms | tok/sec: 37,694 | mfu: 12.11 | total time: 336.85m
step 01469/21400 (6.86%) | loss: 3.199146 | grad norm: 0.1296 | lrm: 1.00 | dt: 13687.15ms | tok/sec: 38,305 | mfu: 12.31 | total time: 337.08m
step 01470/21400 (6.87%) | loss: 3.219821 | grad norm: 0.1083 | lrm: 1.00 | dt: 13897.18ms | tok/sec: 37,726 | mfu: 12.12 | total time: 337.31m
step 01471/21400 (6.87%) | loss: 3.220187 | grad norm: 0.1251 | lrm: 1.00 | dt: 13711.87ms | tok/sec: 38,236 | mfu: 12.28 | total time: 337.54m
step 01472/21400 (6.88%) | loss: 3.227027 | grad norm: 0.1419 | lrm: 1.00 | dt: 13887.94ms | tok/sec: 37,751 | mfu: 12.13 | total time: 337.77m
step 01473/21400 (6.88%) | loss: 3.225347 | grad norm: 0.1253 | lrm: 1.00 | dt: 13725.75ms | tok/sec: 38,197 | mfu: 12.27 | total time: 338.00m
step 01474/21400 (6.89%) | loss: 3.191961 | grad norm: 0.1389 | lrm: 1.00 | dt: 13859.36ms | tok/sec: 37,829 | mfu: 12.15 | total time: 338.23m
step 01475/21400 (6.89%) | loss: 3.196163 | grad norm: 0.1495 | lrm: 1.00 | dt: 13745.86ms | tok/sec: 38,141 | mfu: 12.25 | total time: 338.46m
step 01476/21400 (6.90%) | loss: 3.222892 | grad norm: 0.1430 | lrm: 1.00 | dt: 13851.14ms | tok/sec: 37,851 | mfu: 12.16 | total time: 338.69m
step 01477/21400 (6.90%) | loss: 3.219754 | grad norm: 0.1278 | lrm: 1.00 | dt: 13778.99ms | tok/sec: 38,049 | mfu: 12.22 | total time: 338.92m
step 01478/21400 (6.91%) | loss: 3.245956 | grad norm: 0.1273 | lrm: 1.00 | dt: 13809.38ms | tok/sec: 37,966 | mfu: 12.20 | total time: 339.15m
step 01479/21400 (6.91%) | loss: 3.241279 | grad norm: 0.1270 | lrm: 1.00 | dt: 13823.39ms | tok/sec: 37,927 | mfu: 12.18 | total time: 339.38m
step 01480/21400 (6.92%) | loss: 3.247764 | grad norm: 0.1285 | lrm: 1.00 | dt: 13764.83ms | tok/sec: 38,088 | mfu: 12.24 | total time: 339.61m
step 01481/21400 (6.92%) | loss: 3.253220 | grad norm: 0.1294 | lrm: 1.00 | dt: 13919.06ms | tok/sec: 37,666 | mfu: 12.10 | total time: 339.84m
step 01482/21400 (6.93%) | loss: 3.270464 | grad norm: 0.1298 | lrm: 1.00 | dt: 13681.65ms | tok/sec: 38,320 | mfu: 12.31 | total time: 340.07m
step 01483/21400 (6.93%) | loss: 3.254492 | grad norm: 0.1228 | lrm: 1.00 | dt: 13902.59ms | tok/sec: 37,711 | mfu: 12.11 | total time: 340.30m
step 01484/21400 (6.93%) | loss: 3.251842 | grad norm: 0.1420 | lrm: 1.00 | dt: 13709.35ms | tok/sec: 38,243 | mfu: 12.29 | total time: 340.53m
step 01485/21400 (6.94%) | loss: 3.264397 | grad norm: 0.1506 | lrm: 1.00 | dt: 13893.07ms | tok/sec: 37,737 | mfu: 12.12 | total time: 340.76m
step 01486/21400 (6.94%) | loss: 3.273970 | grad norm: 0.1568 | lrm: 1.00 | dt: 13718.31ms | tok/sec: 38,218 | mfu: 12.28 | total time: 340.99m
step 01487/21400 (6.95%) | loss: 3.269345 | grad norm: 0.1545 | lrm: 1.00 | dt: 13870.66ms | tok/sec: 37,798 | mfu: 12.14 | total time: 341.22m
step 01488/21400 (6.95%) | loss: 3.267422 | grad norm: 0.1521 | lrm: 1.00 | dt: 13750.38ms | tok/sec: 38,128 | mfu: 12.25 | total time: 341.45m
step 01489/21400 (6.96%) | loss: 3.233776 | grad norm: 0.1267 | lrm: 1.00 | dt: 13838.99ms | tok/sec: 37,884 | mfu: 12.17 | total time: 341.68m
step 01490/21400 (6.96%) | loss: 3.239214 | grad norm: 0.1192 | lrm: 1.00 | dt: 13783.21ms | tok/sec: 38,038 | mfu: 12.22 | total time: 341.91m
step 01491/21400 (6.97%) | loss: 3.259502 | grad norm: 0.1302 | lrm: 1.00 | dt: 13806.99ms | tok/sec: 37,972 | mfu: 12.20 | total time: 342.14m
step 01492/21400 (6.97%) | loss: 3.248498 | grad norm: 0.1466 | lrm: 1.00 | dt: 13847.17ms | tok/sec: 37,862 | mfu: 12.16 | total time: 342.37m
step 01493/21400 (6.98%) | loss: 3.221260 | grad norm: 0.1326 | lrm: 1.00 | dt: 13764.48ms | tok/sec: 38,089 | mfu: 12.24 | total time: 342.60m
step 01494/21400 (6.98%) | loss: 3.182818 | grad norm: 0.1457 | lrm: 1.00 | dt: 13907.98ms | tok/sec: 37,696 | mfu: 12.11 | total time: 342.83m
step 01495/21400 (6.99%) | loss: 3.186024 | grad norm: 0.1250 | lrm: 1.00 | dt: 13691.62ms | tok/sec: 38,292 | mfu: 12.30 | total time: 343.06m
step 01496/21400 (6.99%) | loss: 3.185854 | grad norm: 0.1330 | lrm: 1.00 | dt: 13903.57ms | tok/sec: 37,708 | mfu: 12.11 | total time: 343.29m
step 01497/21400 (7.00%) | loss: 3.197589 | grad norm: 0.1263 | lrm: 1.00 | dt: 13713.76ms | tok/sec: 38,230 | mfu: 12.28 | total time: 343.52m
step 01498/21400 (7.00%) | loss: 3.198638 | grad norm: 0.1244 | lrm: 1.00 | dt: 13910.17ms | tok/sec: 37,690 | mfu: 12.11 | total time: 343.76m
step 01499/21400 (7.00%) | loss: 3.234293 | grad norm: 0.1191 | lrm: 1.00 | dt: 13722.19ms | tok/sec: 38,207 | mfu: 12.27 | total time: 343.98m
Step 01500 | Validation bpb: 0.9792
step 01500/21400 (7.01%) | loss: 3.250478 | grad norm: 0.1207 | lrm: 1.00 | dt: 14229.60ms | tok/sec: 36,844 | mfu: 11.84 | total time: 344.22m
2025-11-11 04:57:54,286 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model file to: /home/henny/.cache/nanochat/base_checkpoints/d20/model_001500.pt
2025-11-11 04:57:58,901 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer file to: /home/henny/.cache/nanochat/base_checkpoints/d20/optim_001500.pt
2025-11-11 04:57:58,902 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata file to: /home/henny/.cache/nanochat/base_checkpoints/d20/meta_001500.json
step 01501/21400 (7.01%) | loss: 3.272515 | grad norm: 0.1175 | lrm: 1.00 | dt: 13909.94ms | tok/sec: 37,691 | mfu: 12.11 | total time: 344.45m
step 01502/21400 (7.02%) | loss: 3.265345 | grad norm: 0.1146 | lrm: 1.00 | dt: 13885.33ms | tok/sec: 37,758 | mfu: 12.13 | total time: 344.68m
step 01503/21400 (7.02%) | loss: 3.286226 | grad norm: 0.1186 | lrm: 1.00 | dt: 13708.56ms | tok/sec: 38,245 | mfu: 12.29 | total time: 344.91m
step 01504/21400 (7.03%) | loss: 3.277360 | grad norm: 0.1206 | lrm: 1.00 | dt: 13881.55ms | tok/sec: 37,768 | mfu: 12.13 | total time: 345.14m
step 01505/21400 (7.03%) | loss: 3.286838 | grad norm: 0.1242 | lrm: 1.00 | dt: 13726.64ms | tok/sec: 38,194 | mfu: 12.27 | total time: 345.37m
step 01506/21400 (7.04%) | loss: 3.308171 | grad norm: 0.1152 | lrm: 1.00 | dt: 13866.37ms | tok/sec: 37,810 | mfu: 12.15 | total time: 345.60m
step 01507/21400 (7.04%) | loss: 3.341441 | grad norm: 0.1197 | lrm: 1.00 | dt: 13757.00ms | tok/sec: 38,110 | mfu: 12.24 | total time: 345.83m
step 01508/21400 (7.05%) | loss: 3.330789 | grad norm: 0.1210 | lrm: 1.00 | dt: 13832.84ms | tok/sec: 37,901 | mfu: 12.18 | total time: 346.06m
step 01509/21400 (7.05%) | loss: 3.325557 | grad norm: 0.1309 | lrm: 1.00 | dt: 13820.20ms | tok/sec: 37,936 | mfu: 12.19 | total time: 346.29m
step 01510/21400 (7.06%) | loss: 3.308981 | grad norm: 0.1270 | lrm: 1.00 | dt: 13803.83ms | tok/sec: 37,981 | mfu: 12.20 | total time: 346.52m
step 01511/21400 (7.06%) | loss: 3.249201 | grad norm: 0.1239 | lrm: 1.00 | dt: 13914.03ms | tok/sec: 37,680 | mfu: 12.10 | total time: 346.76m
step 01512/21400 (7.07%) | loss: 3.269574 | grad norm: 0.1325 | lrm: 1.00 | dt: 13686.92ms | tok/sec: 38,305 | mfu: 12.31 | total time: 346.98m
step 01513/21400 (7.07%) | loss: 3.304056 | grad norm: 0.1261 | lrm: 1.00 | dt: 13909.92ms | tok/sec: 37,691 | mfu: 12.11 | total time: 347.22m
step 01514/21400 (7.07%) | loss: 3.307064 | grad norm: 0.1294 | lrm: 1.00 | dt: 13695.96ms | tok/sec: 38,280 | mfu: 12.30 | total time: 347.44m
step 01515/21400 (7.08%) | loss: 3.312776 | grad norm: 0.1330 | lrm: 1.00 | dt: 13894.49ms | tok/sec: 37,733 | mfu: 12.12 | total time: 347.68m
step 01516/21400 (7.08%) | loss: 3.309035 | grad norm: 0.1292 | lrm: 1.00 | dt: 13716.14ms | tok/sec: 38,224 | mfu: 12.28 | total time: 347.90m
step 01517/21400 (7.09%) | loss: 3.283296 | grad norm: 0.1199 | lrm: 1.00 | dt: 13892.05ms | tok/sec: 37,740 | mfu: 12.12 | total time: 348.14m
step 01518/21400 (7.09%) | loss: 3.294444 | grad norm: 0.1285 | lrm: 1.00 | dt: 13731.15ms | tok/sec: 38,182 | mfu: 12.27 | total time: 348.36m
step 01519/21400 (7.10%) | loss: 3.282010 | grad norm: 0.1423 | lrm: 1.00 | dt: 13863.88ms | tok/sec: 37,816 | mfu: 12.15 | total time: 348.60m
step 01520/21400 (7.10%) | loss: 3.296826 | grad norm: 0.1416 | lrm: 1.00 | dt: 13752.49ms | tok/sec: 38,123 | mfu: 12.25 | total time: 348.83m
step 01521/21400 (7.11%) | loss: 3.294776 | grad norm: 0.1337 | lrm: 1.00 | dt: 13856.18ms | tok/sec: 37,837 | mfu: 12.16 | total time: 349.06m
step 01522/21400 (7.11%) | loss: 3.275975 | grad norm: 0.1338 | lrm: 1.00 | dt: 13828.62ms | tok/sec: 37,913 | mfu: 12.18 | total time: 349.29m
step 01523/21400 (7.12%) | loss: 3.193284 | grad norm: 0.1443 | lrm: 1.00 | dt: 13810.19ms | tok/sec: 37,963 | mfu: 12.20 | total time: 349.52m
step 01524/21400 (7.12%) | loss: 3.152231 | grad norm: 0.1411 | lrm: 1.00 | dt: 13921.03ms | tok/sec: 37,661 | mfu: 12.10 | total time: 349.75m
step 01525/21400 (7.13%) | loss: 3.155984 | grad norm: 0.1457 | lrm: 1.00 | dt: 13693.28ms | tok/sec: 38,287 | mfu: 12.30 | total time: 349.98m
step 01526/21400 (7.13%) | loss: 3.148514 | grad norm: 0.1252 | lrm: 1.00 | dt: 13921.72ms | tok/sec: 37,659 | mfu: 12.10 | total time: 350.21m
step 01527/21400 (7.14%) | loss: 3.173584 | grad norm: 0.1202 | lrm: 1.00 | dt: 13707.10ms | tok/sec: 38,249 | mfu: 12.29 | total time: 350.44m
step 01528/21400 (7.14%) | loss: 3.171039 | grad norm: 0.1200 | lrm: 1.00 | dt: 13900.17ms | tok/sec: 37,718 | mfu: 12.12 | total time: 350.67m
step 01529/21400 (7.14%) | loss: 3.190151 | grad norm: 0.1180 | lrm: 1.00 | dt: 13696.34ms | tok/sec: 38,279 | mfu: 12.30 | total time: 350.90m
step 01530/21400 (7.15%) | loss: 3.187917 | grad norm: 0.1276 | lrm: 1.00 | dt: 13891.67ms | tok/sec: 37,741 | mfu: 12.12 | total time: 351.13m
step 01531/21400 (7.15%) | loss: 3.242663 | grad norm: 0.1207 | lrm: 1.00 | dt: 13724.71ms | tok/sec: 38,200 | mfu: 12.27 | total time: 351.36m
step 01532/21400 (7.16%) | loss: 3.273607 | grad norm: 0.1288 | lrm: 1.00 | dt: 13872.55ms | tok/sec: 37,793 | mfu: 12.14 | total time: 351.59m
step 01533/21400 (7.16%) | loss: 3.276249 | grad norm: 0.1328 | lrm: 1.00 | dt: 13761.51ms | tok/sec: 38,098 | mfu: 12.24 | total time: 351.82m
step 01534/21400 (7.17%) | loss: 3.260269 | grad norm: 0.1172 | lrm: 1.00 | dt: 13846.51ms | tok/sec: 37,864 | mfu: 12.16 | total time: 352.05m
step 01535/21400 (7.17%) | loss: 3.253347 | grad norm: 0.1207 | lrm: 1.00 | dt: 13832.21ms | tok/sec: 37,903 | mfu: 12.18 | total time: 352.28m
step 01536/21400 (7.18%) | loss: 3.261793 | grad norm: 0.1235 | lrm: 1.00 | dt: 13921.82ms | tok/sec: 37,659 | mfu: 12.10 | total time: 352.51m
step 01537/21400 (7.18%) | loss: 3.253011 | grad norm: 0.1176 | lrm: 1.00 | dt: 13656.87ms | tok/sec: 38,390 | mfu: 12.33 | total time: 352.74m
step 01538/21400 (7.19%) | loss: 3.280122 | grad norm: 0.1182 | lrm: 1.00 | dt: 13803.81ms | tok/sec: 37,981 | mfu: 12.20 | total time: 352.97m
step 01539/21400 (7.19%) | loss: 3.252953 | grad norm: 0.1222 | lrm: 1.00 | dt: 14365.07ms | tok/sec: 36,497 | mfu: 11.72 | total time: 353.21m
step 01540/21400 (7.20%) | loss: 3.273904 | grad norm: 0.1285 | lrm: 1.00 | dt: 13685.44ms | tok/sec: 38,309 | mfu: 12.31 | total time: 353.44m
step 01541/21400 (7.20%) | loss: 3.265531 | grad norm: 0.1241 | lrm: 1.00 | dt: 13903.90ms | tok/sec: 37,707 | mfu: 12.11 | total time: 353.67m
step 01542/21400 (7.21%) | loss: 3.229121 | grad norm: 0.1276 | lrm: 1.00 | dt: 13703.12ms | tok/sec: 38,260 | mfu: 12.29 | total time: 353.90m
step 01543/21400 (7.21%) | loss: 3.226308 | grad norm: 0.1191 | lrm: 1.00 | dt: 13884.12ms | tok/sec: 37,761 | mfu: 12.13 | total time: 354.13m
step 01544/21400 (7.21%) | loss: 3.240158 | grad norm: 0.1285 | lrm: 1.00 | dt: 13709.14ms | tok/sec: 38,243 | mfu: 12.29 | total time: 354.36m
step 01545/21400 (7.22%) | loss: 3.249008 | grad norm: 0.1400 | lrm: 1.00 | dt: 13875.83ms | tok/sec: 37,784 | mfu: 12.14 | total time: 354.59m
step 01546/21400 (7.22%) | loss: 3.254801 | grad norm: 0.1423 | lrm: 1.00 | dt: 13732.60ms | tok/sec: 38,178 | mfu: 12.26 | total time: 354.82m
step 01547/21400 (7.23%) | loss: 3.249069 | grad norm: 0.1340 | lrm: 1.00 | dt: 13857.61ms | tok/sec: 37,833 | mfu: 12.15 | total time: 355.05m
step 01548/21400 (7.23%) | loss: 3.270523 | grad norm: 0.1386 | lrm: 1.00 | dt: 13767.05ms | tok/sec: 38,082 | mfu: 12.23 | total time: 355.28m
step 01549/21400 (7.24%) | loss: 3.269590 | grad norm: 0.1334 | lrm: 1.00 | dt: 13835.73ms | tok/sec: 37,893 | mfu: 12.17 | total time: 355.51m
step 01550/21400 (7.24%) | loss: 3.276826 | grad norm: 0.1355 | lrm: 1.00 | dt: 13818.08ms | tok/sec: 37,942 | mfu: 12.19 | total time: 355.74m
step 01551/21400 (7.25%) | loss: 3.276031 | grad norm: 0.1288 | lrm: 1.00 | dt: 13784.62ms | tok/sec: 38,034 | mfu: 12.22 | total time: 355.97m
step 01552/21400 (7.25%) | loss: 3.248948 | grad norm: 0.1170 | lrm: 1.00 | dt: 13913.94ms | tok/sec: 37,680 | mfu: 12.11 | total time: 356.20m
step 01553/21400 (7.26%) | loss: 3.223157 | grad norm: 0.1093 | lrm: 1.00 | dt: 13683.04ms | tok/sec: 38,316 | mfu: 12.31 | total time: 356.43m
step 01554/21400 (7.26%) | loss: 3.218091 | grad norm: 0.1265 | lrm: 1.00 | dt: 13911.38ms | tok/sec: 37,687 | mfu: 12.11 | total time: 356.66m
step 01555/21400 (7.27%) | loss: 3.240160 | grad norm: 0.1298 | lrm: 1.00 | dt: 13708.47ms | tok/sec: 38,245 | mfu: 12.29 | total time: 356.89m
step 01556/21400 (7.27%) | loss: 3.213024 | grad norm: 0.1279 | lrm: 1.00 | dt: 13893.67ms | tok/sec: 37,735 | mfu: 12.12 | total time: 357.12m
step 01557/21400 (7.28%) | loss: 3.225084 | grad norm: 0.1299 | lrm: 1.00 | dt: 13715.35ms | tok/sec: 38,226 | mfu: 12.28 | total time: 357.35m
step 01558/21400 (7.28%) | loss: 3.210763 | grad norm: 0.1327 | lrm: 1.00 | dt: 13878.72ms | tok/sec: 37,776 | mfu: 12.14 | total time: 357.58m
step 01559/21400 (7.29%) | loss: 3.277856 | grad norm: 0.1265 | lrm: 1.00 | dt: 13735.73ms | tok/sec: 38,169 | mfu: 12.26 | total time: 357.81m
step 01560/21400 (7.29%) | loss: 3.271136 | grad norm: 0.1305 | lrm: 1.00 | dt: 13852.20ms | tok/sec: 37,848 | mfu: 12.16 | total time: 358.04m
step 01561/21400 (7.29%) | loss: 3.301047 | grad norm: 0.1248 | lrm: 1.00 | dt: 13770.44ms | tok/sec: 38,073 | mfu: 12.23 | total time: 358.27m
step 01562/21400 (7.30%) | loss: 3.320626 | grad norm: 0.1237 | lrm: 1.00 | dt: 13830.97ms | tok/sec: 37,906 | mfu: 12.18 | total time: 358.50m
step 01563/21400 (7.30%) | loss: 3.344843 | grad norm: 0.1303 | lrm: 1.00 | dt: 13810.17ms | tok/sec: 37,963 | mfu: 12.20 | total time: 358.73m
step 01564/21400 (7.31%) | loss: 3.296859 | grad norm: 0.1271 | lrm: 1.00 | dt: 13798.19ms | tok/sec: 37,996 | mfu: 12.21 | total time: 358.96m
step 01565/21400 (7.31%) | loss: 3.289560 | grad norm: 0.1113 | lrm: 1.00 | dt: 13915.47ms | tok/sec: 37,676 | mfu: 12.10 | total time: 359.19m
step 01566/21400 (7.32%) | loss: 3.320031 | grad norm: 0.1198 | lrm: 1.00 | dt: 13685.94ms | tok/sec: 38,308 | mfu: 12.31 | total time: 359.42m
step 01567/21400 (7.32%) | loss: 3.312197 | grad norm: 0.1248 | lrm: 1.00 | dt: 13909.75ms | tok/sec: 37,692 | mfu: 12.11 | total time: 359.65m
step 01568/21400 (7.33%) | loss: 3.304538 | grad norm: 0.1190 | lrm: 1.00 | dt: 13694.95ms | tok/sec: 38,283 | mfu: 12.30 | total time: 359.88m
step 01569/21400 (7.33%) | loss: 3.312913 | grad norm: 0.1182 | lrm: 1.00 | dt: 13893.44ms | tok/sec: 37,736 | mfu: 12.12 | total time: 360.11m
step 01570/21400 (7.34%) | loss: 3.299405 | grad norm: 0.1248 | lrm: 1.00 | dt: 13702.24ms | tok/sec: 38,262 | mfu: 12.29 | total time: 360.34m
step 01571/21400 (7.34%) | loss: 3.264409 | grad norm: 0.1331 | lrm: 1.00 | dt: 13884.20ms | tok/sec: 37,761 | mfu: 12.13 | total time: 360.57m
step 01572/21400 (7.35%) | loss: 3.275127 | grad norm: 0.1237 | lrm: 1.00 | dt: 13727.61ms | tok/sec: 38,192 | mfu: 12.27 | total time: 360.80m
step 01573/21400 (7.35%) | loss: 3.277718 | grad norm: 0.1159 | lrm: 1.00 | dt: 13858.84ms | tok/sec: 37,830 | mfu: 12.15 | total time: 361.03m
step 01574/21400 (7.36%) | loss: 3.261953 | grad norm: 0.1253 | lrm: 1.00 | dt: 13751.28ms | tok/sec: 38,126 | mfu: 12.25 | total time: 361.26m
step 01575/21400 (7.36%) | loss: 3.255645 | grad norm: 0.1276 | lrm: 1.00 | dt: 13829.65ms | tok/sec: 37,910 | mfu: 12.18 | total time: 361.49m
step 01576/21400 (7.36%) | loss: 3.268180 | grad norm: 0.1259 | lrm: 1.00 | dt: 13799.96ms | tok/sec: 37,991 | mfu: 12.21 | total time: 361.72m
step 01577/21400 (7.37%) | loss: 3.233650 | grad norm: 0.1225 | lrm: 1.00 | dt: 13780.28ms | tok/sec: 38,046 | mfu: 12.22 | total time: 361.95m
step 01578/21400 (7.37%) | loss: 3.242699 | grad norm: 0.1212 | lrm: 1.00 | dt: 14307.56ms | tok/sec: 36,644 | mfu: 11.77 | total time: 362.19m
step 01579/21400 (7.38%) | loss: 3.269270 | grad norm: 0.1298 | lrm: 1.00 | dt: 13825.27ms | tok/sec: 37,922 | mfu: 12.18 | total time: 362.42m
step 01580/21400 (7.38%) | loss: 3.286134 | grad norm: 0.1420 | lrm: 1.00 | dt: 13693.07ms | tok/sec: 38,288 | mfu: 12.30 | total time: 362.65m
step 01581/21400 (7.39%) | loss: 3.294794 | grad norm: 0.1382 | lrm: 1.00 | dt: 13774.73ms | tok/sec: 38,061 | mfu: 12.23 | total time: 362.88m
step 01582/21400 (7.39%) | loss: 3.339721 | grad norm: 0.1521 | lrm: 1.00 | dt: 13895.02ms | tok/sec: 37,732 | mfu: 12.12 | total time: 363.11m
step 01583/21400 (7.40%) | loss: 3.347399 | grad norm: 0.1504 | lrm: 1.00 | dt: 13704.71ms | tok/sec: 38,256 | mfu: 12.29 | total time: 363.34m
step 01584/21400 (7.40%) | loss: 3.338894 | grad norm: 0.1370 | lrm: 1.00 | dt: 13885.41ms | tok/sec: 37,758 | mfu: 12.13 | total time: 363.57m
step 01585/21400 (7.41%) | loss: 3.322934 | grad norm: 0.1261 | lrm: 1.00 | dt: 13698.15ms | tok/sec: 38,274 | mfu: 12.30 | total time: 363.80m
step 01586/21400 (7.41%) | loss: 3.309621 | grad norm: 0.1267 | lrm: 1.00 | dt: 13891.19ms | tok/sec: 37,742 | mfu: 12.12 | total time: 364.03m
step 01587/21400 (7.42%) | loss: 3.338081 | grad norm: 0.1185 | lrm: 1.00 | dt: 13770.00ms | tok/sec: 38,074 | mfu: 12.23 | total time: 364.26m
step 01588/21400 (7.42%) | loss: 3.320836 | grad norm: 0.1230 | lrm: 1.00 | dt: 13845.57ms | tok/sec: 37,866 | mfu: 12.16 | total time: 364.49m
step 01589/21400 (7.43%) | loss: 3.330138 | grad norm: 0.1350 | lrm: 1.00 | dt: 13783.98ms | tok/sec: 38,036 | mfu: 12.22 | total time: 364.72m
step 01590/21400 (7.43%) | loss: 3.296373 | grad norm: 0.1312 | lrm: 1.00 | dt: 13811.54ms | tok/sec: 37,960 | mfu: 12.19 | total time: 364.95m
step 01591/21400 (7.43%) | loss: 3.261760 | grad norm: 0.1177 | lrm: 1.00 | dt: 13906.02ms | tok/sec: 37,702 | mfu: 12.11 | total time: 365.18m
step 01592/21400 (7.44%) | loss: 3.186402 | grad norm: 0.1127 | lrm: 1.00 | dt: 13689.90ms | tok/sec: 38,297 | mfu: 12.30 | total time: 365.41m
step 01593/21400 (7.44%) | loss: 3.217985 | grad norm: 0.1311 | lrm: 1.00 | dt: 13906.31ms | tok/sec: 37,701 | mfu: 12.11 | total time: 365.64m
step 01594/21400 (7.45%) | loss: 3.221053 | grad norm: 0.1248 | lrm: 1.00 | dt: 13686.29ms | tok/sec: 38,307 | mfu: 12.31 | total time: 365.87m
step 01595/21400 (7.45%) | loss: 3.198748 | grad norm: 0.1199 | lrm: 1.00 | dt: 13898.31ms | tok/sec: 37,723 | mfu: 12.12 | total time: 366.10m
step 01596/21400 (7.46%) | loss: 3.235176 | grad norm: 0.1459 | lrm: 1.00 | dt: 13721.79ms | tok/sec: 38,208 | mfu: 12.27 | total time: 366.33m
step 01597/21400 (7.46%) | loss: 3.231486 | grad norm: 0.1562 | lrm: 1.00 | dt: 13880.97ms | tok/sec: 37,770 | mfu: 12.13 | total time: 366.56m
step 01598/21400 (7.47%) | loss: 3.234420 | grad norm: 0.1254 | lrm: 1.00 | dt: 13734.49ms | tok/sec: 38,173 | mfu: 12.26 | total time: 366.79m
step 01599/21400 (7.47%) | loss: 3.255688 | grad norm: 0.1150 | lrm: 1.00 | dt: 13862.68ms | tok/sec: 37,820 | mfu: 12.15 | total time: 367.02m
step 01600/21400 (7.48%) | loss: 3.257819 | grad norm: 0.1200 | lrm: 1.00 | dt: 13756.36ms | tok/sec: 38,112 | mfu: 12.24 | total time: 367.25m
step 01601/21400 (7.48%) | loss: 3.250550 | grad norm: 0.1229 | lrm: 1.00 | dt: 13841.71ms | tok/sec: 37,877 | mfu: 12.17 | total time: 367.48m
step 01602/21400 (7.49%) | loss: 3.293053 | grad norm: 0.1270 | lrm: 1.00 | dt: 13791.85ms | tok/sec: 38,014 | mfu: 12.21 | total time: 367.71m
step 01603/21400 (7.49%) | loss: 3.370740 | grad norm: 0.1336 | lrm: 1.00 | dt: 13800.06ms | tok/sec: 37,991 | mfu: 12.20 | total time: 367.94m
step 01604/21400 (7.50%) | loss: 3.381973 | grad norm: 0.1247 | lrm: 1.00 | dt: 13904.07ms | tok/sec: 37,707 | mfu: 12.11 | total time: 368.17m
step 01605/21400 (7.50%) | loss: 3.369328 | grad norm: 0.1224 | lrm: 1.00 | dt: 13686.45ms | tok/sec: 38,307 | mfu: 12.31 | total time: 368.40m
step 01606/21400 (7.50%) | loss: 3.355573 | grad norm: 0.1242 | lrm: 1.00 | dt: 13900.01ms | tok/sec: 37,718 | mfu: 12.12 | total time: 368.63m
step 01607/21400 (7.51%) | loss: 3.343556 | grad norm: 0.1248 | lrm: 1.00 | dt: 13707.85ms | tok/sec: 38,247 | mfu: 12.29 | total time: 368.86m
step 01608/21400 (7.51%) | loss: 3.336023 | grad norm: 0.1382 | lrm: 1.00 | dt: 13904.50ms | tok/sec: 37,706 | mfu: 12.11 | total time: 369.09m
step 01609/21400 (7.52%) | loss: 3.366681 | grad norm: 0.1323 | lrm: 1.00 | dt: 13700.91ms | tok/sec: 38,266 | mfu: 12.29 | total time: 369.32m
step 01610/21400 (7.52%) | loss: 3.372872 | grad norm: 0.1250 | lrm: 1.00 | dt: 13886.31ms | tok/sec: 37,755 | mfu: 12.13 | total time: 369.55m
step 01611/21400 (7.53%) | loss: 3.348072 | grad norm: 0.1215 | lrm: 1.00 | dt: 13750.01ms | tok/sec: 38,130 | mfu: 12.25 | total time: 369.78m
step 01612/21400 (7.53%) | loss: 3.375657 | grad norm: 0.1164 | lrm: 1.00 | dt: 13871.18ms | tok/sec: 37,796 | mfu: 12.14 | total time: 370.01m
step 01613/21400 (7.54%) | loss: 3.395215 | grad norm: 0.1260 | lrm: 1.00 | dt: 13799.25ms | tok/sec: 37,993 | mfu: 12.21 | total time: 370.24m
step 01614/21400 (7.54%) | loss: 3.371875 | grad norm: 0.1201 | lrm: 1.00 | dt: 13861.29ms | tok/sec: 37,823 | mfu: 12.15 | total time: 370.47m
step 01615/21400 (7.55%) | loss: 3.374540 | grad norm: 0.1249 | lrm: 1.00 | dt: 13719.76ms | tok/sec: 38,214 | mfu: 12.28 | total time: 370.70m
step 01616/21400 (7.55%) | loss: 3.353665 | grad norm: 0.1257 | lrm: 1.00 | dt: 13810.80ms | tok/sec: 37,962 | mfu: 12.20 | total time: 370.93m
step 01617/21400 (7.56%) | loss: 3.336668 | grad norm: 0.1253 | lrm: 1.00 | dt: 13912.31ms | tok/sec: 37,685 | mfu: 12.11 | total time: 371.16m
step 01618/21400 (7.56%) | loss: 3.347002 | grad norm: 0.1260 | lrm: 1.00 | dt: 14107.12ms | tok/sec: 37,164 | mfu: 11.94 | total time: 371.40m
step 01619/21400 (7.57%) | loss: 3.355940 | grad norm: 0.1376 | lrm: 1.00 | dt: 13932.55ms | tok/sec: 37,630 | mfu: 12.09 | total time: 371.63m
step 01620/21400 (7.57%) | loss: 3.349802 | grad norm: 0.1440 | lrm: 1.00 | dt: 13707.70ms | tok/sec: 38,247 | mfu: 12.29 | total time: 371.86m
step 01621/21400 (7.57%) | loss: 3.332544 | grad norm: 0.1431 | lrm: 1.00 | dt: 13897.78ms | tok/sec: 37,724 | mfu: 12.12 | total time: 372.09m
step 01622/21400 (7.58%) | loss: 3.349757 | grad norm: 0.1462 | lrm: 1.00 | dt: 13723.43ms | tok/sec: 38,203 | mfu: 12.27 | total time: 372.32m
step 01623/21400 (7.58%) | loss: 3.354062 | grad norm: 0.1391 | lrm: 1.00 | dt: 13889.44ms | tok/sec: 37,747 | mfu: 12.13 | total time: 372.55m
step 01624/21400 (7.59%) | loss: 3.362031 | grad norm: 0.1217 | lrm: 1.00 | dt: 13707.86ms | tok/sec: 38,247 | mfu: 12.29 | total time: 372.78m
step 01625/21400 (7.59%) | loss: 3.389619 | grad norm: 0.1189 | lrm: 1.00 | dt: 13883.38ms | tok/sec: 37,763 | mfu: 12.13 | total time: 373.01m
step 01626/21400 (7.60%) | loss: 3.330658 | grad norm: 0.1353 | lrm: 1.00 | dt: 13788.15ms | tok/sec: 38,024 | mfu: 12.22 | total time: 373.24m
step 01627/21400 (7.60%) | loss: 3.359881 | grad norm: 0.1392 | lrm: 1.00 | dt: 13849.35ms | tok/sec: 37,856 | mfu: 12.16 | total time: 373.47m
step 01628/21400 (7.61%) | loss: 3.354074 | grad norm: 0.1112 | lrm: 1.00 | dt: 13760.76ms | tok/sec: 38,100 | mfu: 12.24 | total time: 373.70m
step 01629/21400 (7.61%) | loss: 3.352770 | grad norm: 0.1117 | lrm: 1.00 | dt: 13846.08ms | tok/sec: 37,865 | mfu: 12.16 | total time: 373.93m
step 01630/21400 (7.62%) | loss: 3.371733 | grad norm: 0.1146 | lrm: 1.00 | dt: 13911.03ms | tok/sec: 37,688 | mfu: 12.11 | total time: 374.16m
step 01631/21400 (7.62%) | loss: 3.358616 | grad norm: 0.1157 | lrm: 1.00 | dt: 13811.39ms | tok/sec: 37,960 | mfu: 12.19 | total time: 374.39m
step 01632/21400 (7.63%) | loss: 3.365529 | grad norm: 0.1246 | lrm: 1.00 | dt: 13678.10ms | tok/sec: 38,330 | mfu: 12.31 | total time: 374.62m
step 01633/21400 (7.63%) | loss: 3.347897 | grad norm: 0.1295 | lrm: 1.00 | dt: 13812.62ms | tok/sec: 37,957 | mfu: 12.19 | total time: 374.85m
step 01634/21400 (7.64%) | loss: 3.346560 | grad norm: 0.1211 | lrm: 1.00 | dt: 13904.89ms | tok/sec: 37,705 | mfu: 12.11 | total time: 375.08m
step 01635/21400 (7.64%) | loss: 3.363814 | grad norm: 0.1095 | lrm: 1.00 | dt: 13706.16ms | tok/sec: 38,252 | mfu: 12.29 | total time: 375.31m
step 01636/21400 (7.64%) | loss: 3.346161 | grad norm: 0.1220 | lrm: 1.00 | dt: 13881.15ms | tok/sec: 37,769 | mfu: 12.13 | total time: 375.54m
step 01637/21400 (7.65%) | loss: 3.363640 | grad norm: 0.1271 | lrm: 1.00 | dt: 13726.61ms | tok/sec: 38,195 | mfu: 12.27 | total time: 375.77m
step 01638/21400 (7.65%) | loss: 3.336359 | grad norm: 0.1239 | lrm: 1.00 | dt: 13870.76ms | tok/sec: 37,798 | mfu: 12.14 | total time: 376.00m
step 01639/21400 (7.66%) | loss: 3.321202 | grad norm: 0.1176 | lrm: 1.00 | dt: 13803.12ms | tok/sec: 37,983 | mfu: 12.20 | total time: 376.23m
step 01640/21400 (7.66%) | loss: 3.270013 | grad norm: 0.1215 | lrm: 1.00 | dt: 13823.41ms | tok/sec: 37,927 | mfu: 12.18 | total time: 376.46m
step 01641/21400 (7.67%) | loss: 3.291249 | grad norm: 0.1208 | lrm: 1.00 | dt: 13748.88ms | tok/sec: 38,133 | mfu: 12.25 | total time: 376.69m
step 01642/21400 (7.67%) | loss: 3.275067 | grad norm: 0.1694 | lrm: 1.00 | dt: 13830.53ms | tok/sec: 37,908 | mfu: 12.18 | total time: 376.92m
step 01643/21400 (7.68%) | loss: 3.366601 | grad norm: 0.1457 | lrm: 1.00 | dt: 13885.42ms | tok/sec: 37,758 | mfu: 12.13 | total time: 377.15m
step 01644/21400 (7.68%) | loss: 3.334619 | grad norm: 0.1127 | lrm: 1.00 | dt: 13709.31ms | tok/sec: 38,243 | mfu: 12.29 | total time: 377.38m
step 01645/21400 (7.69%) | loss: 3.309196 | grad norm: 0.1153 | lrm: 1.00 | dt: 13895.70ms | tok/sec: 37,730 | mfu: 12.12 | total time: 377.61m
step 01646/21400 (7.69%) | loss: 3.287230 | grad norm: 0.1139 | lrm: 1.00 | dt: 13689.01ms | tok/sec: 38,299 | mfu: 12.30 | total time: 377.84m
step 01647/21400 (7.70%) | loss: 3.305598 | grad norm: 0.1037 | lrm: 1.00 | dt: 13896.26ms | tok/sec: 37,728 | mfu: 12.12 | total time: 378.07m
step 01648/21400 (7.70%) | loss: 3.320333 | grad norm: 0.1120 | lrm: 1.00 | dt: 13770.93ms | tok/sec: 38,072 | mfu: 12.23 | total time: 378.30m
step 01649/21400 (7.71%) | loss: 3.313755 | grad norm: 0.1201 | lrm: 1.00 | dt: 13742.42ms | tok/sec: 38,151 | mfu: 12.26 | total time: 378.53m
step 01650/21400 (7.71%) | loss: 3.308801 | grad norm: 0.1190 | lrm: 1.00 | dt: 13826.03ms | tok/sec: 37,920 | mfu: 12.18 | total time: 378.76m
step 01651/21400 (7.71%) | loss: 3.278183 | grad norm: 0.1155 | lrm: 1.00 | dt: 13855.27ms | tok/sec: 37,840 | mfu: 12.16 | total time: 378.99m
step 01652/21400 (7.72%) | loss: 3.277973 | grad norm: 0.1248 | lrm: 1.00 | dt: 13769.51ms | tok/sec: 38,076 | mfu: 12.23 | total time: 379.22m
step 01653/21400 (7.72%) | loss: 3.264077 | grad norm: 0.1551 | lrm: 1.00 | dt: 13830.80ms | tok/sec: 37,907 | mfu: 12.18 | total time: 379.45m
step 01654/21400 (7.73%) | loss: 3.259754 | grad norm: 0.2522 | lrm: 1.00 | dt: 13795.39ms | tok/sec: 38,004 | mfu: 12.21 | total time: 379.68m
step 01655/21400 (7.73%) | loss: 3.244202 | grad norm: 0.1285 | lrm: 1.00 | dt: 13796.20ms | tok/sec: 38,002 | mfu: 12.21 | total time: 379.91m
step 01656/21400 (7.74%) | loss: 3.263780 | grad norm: 0.1386 | lrm: 1.00 | dt: 13908.26ms | tok/sec: 37,696 | mfu: 12.11 | total time: 380.15m
step 01657/21400 (7.74%) | loss: 3.305439 | grad norm: 0.1207 | lrm: 1.00 | dt: 13720.48ms | tok/sec: 38,212 | mfu: 12.28 | total time: 380.37m
step 01658/21400 (7.75%) | loss: 3.299366 | grad norm: 0.1272 | lrm: 1.00 | dt: 14290.09ms | tok/sec: 36,688 | mfu: 11.79 | total time: 380.61m
step 01659/21400 (7.75%) | loss: 3.225850 | grad norm: 0.1219 | lrm: 1.00 | dt: 13744.50ms | tok/sec: 38,145 | mfu: 12.25 | total time: 380.84m
step 01660/21400 (7.76%) | loss: 3.265583 | grad norm: 0.1185 | lrm: 1.00 | dt: 13861.53ms | tok/sec: 37,823 | mfu: 12.15 | total time: 381.07m
step 01661/21400 (7.76%) | loss: 3.249837 | grad norm: 0.1259 | lrm: 1.00 | dt: 13746.16ms | tok/sec: 38,140 | mfu: 12.25 | total time: 381.30m
step 01662/21400 (7.77%) | loss: 3.230082 | grad norm: 0.1336 | lrm: 1.00 | dt: 13860.27ms | tok/sec: 37,826 | mfu: 12.15 | total time: 381.53m
step 01663/21400 (7.77%) | loss: 3.229498 | grad norm: 0.1332 | lrm: 1.00 | dt: 13754.97ms | tok/sec: 38,116 | mfu: 12.24 | total time: 381.76m
step 01664/21400 (7.78%) | loss: 3.266168 | grad norm: 0.1222 | lrm: 1.00 | dt: 13857.03ms | tok/sec: 37,835 | mfu: 12.15 | total time: 381.99m
step 01665/21400 (7.78%) | loss: 3.263214 | grad norm: 0.1128 | lrm: 1.00 | dt: 13777.84ms | tok/sec: 38,052 | mfu: 12.22 | total time: 382.22m
step 01666/21400 (7.79%) | loss: 3.281329 | grad norm: 0.1231 | lrm: 1.00 | dt: 13834.06ms | tok/sec: 37,898 | mfu: 12.17 | total time: 382.45m
step 01667/21400 (7.79%) | loss: 3.254376 | grad norm: 0.1261 | lrm: 1.00 | dt: 13804.76ms | tok/sec: 37,978 | mfu: 12.20 | total time: 382.68m
step 01668/21400 (7.79%) | loss: 3.248711 | grad norm: 0.1201 | lrm: 1.00 | dt: 13822.98ms | tok/sec: 37,928 | mfu: 12.18 | total time: 382.91m
step 01669/21400 (7.80%) | loss: 3.244214 | grad norm: 0.1205 | lrm: 1.00 | dt: 13875.32ms | tok/sec: 37,785 | mfu: 12.14 | total time: 383.15m
step 01670/21400 (7.80%) | loss: 3.207677 | grad norm: 0.1261 | lrm: 1.00 | dt: 13727.38ms | tok/sec: 38,192 | mfu: 12.27 | total time: 383.37m
step 01671/21400 (7.81%) | loss: 3.226021 | grad norm: 0.1216 | lrm: 1.00 | dt: 13869.39ms | tok/sec: 37,801 | mfu: 12.14 | total time: 383.61m
step 01672/21400 (7.81%) | loss: 3.251202 | grad norm: 0.1199 | lrm: 1.00 | dt: 13729.92ms | tok/sec: 38,185 | mfu: 12.27 | total time: 383.83m
step 01673/21400 (7.82%) | loss: 3.201973 | grad norm: 0.1259 | lrm: 1.00 | dt: 13876.22ms | tok/sec: 37,783 | mfu: 12.14 | total time: 384.07m
step 01674/21400 (7.82%) | loss: 3.210144 | grad norm: 0.1238 | lrm: 1.00 | dt: 13744.28ms | tok/sec: 38,145 | mfu: 12.25 | total time: 384.29m
step 01675/21400 (7.83%) | loss: 3.231384 | grad norm: 0.1301 | lrm: 1.00 | dt: 13862.59ms | tok/sec: 37,820 | mfu: 12.15 | total time: 384.53m
step 01676/21400 (7.83%) | loss: 3.226759 | grad norm: 0.1348 | lrm: 1.00 | dt: 13767.48ms | tok/sec: 38,081 | mfu: 12.23 | total time: 384.75m
step 01677/21400 (7.84%) | loss: 3.330440 | grad norm: 0.1284 | lrm: 1.00 | dt: 13848.08ms | tok/sec: 37,859 | mfu: 12.16 | total time: 384.99m
step 01678/21400 (7.84%) | loss: 3.340321 | grad norm: 0.1238 | lrm: 1.00 | dt: 13771.80ms | tok/sec: 38,069 | mfu: 12.23 | total time: 385.22m
step 01679/21400 (7.85%) | loss: 3.321022 | grad norm: 0.1209 | lrm: 1.00 | dt: 13832.90ms | tok/sec: 37,901 | mfu: 12.18 | total time: 385.45m
step 01680/21400 (7.85%) | loss: 3.317469 | grad norm: 0.1320 | lrm: 1.00 | dt: 13791.50ms | tok/sec: 38,015 | mfu: 12.21 | total time: 385.68m
step 01681/21400 (7.86%) | loss: 3.290312 | grad norm: 0.1336 | lrm: 1.00 | dt: 13806.45ms | tok/sec: 37,974 | mfu: 12.20 | total time: 385.91m
step 01682/21400 (7.86%) | loss: 3.296221 | grad norm: 0.1123 | lrm: 1.00 | dt: 13882.28ms | tok/sec: 37,766 | mfu: 12.13 | total time: 386.14m
step 01683/21400 (7.86%) | loss: 3.339674 | grad norm: 0.1097 | lrm: 1.00 | dt: 13742.47ms | tok/sec: 38,150 | mfu: 12.26 | total time: 386.37m
step 01684/21400 (7.87%) | loss: 3.346767 | grad norm: 0.1265 | lrm: 1.00 | dt: 13876.21ms | tok/sec: 37,783 | mfu: 12.14 | total time: 386.60m
step 01685/21400 (7.87%) | loss: 3.313251 | grad norm: 0.1268 | lrm: 1.00 | dt: 13741.77ms | tok/sec: 38,152 | mfu: 12.26 | total time: 386.83m
step 01686/21400 (7.88%) | loss: 3.312702 | grad norm: 0.1229 | lrm: 1.00 | dt: 13870.46ms | tok/sec: 37,798 | mfu: 12.14 | total time: 387.06m
step 01687/21400 (7.88%) | loss: 3.330626 | grad norm: 0.1336 | lrm: 1.00 | dt: 13752.07ms | tok/sec: 38,124 | mfu: 12.25 | total time: 387.29m
step 01688/21400 (7.89%) | loss: 3.348442 | grad norm: 0.1322 | lrm: 1.00 | dt: 13855.49ms | tok/sec: 37,839 | mfu: 12.16 | total time: 387.52m
step 01689/21400 (7.89%) | loss: 3.353299 | grad norm: 0.1174 | lrm: 1.00 | dt: 13759.61ms | tok/sec: 38,103 | mfu: 12.24 | total time: 387.75m
step 01690/21400 (7.90%) | loss: 3.382513 | grad norm: 0.1195 | lrm: 1.00 | dt: 13845.13ms | tok/sec: 37,868 | mfu: 12.17 | total time: 387.98m
step 01691/21400 (7.90%) | loss: 3.362019 | grad norm: 0.1266 | lrm: 1.00 | dt: 13782.89ms | tok/sec: 38,039 | mfu: 12.22 | total time: 388.21m
step 01692/21400 (7.91%) | loss: 3.326669 | grad norm: 0.1333 | lrm: 1.00 | dt: 13835.41ms | tok/sec: 37,894 | mfu: 12.17 | total time: 388.44m
step 01693/21400 (7.91%) | loss: 3.309154 | grad norm: 0.1344 | lrm: 1.00 | dt: 13801.32ms | tok/sec: 37,988 | mfu: 12.20 | total time: 388.67m
step 01694/21400 (7.92%) | loss: 3.309023 | grad norm: 0.1236 | lrm: 1.00 | dt: 13793.46ms | tok/sec: 38,009 | mfu: 12.21 | total time: 388.90m
step 01695/21400 (7.92%) | loss: 3.287141 | grad norm: 0.1227 | lrm: 1.00 | dt: 13878.05ms | tok/sec: 37,778 | mfu: 12.14 | total time: 389.13m
step 01696/21400 (7.93%) | loss: 3.312692 | grad norm: 0.1139 | lrm: 1.00 | dt: 13728.12ms | tok/sec: 38,190 | mfu: 12.27 | total time: 389.36m
step 01697/21400 (7.93%) | loss: 3.242134 | grad norm: 0.1259 | lrm: 1.00 | dt: 13880.18ms | tok/sec: 37,772 | mfu: 12.13 | total time: 389.59m
step 01698/21400 (7.93%) | loss: 3.246377 | grad norm: 0.1227 | lrm: 1.00 | dt: 13741.13ms | tok/sec: 38,154 | mfu: 12.26 | total time: 389.82m
step 01699/21400 (7.94%) | loss: 3.262338 | grad norm: 0.1254 | lrm: 1.00 | dt: 14325.17ms | tok/sec: 36,599 | mfu: 11.76 | total time: 390.06m
step 01700/21400 (7.94%) | loss: 3.264809 | grad norm: 0.1223 | lrm: 1.00 | dt: 13746.48ms | tok/sec: 38,139 | mfu: 12.25 | total time: 390.29m
step 01701/21400 (7.95%) | loss: 3.276797 | grad norm: 0.1280 | lrm: 1.00 | dt: 13863.77ms | tok/sec: 37,817 | mfu: 12.15 | total time: 390.52m
step 01702/21400 (7.95%) | loss: 3.271307 | grad norm: 0.1269 | lrm: 1.00 | dt: 13753.68ms | tok/sec: 38,119 | mfu: 12.25 | total time: 390.75m
step 01703/21400 (7.96%) | loss: 3.293511 | grad norm: 0.1308 | lrm: 1.00 | dt: 13852.22ms | tok/sec: 37,848 | mfu: 12.16 | total time: 390.98m
step 01704/21400 (7.96%) | loss: 3.318129 | grad norm: 0.1281 | lrm: 1.00 | dt: 13771.55ms | tok/sec: 38,070 | mfu: 12.23 | total time: 391.21m
step 01705/21400 (7.97%) | loss: 3.299957 | grad norm: 0.1250 | lrm: 1.00 | dt: 13844.52ms | tok/sec: 37,869 | mfu: 12.17 | total time: 391.44m
step 01706/21400 (7.97%) | loss: 3.296844 | grad norm: 0.1471 | lrm: 1.00 | dt: 13785.89ms | tok/sec: 38,030 | mfu: 12.22 | total time: 391.67m
step 01707/21400 (7.98%) | loss: 3.315253 | grad norm: 0.1092 | lrm: 1.00 | dt: 13814.22ms | tok/sec: 37,952 | mfu: 12.19 | total time: 391.90m
step 01708/21400 (7.98%) | loss: 3.334732 | grad norm: 0.1036 | lrm: 1.00 | dt: 13882.43ms | tok/sec: 37,766 | mfu: 12.13 | total time: 392.13m
step 01709/21400 (7.99%) | loss: 3.299771 | grad norm: 0.1139 | lrm: 1.00 | dt: 13727.54ms | tok/sec: 38,192 | mfu: 12.27 | total time: 392.36m
step 01710/21400 (7.99%) | loss: 3.315442 | grad norm: 0.1110 | lrm: 1.00 | dt: 13872.53ms | tok/sec: 37,793 | mfu: 12.14 | total time: 392.59m
step 01711/21400 (8.00%) | loss: 3.325606 | grad norm: 0.1084 | lrm: 1.00 | dt: 13730.47ms | tok/sec: 38,184 | mfu: 12.27 | total time: 392.82m
step 01712/21400 (8.00%) | loss: 3.349201 | grad norm: 0.1315 | lrm: 1.00 | dt: 13878.21ms | tok/sec: 37,777 | mfu: 12.14 | total time: 393.05m
step 01713/21400 (8.00%) | loss: 3.327766 | grad norm: 0.1336 | lrm: 1.00 | dt: 13752.73ms | tok/sec: 38,122 | mfu: 12.25 | total time: 393.28m
step 01714/21400 (8.01%) | loss: 3.350814 | grad norm: 0.1255 | lrm: 1.00 | dt: 13862.07ms | tok/sec: 37,821 | mfu: 12.15 | total time: 393.51m
step 01715/21400 (8.01%) | loss: 3.317439 | grad norm: 0.1129 | lrm: 1.00 | dt: 13750.44ms | tok/sec: 38,128 | mfu: 12.25 | total time: 393.74m
step 01716/21400 (8.02%) | loss: 3.336746 | grad norm: 0.1135 | lrm: 1.00 | dt: 13847.20ms | tok/sec: 37,862 | mfu: 12.16 | total time: 393.97m
step 01717/21400 (8.02%) | loss: 3.261553 | grad norm: 0.1197 | lrm: 1.00 | dt: 13770.17ms | tok/sec: 38,074 | mfu: 12.23 | total time: 394.20m
step 01718/21400 (8.03%) | loss: 3.242021 | grad norm: 0.1245 | lrm: 1.00 | dt: 13837.69ms | tok/sec: 37,888 | mfu: 12.17 | total time: 394.43m
step 01719/21400 (8.03%) | loss: 3.233153 | grad norm: 0.1319 | lrm: 1.00 | dt: 13795.39ms | tok/sec: 38,004 | mfu: 12.21 | total time: 394.66m
step 01720/21400 (8.04%) | loss: 3.205643 | grad norm: 0.1367 | lrm: 1.00 | dt: 13815.45ms | tok/sec: 37,949 | mfu: 12.19 | total time: 394.89m
step 01721/21400 (8.04%) | loss: 3.186658 | grad norm: 0.1240 | lrm: 1.00 | dt: 13884.36ms | tok/sec: 37,761 | mfu: 12.13 | total time: 395.12m
step 01722/21400 (8.05%) | loss: 3.177593 | grad norm: 0.1162 | lrm: 1.00 | dt: 13726.58ms | tok/sec: 38,195 | mfu: 12.27 | total time: 395.35m
step 01723/21400 (8.05%) | loss: 3.175405 | grad norm: 0.1166 | lrm: 1.00 | dt: 13876.40ms | tok/sec: 37,782 | mfu: 12.14 | total time: 395.58m
step 01724/21400 (8.06%) | loss: 3.219036 | grad norm: 0.1104 | lrm: 1.00 | dt: 13740.37ms | tok/sec: 38,156 | mfu: 12.26 | total time: 395.81m
step 01725/21400 (8.06%) | loss: 3.220315 | grad norm: 0.1139 | lrm: 1.00 | dt: 13871.77ms | tok/sec: 37,795 | mfu: 12.14 | total time: 396.04m
step 01726/21400 (8.07%) | loss: 3.201954 | grad norm: 0.1205 | lrm: 1.00 | dt: 13747.06ms | tok/sec: 38,138 | mfu: 12.25 | total time: 396.27m
step 01727/21400 (8.07%) | loss: 3.214595 | grad norm: 0.1286 | lrm: 1.00 | dt: 13865.85ms | tok/sec: 37,811 | mfu: 12.15 | total time: 396.50m
step 01728/21400 (8.07%) | loss: 3.197284 | grad norm: 0.1292 | lrm: 1.00 | dt: 13763.72ms | tok/sec: 38,092 | mfu: 12.24 | total time: 396.73m
step 01729/21400 (8.08%) | loss: 3.214287 | grad norm: 0.1439 | lrm: 1.00 | dt: 13852.84ms | tok/sec: 37,846 | mfu: 12.16 | total time: 396.96m
step 01730/21400 (8.08%) | loss: 3.207988 | grad norm: 0.1300 | lrm: 1.00 | dt: 13759.70ms | tok/sec: 38,103 | mfu: 12.24 | total time: 397.19m
step 01731/21400 (8.09%) | loss: 3.216826 | grad norm: 0.1208 | lrm: 1.00 | dt: 13851.62ms | tok/sec: 37,850 | mfu: 12.16 | total time: 397.42m
step 01732/21400 (8.09%) | loss: 3.268323 | grad norm: 0.1173 | lrm: 1.00 | dt: 13808.94ms | tok/sec: 37,967 | mfu: 12.20 | total time: 397.65m
step 01733/21400 (8.10%) | loss: 3.260749 | grad norm: 0.1121 | lrm: 1.00 | dt: 13823.97ms | tok/sec: 37,926 | mfu: 12.18 | total time: 397.88m
step 01734/21400 (8.10%) | loss: 3.301001 | grad norm: 0.1248 | lrm: 1.00 | dt: 13885.96ms | tok/sec: 37,756 | mfu: 12.13 | total time: 398.11m
step 01735/21400 (8.11%) | loss: 3.316708 | grad norm: 0.1332 | lrm: 1.00 | dt: 13730.59ms | tok/sec: 38,183 | mfu: 12.27 | total time: 398.34m
step 01736/21400 (8.11%) | loss: 3.326893 | grad norm: 0.1402 | lrm: 1.00 | dt: 13880.31ms | tok/sec: 37,772 | mfu: 12.13 | total time: 398.57m
step 01737/21400 (8.12%) | loss: 3.322590 | grad norm: 0.1366 | lrm: 1.00 | dt: 13734.26ms | tok/sec: 38,173 | mfu: 12.26 | total time: 398.80m
step 01738/21400 (8.12%) | loss: 3.330084 | grad norm: 0.1202 | lrm: 1.00 | dt: 13874.32ms | tok/sec: 37,788 | mfu: 12.14 | total time: 399.03m
step 01739/21400 (8.13%) | loss: 3.326817 | grad norm: 0.1243 | lrm: 1.00 | dt: 14171.10ms | tok/sec: 36,996 | mfu: 11.89 | total time: 399.27m
step 01740/21400 (8.13%) | loss: 3.269505 | grad norm: 0.1274 | lrm: 1.00 | dt: 13863.80ms | tok/sec: 37,817 | mfu: 12.15 | total time: 399.50m
step 01741/21400 (8.14%) | loss: 3.243784 | grad norm: 0.1272 | lrm: 1.00 | dt: 13768.87ms | tok/sec: 38,077 | mfu: 12.23 | total time: 399.73m
step 01742/21400 (8.14%) | loss: 3.241444 | grad norm: 0.1319 | lrm: 1.00 | dt: 13852.90ms | tok/sec: 37,846 | mfu: 12.16 | total time: 399.96m
step 01743/21400 (8.14%) | loss: 3.262981 | grad norm: 0.1274 | lrm: 1.00 | dt: 13771.41ms | tok/sec: 38,070 | mfu: 12.23 | total time: 400.19m
step 01744/21400 (8.15%) | loss: 3.289715 | grad norm: 0.1315 | lrm: 1.00 | dt: 13851.94ms | tok/sec: 37,849 | mfu: 12.16 | total time: 400.42m
step 01745/21400 (8.15%) | loss: 3.260855 | grad norm: 0.1372 | lrm: 1.00 | dt: 13796.03ms | tok/sec: 38,002 | mfu: 12.21 | total time: 400.65m
step 01746/21400 (8.16%) | loss: 3.269866 | grad norm: 0.1338 | lrm: 1.00 | dt: 13820.17ms | tok/sec: 37,936 | mfu: 12.19 | total time: 400.88m
step 01747/21400 (8.16%) | loss: 3.293037 | grad norm: 0.1194 | lrm: 1.00 | dt: 13804.06ms | tok/sec: 37,980 | mfu: 12.20 | total time: 401.11m
step 01748/21400 (8.17%) | loss: 3.296161 | grad norm: 0.1180 | lrm: 1.00 | dt: 13779.84ms | tok/sec: 38,047 | mfu: 12.22 | total time: 401.34m
step 01749/21400 (8.17%) | loss: 3.314939 | grad norm: 0.1180 | lrm: 1.00 | dt: 13882.34ms | tok/sec: 37,766 | mfu: 12.13 | total time: 401.57m
Step 01750 | Validation bpb: 0.9667
step 01750/21400 (8.18%) | loss: 3.363768 | grad norm: 0.1596 | lrm: 1.00 | dt: 13746.71ms | tok/sec: 38,139 | mfu: 12.25 | total time: 401.80m
step 01751/21400 (8.18%) | loss: 3.322248 | grad norm: 0.1181 | lrm: 1.00 | dt: 13856.49ms | tok/sec: 37,836 | mfu: 12.16 | total time: 402.03m
step 01752/21400 (8.19%) | loss: 3.306123 | grad norm: 0.1098 | lrm: 1.00 | dt: 13767.35ms | tok/sec: 38,081 | mfu: 12.23 | total time: 402.26m
step 01753/21400 (8.19%) | loss: 3.291627 | grad norm: 0.1096 | lrm: 1.00 | dt: 13846.74ms | tok/sec: 37,863 | mfu: 12.16 | total time: 402.49m
step 01754/21400 (8.20%) | loss: 3.298866 | grad norm: 0.1166 | lrm: 1.00 | dt: 13776.01ms | tok/sec: 38,058 | mfu: 12.23 | total time: 402.72m
step 01755/21400 (8.20%) | loss: 3.292868 | grad norm: 0.1315 | lrm: 1.00 | dt: 13847.53ms | tok/sec: 37,861 | mfu: 12.16 | total time: 402.95m
step 01756/21400 (8.21%) | loss: 3.304901 | grad norm: 0.1381 | lrm: 1.00 | dt: 13798.31ms | tok/sec: 37,996 | mfu: 12.21 | total time: 403.18m
step 01757/21400 (8.21%) | loss: 3.280105 | grad norm: 0.1357 | lrm: 1.00 | dt: 13814.55ms | tok/sec: 37,951 | mfu: 12.19 | total time: 403.41m
step 01758/21400 (8.21%) | loss: 3.276922 | grad norm: 0.1372 | lrm: 1.00 | dt: 13878.15ms | tok/sec: 37,777 | mfu: 12.14 | total time: 403.65m
step 01759/21400 (8.22%) | loss: 3.275112 | grad norm: 0.1255 | lrm: 1.00 | dt: 13732.45ms | tok/sec: 38,178 | mfu: 12.27 | total time: 403.87m
step 01760/21400 (8.22%) | loss: 3.266730 | grad norm: 0.1246 | lrm: 1.00 | dt: 13864.66ms | tok/sec: 37,814 | mfu: 12.15 | total time: 404.11m
step 01761/21400 (8.23%) | loss: 3.271851 | grad norm: 0.1254 | lrm: 1.00 | dt: 13737.86ms | tok/sec: 38,163 | mfu: 12.26 | total time: 404.33m
step 01762/21400 (8.23%) | loss: 3.248180 | grad norm: 0.1316 | lrm: 1.00 | dt: 13865.56ms | tok/sec: 37,812 | mfu: 12.15 | total time: 404.57m
step 01763/21400 (8.24%) | loss: 3.255702 | grad norm: 0.1360 | lrm: 1.00 | dt: 13760.86ms | tok/sec: 38,099 | mfu: 12.24 | total time: 404.80m
step 01764/21400 (8.24%) | loss: 3.283823 | grad norm: 0.1202 | lrm: 1.00 | dt: 13860.35ms | tok/sec: 37,826 | mfu: 12.15 | total time: 405.03m
step 01765/21400 (8.25%) | loss: 3.255185 | grad norm: 0.1123 | lrm: 1.00 | dt: 13755.91ms | tok/sec: 38,113 | mfu: 12.24 | total time: 405.26m
step 01766/21400 (8.25%) | loss: 3.228041 | grad norm: 0.1097 | lrm: 1.00 | dt: 13853.49ms | tok/sec: 37,845 | mfu: 12.16 | total time: 405.49m
step 01767/21400 (8.26%) | loss: 3.225892 | grad norm: 0.1177 | lrm: 1.00 | dt: 13778.25ms | tok/sec: 38,051 | mfu: 12.22 | total time: 405.72m
step 01768/21400 (8.26%) | loss: 3.224554 | grad norm: 0.1357 | lrm: 1.00 | dt: 13833.79ms | tok/sec: 37,899 | mfu: 12.18 | total time: 405.95m
step 01769/21400 (8.27%) | loss: 3.251565 | grad norm: 0.1246 | lrm: 1.00 | dt: 13811.42ms | tok/sec: 37,960 | mfu: 12.19 | total time: 406.18m
step 01770/21400 (8.27%) | loss: 3.286811 | grad norm: 0.1182 | lrm: 1.00 | dt: 13809.74ms | tok/sec: 37,965 | mfu: 12.20 | total time: 406.41m
step 01771/21400 (8.28%) | loss: 3.237536 | grad norm: 0.1365 | lrm: 1.00 | dt: 13866.60ms | tok/sec: 37,809 | mfu: 12.15 | total time: 406.64m
step 01772/21400 (8.28%) | loss: 3.229581 | grad norm: 0.1269 | lrm: 1.00 | dt: 13732.87ms | tok/sec: 38,177 | mfu: 12.26 | total time: 406.87m
step 01773/21400 (8.29%) | loss: 3.245738 | grad norm: 0.1219 | lrm: 1.00 | dt: 13876.72ms | tok/sec: 37,781 | mfu: 12.14 | total time: 407.10m
step 01774/21400 (8.29%) | loss: 3.293287 | grad norm: 0.1221 | lrm: 1.00 | dt: 13759.37ms | tok/sec: 38,104 | mfu: 12.24 | total time: 407.33m
step 01775/21400 (8.29%) | loss: 3.298055 | grad norm: 0.1196 | lrm: 1.00 | dt: 13862.39ms | tok/sec: 37,820 | mfu: 12.15 | total time: 407.56m
step 01776/21400 (8.30%) | loss: 3.299726 | grad norm: 0.1173 | lrm: 1.00 | dt: 13756.73ms | tok/sec: 38,111 | mfu: 12.24 | total time: 407.79m
step 01777/21400 (8.30%) | loss: 3.326810 | grad norm: 0.1195 | lrm: 1.00 | dt: 13851.46ms | tok/sec: 37,850 | mfu: 12.16 | total time: 408.02m
step 01778/21400 (8.31%) | loss: 3.316667 | grad norm: 0.1177 | lrm: 1.00 | dt: 14218.03ms | tok/sec: 36,874 | mfu: 11.85 | total time: 408.26m
step 01779/21400 (8.31%) | loss: 3.257267 | grad norm: 0.1204 | lrm: 1.00 | dt: 13842.88ms | tok/sec: 37,874 | mfu: 12.17 | total time: 408.49m
step 01780/21400 (8.32%) | loss: 3.297043 | grad norm: 0.1290 | lrm: 1.00 | dt: 13770.85ms | tok/sec: 38,072 | mfu: 12.23 | total time: 408.72m
step 01781/21400 (8.32%) | loss: 3.303502 | grad norm: 0.1249 | lrm: 1.00 | dt: 13827.11ms | tok/sec: 37,917 | mfu: 12.18 | total time: 408.95m
step 01782/21400 (8.33%) | loss: 3.288032 | grad norm: 0.1353 | lrm: 1.00 | dt: 13795.04ms | tok/sec: 38,005 | mfu: 12.21 | total time: 409.18m
step 01783/21400 (8.33%) | loss: 3.283789 | grad norm: 0.1325 | lrm: 1.00 | dt: 13798.96ms | tok/sec: 37,994 | mfu: 12.21 | total time: 409.41m
step 01784/21400 (8.34%) | loss: 3.292697 | grad norm: 0.1183 | lrm: 1.00 | dt: 13888.96ms | tok/sec: 37,748 | mfu: 12.13 | total time: 409.64m
step 01785/21400 (8.34%) | loss: 3.319217 | grad norm: 0.1113 | lrm: 1.00 | dt: 13745.19ms | tok/sec: 38,143 | mfu: 12.25 | total time: 409.87m
step 01786/21400 (8.35%) | loss: 3.348126 | grad norm: 0.1175 | lrm: 1.00 | dt: 13864.43ms | tok/sec: 37,815 | mfu: 12.15 | total time: 410.10m
step 01787/21400 (8.35%) | loss: 3.321431 | grad norm: 0.1139 | lrm: 1.00 | dt: 13750.06ms | tok/sec: 38,129 | mfu: 12.25 | total time: 410.33m
step 01788/21400 (8.36%) | loss: 3.327001 | grad norm: 0.1100 | lrm: 1.00 | dt: 13860.25ms | tok/sec: 37,826 | mfu: 12.15 | total time: 410.56m
step 01789/21400 (8.36%) | loss: 3.307768 | grad norm: 0.1160 | lrm: 1.00 | dt: 13745.94ms | tok/sec: 38,141 | mfu: 12.25 | total time: 410.79m
step 01790/21400 (8.36%) | loss: 3.300312 | grad norm: 0.1161 | lrm: 1.00 | dt: 13871.34ms | tok/sec: 37,796 | mfu: 12.14 | total time: 411.02m
step 01791/21400 (8.37%) | loss: 3.279010 | grad norm: 0.1176 | lrm: 1.00 | dt: 13758.65ms | tok/sec: 38,106 | mfu: 12.24 | total time: 411.25m
step 01792/21400 (8.37%) | loss: 3.282543 | grad norm: 0.1260 | lrm: 1.00 | dt: 13866.49ms | tok/sec: 37,809 | mfu: 12.15 | total time: 411.48m
step 01793/21400 (8.38%) | loss: 3.253041 | grad norm: 0.1255 | lrm: 1.00 | dt: 13783.10ms | tok/sec: 38,038 | mfu: 12.22 | total time: 411.71m
step 01794/21400 (8.38%) | loss: 3.209316 | grad norm: 0.1295 | lrm: 1.00 | dt: 13839.69ms | tok/sec: 37,882 | mfu: 12.17 | total time: 411.94m
step 01795/21400 (8.39%) | loss: 3.207283 | grad norm: 0.1208 | lrm: 1.00 | dt: 13791.38ms | tok/sec: 38,015 | mfu: 12.21 | total time: 412.17m
step 01796/21400 (8.39%) | loss: 3.199980 | grad norm: 0.1139 | lrm: 1.00 | dt: 13832.24ms | tok/sec: 37,903 | mfu: 12.18 | total time: 412.40m
step 01797/21400 (8.40%) | loss: 3.212639 | grad norm: 0.1414 | lrm: 1.00 | dt: 13875.93ms | tok/sec: 37,783 | mfu: 12.14 | total time: 412.63m
step 01798/21400 (8.40%) | loss: 3.212220 | grad norm: 0.1339 | lrm: 1.00 | dt: 13733.02ms | tok/sec: 38,177 | mfu: 12.26 | total time: 412.86m
step 01799/21400 (8.41%) | loss: 3.190770 | grad norm: 0.1247 | lrm: 1.00 | dt: 13869.89ms | tok/sec: 37,800 | mfu: 12.14 | total time: 413.09m
step 01800/21400 (8.41%) | loss: 3.212208 | grad norm: 0.1277 | lrm: 1.00 | dt: 13745.03ms | tok/sec: 38,143 | mfu: 12.25 | total time: 413.32m
step 01801/21400 (8.42%) | loss: 3.176359 | grad norm: 0.1138 | lrm: 1.00 | dt: 13863.20ms | tok/sec: 37,818 | mfu: 12.15 | total time: 413.55m
step 01802/21400 (8.42%) | loss: 3.176093 | grad norm: 0.1189 | lrm: 1.00 | dt: 13774.28ms | tok/sec: 38,062 | mfu: 12.23 | total time: 413.78m
step 01803/21400 (8.43%) | loss: 3.239788 | grad norm: 0.1228 | lrm: 1.00 | dt: 13860.34ms | tok/sec: 37,826 | mfu: 12.15 | total time: 414.01m
step 01804/21400 (8.43%) | loss: 3.236267 | grad norm: 0.1259 | lrm: 1.00 | dt: 13771.12ms | tok/sec: 38,071 | mfu: 12.23 | total time: 414.24m
step 01805/21400 (8.43%) | loss: 3.253331 | grad norm: 0.1160 | lrm: 1.00 | dt: 13846.80ms | tok/sec: 37,863 | mfu: 12.16 | total time: 414.47m
step 01806/21400 (8.44%) | loss: 3.256777 | grad norm: 0.1267 | lrm: 1.00 | dt: 13773.99ms | tok/sec: 38,063 | mfu: 12.23 | total time: 414.70m
step 01807/21400 (8.44%) | loss: 3.323274 | grad norm: 0.1202 | lrm: 1.00 | dt: 13830.45ms | tok/sec: 37,908 | mfu: 12.18 | total time: 414.93m
step 01808/21400 (8.45%) | loss: 3.288730 | grad norm: 0.1262 | lrm: 1.00 | dt: 13799.26ms | tok/sec: 37,993 | mfu: 12.21 | total time: 415.16m
step 01809/21400 (8.45%) | loss: 3.250041 | grad norm: 0.1184 | lrm: 1.00 | dt: 13815.10ms | tok/sec: 37,950 | mfu: 12.19 | total time: 415.39m
step 01810/21400 (8.46%) | loss: 3.270113 | grad norm: 0.1206 | lrm: 1.00 | dt: 13867.22ms | tok/sec: 37,807 | mfu: 12.15 | total time: 415.62m
step 01811/21400 (8.46%) | loss: 3.223908 | grad norm: 0.1178 | lrm: 1.00 | dt: 13742.13ms | tok/sec: 38,151 | mfu: 12.26 | total time: 415.85m
step 01812/21400 (8.47%) | loss: 3.232031 | grad norm: 0.1176 | lrm: 1.00 | dt: 13880.34ms | tok/sec: 37,771 | mfu: 12.13 | total time: 416.08m
step 01813/21400 (8.47%) | loss: 3.247591 | grad norm: 0.1334 | lrm: 1.00 | dt: 13742.46ms | tok/sec: 38,150 | mfu: 12.26 | total time: 416.31m
step 01814/21400 (8.48%) | loss: 3.225973 | grad norm: 0.1150 | lrm: 1.00 | dt: 13856.12ms | tok/sec: 37,838 | mfu: 12.16 | total time: 416.54m
step 01815/21400 (8.48%) | loss: 3.223797 | grad norm: 0.1035 | lrm: 1.00 | dt: 13734.29ms | tok/sec: 38,173 | mfu: 12.26 | total time: 416.77m
step 01816/21400 (8.49%) | loss: 3.189054 | grad norm: 0.1411 | lrm: 1.00 | dt: 13868.51ms | tok/sec: 37,804 | mfu: 12.14 | total time: 417.00m
step 01817/21400 (8.49%) | loss: 3.215387 | grad norm: 0.1200 | lrm: 1.00 | dt: 13780.70ms | tok/sec: 38,045 | mfu: 12.22 | total time: 417.23m
step 01818/21400 (8.50%) | loss: 3.216038 | grad norm: 0.1224 | lrm: 1.00 | dt: 13843.25ms | tok/sec: 37,873 | mfu: 12.17 | total time: 417.46m
step 01819/21400 (8.50%) | loss: 3.220126 | grad norm: 0.1269 | lrm: 1.00 | dt: 13779.83ms | tok/sec: 38,047 | mfu: 12.22 | total time: 417.69m
step 01820/21400 (8.50%) | loss: 3.186926 | grad norm: 0.1151 | lrm: 1.00 | dt: 13838.12ms | tok/sec: 37,887 | mfu: 12.17 | total time: 417.92m
step 01821/21400 (8.51%) | loss: 3.203912 | grad norm: 0.1112 | lrm: 1.00 | dt: 14231.78ms | tok/sec: 36,839 | mfu: 11.83 | total time: 418.16m
step 01822/21400 (8.51%) | loss: 3.213551 | grad norm: 0.1206 | lrm: 1.00 | dt: 13817.71ms | tok/sec: 37,943 | mfu: 12.19 | total time: 418.39m
step 01823/21400 (8.52%) | loss: 3.215236 | grad norm: 0.2365 | lrm: 1.00 | dt: 13856.61ms | tok/sec: 37,836 | mfu: 12.16 | total time: 418.62m
step 01824/21400 (8.52%) | loss: 3.209227 | grad norm: 0.1099 | lrm: 1.00 | dt: 13720.76ms | tok/sec: 38,211 | mfu: 12.28 | total time: 418.85m
step 01825/21400 (8.53%) | loss: 3.216821 | grad norm: 0.1222 | lrm: 1.00 | dt: 13860.81ms | tok/sec: 37,825 | mfu: 12.15 | total time: 419.08m
step 01826/21400 (8.53%) | loss: 3.235510 | grad norm: 0.1563 | lrm: 1.00 | dt: 13761.70ms | tok/sec: 38,097 | mfu: 12.24 | total time: 419.31m
step 01827/21400 (8.54%) | loss: 3.250730 | grad norm: 0.1136 | lrm: 1.00 | dt: 13852.45ms | tok/sec: 37,848 | mfu: 12.16 | total time: 419.54m
step 01828/21400 (8.54%) | loss: 3.213281 | grad norm: 0.1200 | lrm: 1.00 | dt: 13748.38ms | tok/sec: 38,134 | mfu: 12.25 | total time: 419.77m
step 01829/21400 (8.55%) | loss: 3.219674 | grad norm: 0.1230 | lrm: 1.00 | dt: 13851.06ms | tok/sec: 37,851 | mfu: 12.16 | total time: 420.00m
step 01830/21400 (8.55%) | loss: 3.244450 | grad norm: 0.1216 | lrm: 1.00 | dt: 13807.73ms | tok/sec: 37,970 | mfu: 12.20 | total time: 420.23m
step 01831/21400 (8.56%) | loss: 3.233550 | grad norm: 0.1183 | lrm: 1.00 | dt: 13778.47ms | tok/sec: 38,051 | mfu: 12.22 | total time: 420.46m
step 01832/21400 (8.56%) | loss: 3.235995 | grad norm: 0.1158 | lrm: 1.00 | dt: 13794.66ms | tok/sec: 38,006 | mfu: 12.21 | total time: 420.69m
step 01833/21400 (8.57%) | loss: 3.242853 | grad norm: 0.1179 | lrm: 1.00 | dt: 13829.43ms | tok/sec: 37,911 | mfu: 12.18 | total time: 420.92m
step 01834/21400 (8.57%) | loss: 3.240338 | grad norm: 0.1155 | lrm: 1.00 | dt: 13783.12ms | tok/sec: 38,038 | mfu: 12.22 | total time: 421.15m
step 01835/21400 (8.57%) | loss: 3.221405 | grad norm: 0.1124 | lrm: 1.00 | dt: 13810.29ms | tok/sec: 37,963 | mfu: 12.20 | total time: 421.38m
step 01836/21400 (8.58%) | loss: 3.200886 | grad norm: 0.1307 | lrm: 1.00 | dt: 13866.79ms | tok/sec: 37,808 | mfu: 12.15 | total time: 421.61m
step 01837/21400 (8.58%) | loss: 3.205766 | grad norm: 0.1184 | lrm: 1.00 | dt: 13743.74ms | tok/sec: 38,147 | mfu: 12.25 | total time: 421.84m
step 01838/21400 (8.59%) | loss: 3.179737 | grad norm: 0.1097 | lrm: 1.00 | dt: 13864.23ms | tok/sec: 37,815 | mfu: 12.15 | total time: 422.07m
step 01839/21400 (8.59%) | loss: 3.171661 | grad norm: 0.1298 | lrm: 1.00 | dt: 13716.80ms | tok/sec: 38,222 | mfu: 12.28 | total time: 422.30m
step 01840/21400 (8.60%) | loss: 3.142865 | grad norm: 0.1222 | lrm: 1.00 | dt: 13876.30ms | tok/sec: 37,782 | mfu: 12.14 | total time: 422.53m
step 01841/21400 (8.60%) | loss: 3.137218 | grad norm: 0.1116 | lrm: 1.00 | dt: 13749.47ms | tok/sec: 38,131 | mfu: 12.25 | total time: 422.76m
step 01842/21400 (8.61%) | loss: 3.085852 | grad norm: 0.1128 | lrm: 1.00 | dt: 13841.33ms | tok/sec: 37,878 | mfu: 12.17 | total time: 422.99m
step 01843/21400 (8.61%) | loss: 3.049059 | grad norm: 0.1180 | lrm: 1.00 | dt: 13762.71ms | tok/sec: 38,094 | mfu: 12.24 | total time: 423.22m
step 01844/21400 (8.62%) | loss: 3.088359 | grad norm: 0.1214 | lrm: 1.00 | dt: 13834.93ms | tok/sec: 37,895 | mfu: 12.17 | total time: 423.45m
step 01845/21400 (8.62%) | loss: 3.103568 | grad norm: 0.1206 | lrm: 1.00 | dt: 13775.37ms | tok/sec: 38,059 | mfu: 12.23 | total time: 423.68m
step 01846/21400 (8.63%) | loss: 3.126179 | grad norm: 0.1081 | lrm: 1.00 | dt: 13832.78ms | tok/sec: 37,901 | mfu: 12.18 | total time: 423.91m
step 01847/21400 (8.63%) | loss: 3.121339 | grad norm: 0.1073 | lrm: 1.00 | dt: 13798.46ms | tok/sec: 37,996 | mfu: 12.21 | total time: 424.14m
step 01848/21400 (8.64%) | loss: 3.134293 | grad norm: 0.1066 | lrm: 1.00 | dt: 13804.98ms | tok/sec: 37,978 | mfu: 12.20 | total time: 424.37m
step 01849/21400 (8.64%) | loss: 3.137689 | grad norm: 0.1117 | lrm: 1.00 | dt: 13861.71ms | tok/sec: 37,822 | mfu: 12.15 | total time: 424.61m
step 01850/21400 (8.64%) | loss: 3.153792 | grad norm: 0.1134 | lrm: 1.00 | dt: 13748.89ms | tok/sec: 38,133 | mfu: 12.25 | total time: 424.83m
step 01851/21400 (8.65%) | loss: 3.164640 | grad norm: 0.1124 | lrm: 1.00 | dt: 13858.79ms | tok/sec: 37,830 | mfu: 12.15 | total time: 425.07m
step 01852/21400 (8.65%) | loss: 3.142578 | grad norm: 0.1111 | lrm: 1.00 | dt: 13753.53ms | tok/sec: 38,120 | mfu: 12.25 | total time: 425.29m
step 01853/21400 (8.66%) | loss: 3.119275 | grad norm: 0.1107 | lrm: 1.00 | dt: 13852.80ms | tok/sec: 37,847 | mfu: 12.16 | total time: 425.53m
step 01854/21400 (8.66%) | loss: 3.077739 | grad norm: 0.1174 | lrm: 1.00 | dt: 13749.12ms | tok/sec: 38,132 | mfu: 12.25 | total time: 425.75m
step 01855/21400 (8.67%) | loss: 3.116514 | grad norm: 0.1228 | lrm: 1.00 | dt: 13852.19ms | tok/sec: 37,848 | mfu: 12.16 | total time: 425.99m
step 01856/21400 (8.67%) | loss: 3.122031 | grad norm: 0.1303 | lrm: 1.00 | dt: 13764.89ms | tok/sec: 38,088 | mfu: 12.24 | total time: 426.21m
step 01857/21400 (8.68%) | loss: 3.162591 | grad norm: 0.1330 | lrm: 1.00 | dt: 13837.65ms | tok/sec: 37,888 | mfu: 12.17 | total time: 426.45m
step 01858/21400 (8.68%) | loss: 3.145541 | grad norm: 0.1182 | lrm: 1.00 | dt: 13792.13ms | tok/sec: 38,013 | mfu: 12.21 | total time: 426.68m
step 01859/21400 (8.69%) | loss: 3.167319 | grad norm: 0.1136 | lrm: 1.00 | dt: 13821.04ms | tok/sec: 37,934 | mfu: 12.19 | total time: 426.91m
step 01860/21400 (8.69%) | loss: 3.169679 | grad norm: 0.1120 | lrm: 1.00 | dt: 13799.49ms | tok/sec: 37,993 | mfu: 12.21 | total time: 427.14m
step 01861/21400 (8.70%) | loss: 3.180947 | grad norm: 0.1128 | lrm: 1.00 | dt: 13806.05ms | tok/sec: 37,975 | mfu: 12.20 | total time: 427.37m
step 01862/21400 (8.70%) | loss: 3.203274 | grad norm: 0.1169 | lrm: 1.00 | dt: 13855.77ms | tok/sec: 37,838 | mfu: 12.16 | total time: 427.60m
step 01863/21400 (8.71%) | loss: 3.191949 | grad norm: 0.1160 | lrm: 1.00 | dt: 14218.95ms | tok/sec: 36,872 | mfu: 11.85 | total time: 427.83m
step 01864/21400 (8.71%) | loss: 3.186649 | grad norm: 0.1154 | lrm: 1.00 | dt: 13856.88ms | tok/sec: 37,835 | mfu: 12.15 | total time: 428.06m
step 01865/21400 (8.71%) | loss: 3.157814 | grad norm: 0.1185 | lrm: 1.00 | dt: 13739.53ms | tok/sec: 38,159 | mfu: 12.26 | total time: 428.29m
step 01866/21400 (8.72%) | loss: 3.197041 | grad norm: 0.1321 | lrm: 1.00 | dt: 13853.55ms | tok/sec: 37,845 | mfu: 12.16 | total time: 428.52m
step 01867/21400 (8.72%) | loss: 3.202548 | grad norm: 0.1302 | lrm: 1.00 | dt: 13760.18ms | tok/sec: 38,101 | mfu: 12.24 | total time: 428.75m
step 01868/21400 (8.73%) | loss: 3.240067 | grad norm: 0.1218 | lrm: 1.00 | dt: 13856.02ms | tok/sec: 37,838 | mfu: 12.16 | total time: 428.98m
step 01869/21400 (8.73%) | loss: 3.239361 | grad norm: 0.1211 | lrm: 1.00 | dt: 13772.93ms | tok/sec: 38,066 | mfu: 12.23 | total time: 429.21m
step 01870/21400 (8.74%) | loss: 3.243100 | grad norm: 0.1218 | lrm: 1.00 | dt: 13840.49ms | tok/sec: 37,880 | mfu: 12.17 | total time: 429.45m
step 01871/21400 (8.74%) | loss: 3.221172 | grad norm: 0.1231 | lrm: 1.00 | dt: 13776.07ms | tok/sec: 38,057 | mfu: 12.23 | total time: 429.67m
step 01872/21400 (8.75%) | loss: 3.178095 | grad norm: 0.1228 | lrm: 1.00 | dt: 13828.68ms | tok/sec: 37,913 | mfu: 12.18 | total time: 429.91m
step 01873/21400 (8.75%) | loss: 3.178278 | grad norm: 0.1276 | lrm: 1.00 | dt: 13786.23ms | tok/sec: 38,029 | mfu: 12.22 | total time: 430.13m
step 01874/21400 (8.76%) | loss: 3.176989 | grad norm: 0.1211 | lrm: 1.00 | dt: 13806.71ms | tok/sec: 37,973 | mfu: 12.20 | total time: 430.37m
step 01875/21400 (8.76%) | loss: 3.183428 | grad norm: 0.1215 | lrm: 1.00 | dt: 13855.74ms | tok/sec: 37,839 | mfu: 12.16 | total time: 430.60m
step 01876/21400 (8.77%) | loss: 3.211165 | grad norm: 0.1375 | lrm: 1.00 | dt: 13737.74ms | tok/sec: 38,164 | mfu: 12.26 | total time: 430.82m
step 01877/21400 (8.77%) | loss: 3.243138 | grad norm: 0.1367 | lrm: 1.00 | dt: 13865.01ms | tok/sec: 37,813 | mfu: 12.15 | total time: 431.06m
step 01878/21400 (8.78%) | loss: 3.249329 | grad norm: 0.1193 | lrm: 1.00 | dt: 13746.01ms | tok/sec: 38,141 | mfu: 12.25 | total time: 431.29m
step 01879/21400 (8.78%) | loss: 3.318036 | grad norm: 0.1229 | lrm: 1.00 | dt: 13848.29ms | tok/sec: 37,859 | mfu: 12.16 | total time: 431.52m
step 01880/21400 (8.79%) | loss: 3.294341 | grad norm: 0.1348 | lrm: 1.00 | dt: 13755.52ms | tok/sec: 38,114 | mfu: 12.24 | total time: 431.75m
step 01881/21400 (8.79%) | loss: 3.342885 | grad norm: 0.1253 | lrm: 1.00 | dt: 13843.54ms | tok/sec: 37,872 | mfu: 12.17 | total time: 431.98m
step 01882/21400 (8.79%) | loss: 3.306830 | grad norm: 0.1210 | lrm: 1.00 | dt: 13759.64ms | tok/sec: 38,103 | mfu: 12.24 | total time: 432.21m
step 01883/21400 (8.80%) | loss: 3.283631 | grad norm: 0.1334 | lrm: 1.00 | dt: 13831.72ms | tok/sec: 37,904 | mfu: 12.18 | total time: 432.44m
step 01884/21400 (8.80%) | loss: 3.245603 | grad norm: 0.1195 | lrm: 1.00 | dt: 13776.38ms | tok/sec: 38,057 | mfu: 12.23 | total time: 432.67m
step 01885/21400 (8.81%) | loss: 3.231104 | grad norm: 0.1113 | lrm: 1.00 | dt: 13819.48ms | tok/sec: 37,938 | mfu: 12.19 | total time: 432.90m
step 01886/21400 (8.81%) | loss: 3.289704 | grad norm: 0.1204 | lrm: 1.00 | dt: 13790.76ms | tok/sec: 38,017 | mfu: 12.21 | total time: 433.13m
step 01887/21400 (8.82%) | loss: 3.269198 | grad norm: 0.1270 | lrm: 1.00 | dt: 13807.30ms | tok/sec: 37,971 | mfu: 12.20 | total time: 433.36m
step 01888/21400 (8.82%) | loss: 3.219975 | grad norm: 0.1176 | lrm: 1.00 | dt: 13859.24ms | tok/sec: 37,829 | mfu: 12.15 | total time: 433.59m
step 01889/21400 (8.83%) | loss: 3.205321 | grad norm: 0.1180 | lrm: 1.00 | dt: 13736.89ms | tok/sec: 38,166 | mfu: 12.26 | total time: 433.82m
step 01890/21400 (8.83%) | loss: 3.206791 | grad norm: 0.1156 | lrm: 1.00 | dt: 13859.35ms | tok/sec: 37,829 | mfu: 12.15 | total time: 434.05m
step 01891/21400 (8.84%) | loss: 3.212815 | grad norm: 0.1231 | lrm: 1.00 | dt: 13753.83ms | tok/sec: 38,119 | mfu: 12.25 | total time: 434.28m
step 01892/21400 (8.84%) | loss: 3.230415 | grad norm: 0.1109 | lrm: 1.00 | dt: 13845.77ms | tok/sec: 37,866 | mfu: 12.16 | total time: 434.51m
step 01893/21400 (8.85%) | loss: 3.216342 | grad norm: 0.1122 | lrm: 1.00 | dt: 13758.50ms | tok/sec: 38,106 | mfu: 12.24 | total time: 434.74m
step 01894/21400 (8.85%) | loss: 3.251892 | grad norm: 0.1208 | lrm: 1.00 | dt: 13843.16ms | tok/sec: 37,873 | mfu: 12.17 | total time: 434.97m
step 01895/21400 (8.86%) | loss: 3.243099 | grad norm: 0.1174 | lrm: 1.00 | dt: 13764.50ms | tok/sec: 38,089 | mfu: 12.24 | total time: 435.20m
step 01896/21400 (8.86%) | loss: 3.234609 | grad norm: 0.1104 | lrm: 1.00 | dt: 13833.27ms | tok/sec: 37,900 | mfu: 12.18 | total time: 435.43m
step 01897/21400 (8.86%) | loss: 3.243864 | grad norm: 0.1084 | lrm: 1.00 | dt: 13789.50ms | tok/sec: 38,020 | mfu: 12.21 | total time: 435.66m
step 01898/21400 (8.87%) | loss: 3.271646 | grad norm: 0.1092 | lrm: 1.00 | dt: 13817.97ms | tok/sec: 37,942 | mfu: 12.19 | total time: 435.89m
step 01899/21400 (8.87%) | loss: 3.264093 | grad norm: 0.1231 | lrm: 1.00 | dt: 13799.59ms | tok/sec: 37,993 | mfu: 12.21 | total time: 436.12m
step 01900/21400 (8.88%) | loss: 3.280605 | grad norm: 0.1397 | lrm: 1.00 | dt: 13810.26ms | tok/sec: 37,963 | mfu: 12.20 | total time: 436.35m
step 01901/21400 (8.88%) | loss: 3.253064 | grad norm: 0.1588 | lrm: 1.00 | dt: 13857.14ms | tok/sec: 37,835 | mfu: 12.15 | total time: 436.58m
step 01902/21400 (8.89%) | loss: 3.254147 | grad norm: 0.1242 | lrm: 1.00 | dt: 13740.33ms | tok/sec: 38,156 | mfu: 12.26 | total time: 436.81m
step 01903/21400 (8.89%) | loss: 3.226552 | grad norm: 0.1173 | lrm: 1.00 | dt: 13858.49ms | tok/sec: 37,831 | mfu: 12.15 | total time: 437.04m
step 01904/21400 (8.90%) | loss: 3.231812 | grad norm: 0.1219 | lrm: 1.00 | dt: 13744.10ms | tok/sec: 38,146 | mfu: 12.25 | total time: 437.27m
step 01905/21400 (8.90%) | loss: 3.242381 | grad norm: 0.1379 | lrm: 1.00 | dt: 13850.22ms | tok/sec: 37,854 | mfu: 12.16 | total time: 437.50m
step 01906/21400 (8.91%) | loss: 3.266502 | grad norm: 0.1656 | lrm: 1.00 | dt: 13757.53ms | tok/sec: 38,109 | mfu: 12.24 | total time: 437.73m
step 01907/21400 (8.91%) | loss: 3.314994 | grad norm: 0.1156 | lrm: 1.00 | dt: 14296.18ms | tok/sec: 36,673 | mfu: 11.78 | total time: 437.97m
step 01908/21400 (8.92%) | loss: 3.308026 | grad norm: 0.1176 | lrm: 1.00 | dt: 13762.69ms | tok/sec: 38,094 | mfu: 12.24 | total time: 438.19m
step 01909/21400 (8.92%) | loss: 3.285780 | grad norm: 0.1164 | lrm: 1.00 | dt: 13836.22ms | tok/sec: 37,892 | mfu: 12.17 | total time: 438.43m
step 01910/21400 (8.93%) | loss: 3.286582 | grad norm: 0.1367 | lrm: 1.00 | dt: 13776.33ms | tok/sec: 38,057 | mfu: 12.23 | total time: 438.65m
step 01911/21400 (8.93%) | loss: 3.302688 | grad norm: 0.1227 | lrm: 1.00 | dt: 13820.12ms | tok/sec: 37,936 | mfu: 12.19 | total time: 438.89m
step 01912/21400 (8.93%) | loss: 3.292554 | grad norm: 0.1162 | lrm: 1.00 | dt: 13797.11ms | tok/sec: 37,999 | mfu: 12.21 | total time: 439.12m
step 01913/21400 (8.94%) | loss: 3.302764 | grad norm: 0.1254 | lrm: 1.00 | dt: 13856.92ms | tok/sec: 37,835 | mfu: 12.15 | total time: 439.35m
step 01914/21400 (8.94%) | loss: 3.278145 | grad norm: 0.1239 | lrm: 1.00 | dt: 13739.33ms | tok/sec: 38,159 | mfu: 12.26 | total time: 439.58m
step 01915/21400 (8.95%) | loss: 3.299751 | grad norm: 0.1346 | lrm: 1.00 | dt: 13798.25ms | tok/sec: 37,996 | mfu: 12.21 | total time: 439.81m
step 01916/21400 (8.95%) | loss: 3.317282 | grad norm: 0.1339 | lrm: 1.00 | dt: 13855.37ms | tok/sec: 37,840 | mfu: 12.16 | total time: 440.04m
step 01917/21400 (8.96%) | loss: 3.320997 | grad norm: 0.1137 | lrm: 1.00 | dt: 13745.79ms | tok/sec: 38,141 | mfu: 12.25 | total time: 440.27m
step 01918/21400 (8.96%) | loss: 3.301624 | grad norm: 0.1056 | lrm: 1.00 | dt: 13854.39ms | tok/sec: 37,842 | mfu: 12.16 | total time: 440.50m
step 01919/21400 (8.97%) | loss: 3.293822 | grad norm: 0.1124 | lrm: 1.00 | dt: 13767.75ms | tok/sec: 38,080 | mfu: 12.23 | total time: 440.73m
step 01920/21400 (8.97%) | loss: 3.295314 | grad norm: 0.1175 | lrm: 1.00 | dt: 13841.65ms | tok/sec: 37,877 | mfu: 12.17 | total time: 440.96m
step 01921/21400 (8.98%) | loss: 3.315857 | grad norm: 0.1190 | lrm: 1.00 | dt: 13766.57ms | tok/sec: 38,084 | mfu: 12.23 | total time: 441.19m
step 01922/21400 (8.98%) | loss: 3.276048 | grad norm: 0.2051 | lrm: 1.00 | dt: 13835.87ms | tok/sec: 37,893 | mfu: 12.17 | total time: 441.42m
step 01923/21400 (8.99%) | loss: 3.233384 | grad norm: 0.1152 | lrm: 1.00 | dt: 13784.51ms | tok/sec: 38,034 | mfu: 12.22 | total time: 441.65m
step 01924/21400 (8.99%) | loss: 3.222790 | grad norm: 0.1076 | lrm: 1.00 | dt: 13829.06ms | tok/sec: 37,912 | mfu: 12.18 | total time: 441.88m
step 01925/21400 (9.00%) | loss: 3.197891 | grad norm: 0.1109 | lrm: 1.00 | dt: 13809.60ms | tok/sec: 37,965 | mfu: 12.20 | total time: 442.11m
step 01926/21400 (9.00%) | loss: 3.224984 | grad norm: 0.1134 | lrm: 1.00 | dt: 13809.14ms | tok/sec: 37,966 | mfu: 12.20 | total time: 442.34m
step 01927/21400 (9.00%) | loss: 3.260253 | grad norm: 0.1108 | lrm: 1.00 | dt: 13866.48ms | tok/sec: 37,809 | mfu: 12.15 | total time: 442.57m
step 01928/21400 (9.01%) | loss: 3.257418 | grad norm: 0.1165 | lrm: 1.00 | dt: 13750.88ms | tok/sec: 38,127 | mfu: 12.25 | total time: 442.80m
step 01929/21400 (9.01%) | loss: 3.254765 | grad norm: 0.1215 | lrm: 1.00 | dt: 13851.67ms | tok/sec: 37,850 | mfu: 12.16 | total time: 443.03m
step 01930/21400 (9.02%) | loss: 3.237302 | grad norm: 0.1216 | lrm: 1.00 | dt: 13755.19ms | tok/sec: 38,115 | mfu: 12.24 | total time: 443.26m
step 01931/21400 (9.02%) | loss: 3.265191 | grad norm: 0.1141 | lrm: 1.00 | dt: 13831.83ms | tok/sec: 37,904 | mfu: 12.18 | total time: 443.49m
step 01932/21400 (9.03%) | loss: 3.301738 | grad norm: 0.1179 | lrm: 1.00 | dt: 13771.61ms | tok/sec: 38,070 | mfu: 12.23 | total time: 443.72m
step 01933/21400 (9.03%) | loss: 3.263171 | grad norm: 0.1182 | lrm: 1.00 | dt: 13842.07ms | tok/sec: 37,876 | mfu: 12.17 | total time: 443.95m
step 01934/21400 (9.04%) | loss: 3.248869 | grad norm: 0.1013 | lrm: 1.00 | dt: 13764.70ms | tok/sec: 38,089 | mfu: 12.24 | total time: 444.18m
step 01935/21400 (9.04%) | loss: 3.275717 | grad norm: 0.1029 | lrm: 1.00 | dt: 13827.09ms | tok/sec: 37,917 | mfu: 12.18 | total time: 444.41m
step 01936/21400 (9.05%) | loss: 3.258428 | grad norm: 0.1089 | lrm: 1.00 | dt: 13782.31ms | tok/sec: 38,040 | mfu: 12.22 | total time: 444.64m
step 01937/21400 (9.05%) | loss: 3.322711 | grad norm: 0.1087 | lrm: 1.00 | dt: 13818.59ms | tok/sec: 37,940 | mfu: 12.19 | total time: 444.87m
step 01938/21400 (9.06%) | loss: 3.302991 | grad norm: 0.1088 | lrm: 1.00 | dt: 13802.65ms | tok/sec: 37,984 | mfu: 12.20 | total time: 445.10m
step 01939/21400 (9.06%) | loss: 3.283934 | grad norm: 0.1114 | lrm: 1.00 | dt: 13804.29ms | tok/sec: 37,980 | mfu: 12.20 | total time: 445.33m
step 01940/21400 (9.07%) | loss: 3.285125 | grad norm: 0.1102 | lrm: 1.00 | dt: 13858.15ms | tok/sec: 37,832 | mfu: 12.15 | total time: 445.56m
step 01941/21400 (9.07%) | loss: 3.274999 | grad norm: 0.1152 | lrm: 1.00 | dt: 13763.45ms | tok/sec: 38,092 | mfu: 12.24 | total time: 445.79m
step 01942/21400 (9.07%) | loss: 3.231260 | grad norm: 0.1172 | lrm: 1.00 | dt: 13890.47ms | tok/sec: 37,744 | mfu: 12.13 | total time: 446.02m
step 01943/21400 (9.08%) | loss: 3.233566 | grad norm: 0.1206 | lrm: 1.00 | dt: 13751.93ms | tok/sec: 38,124 | mfu: 12.25 | total time: 446.25m
step 01944/21400 (9.08%) | loss: 3.228890 | grad norm: 0.1253 | lrm: 1.00 | dt: 13840.65ms | tok/sec: 37,880 | mfu: 12.17 | total time: 446.48m
step 01945/21400 (9.09%) | loss: 3.246656 | grad norm: 0.1289 | lrm: 1.00 | dt: 13767.71ms | tok/sec: 38,080 | mfu: 12.23 | total time: 446.71m
step 01946/21400 (9.09%) | loss: 3.237957 | grad norm: 0.1198 | lrm: 1.00 | dt: 13831.79ms | tok/sec: 37,904 | mfu: 12.18 | total time: 446.94m
step 01947/21400 (9.10%) | loss: 3.177639 | grad norm: 0.1136 | lrm: 1.00 | dt: 13764.39ms | tok/sec: 38,090 | mfu: 12.24 | total time: 447.17m
step 01948/21400 (9.10%) | loss: 3.168519 | grad norm: 0.1157 | lrm: 1.00 | dt: 13825.60ms | tok/sec: 37,921 | mfu: 12.18 | total time: 447.40m
step 01949/21400 (9.11%) | loss: 3.207809 | grad norm: 0.1193 | lrm: 1.00 | dt: 13781.71ms | tok/sec: 38,042 | mfu: 12.22 | total time: 447.63m
step 01950/21400 (9.11%) | loss: 3.182877 | grad norm: 0.1143 | lrm: 1.00 | dt: 13830.48ms | tok/sec: 37,908 | mfu: 12.18 | total time: 447.86m
step 01951/21400 (9.12%) | loss: 3.171902 | grad norm: 0.1123 | lrm: 1.00 | dt: 14295.02ms | tok/sec: 36,676 | mfu: 11.78 | total time: 448.10m
step 01952/21400 (9.12%) | loss: 3.207114 | grad norm: 0.1155 | lrm: 1.00 | dt: 13809.69ms | tok/sec: 37,965 | mfu: 12.20 | total time: 448.33m
step 01953/21400 (9.13%) | loss: 3.237227 | grad norm: 0.1134 | lrm: 1.00 | dt: 13868.40ms | tok/sec: 37,804 | mfu: 12.14 | total time: 448.56m
step 01954/21400 (9.13%) | loss: 3.264533 | grad norm: 0.1103 | lrm: 1.00 | dt: 13761.61ms | tok/sec: 38,097 | mfu: 12.24 | total time: 448.79m
step 01955/21400 (9.14%) | loss: 3.216166 | grad norm: 0.1146 | lrm: 1.00 | dt: 13849.69ms | tok/sec: 37,855 | mfu: 12.16 | total time: 449.02m
step 01956/21400 (9.14%) | loss: 3.221561 | grad norm: 0.1149 | lrm: 1.00 | dt: 13767.29ms | tok/sec: 38,082 | mfu: 12.23 | total time: 449.25m
step 01957/21400 (9.14%) | loss: 3.218967 | grad norm: 0.1051 | lrm: 1.00 | dt: 13855.39ms | tok/sec: 37,839 | mfu: 12.16 | total time: 449.48m
step 01958/21400 (9.15%) | loss: 3.256346 | grad norm: 0.1106 | lrm: 1.00 | dt: 13774.79ms | tok/sec: 38,061 | mfu: 12.23 | total time: 449.71m
step 01959/21400 (9.15%) | loss: 3.249031 | grad norm: 0.1098 | lrm: 1.00 | dt: 13843.21ms | tok/sec: 37,873 | mfu: 12.17 | total time: 449.94m
step 01960/21400 (9.16%) | loss: 3.234328 | grad norm: 0.1106 | lrm: 1.00 | dt: 13768.95ms | tok/sec: 38,077 | mfu: 12.23 | total time: 450.17m
step 01961/21400 (9.16%) | loss: 3.241751 | grad norm: 0.1125 | lrm: 1.00 | dt: 13835.76ms | tok/sec: 37,893 | mfu: 12.17 | total time: 450.40m
step 01962/21400 (9.17%) | loss: 3.266736 | grad norm: 0.1192 | lrm: 1.00 | dt: 13790.61ms | tok/sec: 38,017 | mfu: 12.21 | total time: 450.63m
step 01963/21400 (9.17%) | loss: 3.280957 | grad norm: 0.1235 | lrm: 1.00 | dt: 13824.47ms | tok/sec: 37,924 | mfu: 12.18 | total time: 450.86m
step 01964/21400 (9.18%) | loss: 3.257308 | grad norm: 0.1299 | lrm: 1.00 | dt: 13807.20ms | tok/sec: 37,972 | mfu: 12.20 | total time: 451.09m
step 01965/21400 (9.18%) | loss: 3.249912 | grad norm: 0.1223 | lrm: 1.00 | dt: 13808.87ms | tok/sec: 37,967 | mfu: 12.20 | total time: 451.32m
step 01966/21400 (9.19%) | loss: 3.269064 | grad norm: 0.1224 | lrm: 1.00 | dt: 13867.84ms | tok/sec: 37,806 | mfu: 12.15 | total time: 451.55m
step 01967/21400 (9.19%) | loss: 3.225279 | grad norm: 0.1220 | lrm: 1.00 | dt: 13755.65ms | tok/sec: 38,114 | mfu: 12.24 | total time: 451.78m
step 01968/21400 (9.20%) | loss: 3.183381 | grad norm: 0.1238 | lrm: 1.00 | dt: 13854.42ms | tok/sec: 37,842 | mfu: 12.16 | total time: 452.01m
step 01969/21400 (9.20%) | loss: 3.183574 | grad norm: 0.1247 | lrm: 1.00 | dt: 13764.81ms | tok/sec: 38,089 | mfu: 12.24 | total time: 452.24m
step 01970/21400 (9.21%) | loss: 3.149431 | grad norm: 0.1136 | lrm: 1.00 | dt: 13844.79ms | tok/sec: 37,868 | mfu: 12.17 | total time: 452.47m
step 01971/21400 (9.21%) | loss: 3.182255 | grad norm: 0.1168 | lrm: 1.00 | dt: 13765.42ms | tok/sec: 38,087 | mfu: 12.24 | total time: 452.70m
step 01972/21400 (9.21%) | loss: 3.208361 | grad norm: 0.1225 | lrm: 1.00 | dt: 13854.06ms | tok/sec: 37,843 | mfu: 12.16 | total time: 452.93m
step 01973/21400 (9.22%) | loss: 3.219679 | grad norm: 0.1278 | lrm: 1.00 | dt: 13784.59ms | tok/sec: 38,034 | mfu: 12.22 | total time: 453.16m
step 01974/21400 (9.22%) | loss: 3.250943 | grad norm: 0.1191 | lrm: 1.00 | dt: 13842.69ms | tok/sec: 37,874 | mfu: 12.17 | total time: 453.39m
step 01975/21400 (9.23%) | loss: 3.245029 | grad norm: 0.1170 | lrm: 1.00 | dt: 13795.63ms | tok/sec: 38,003 | mfu: 12.21 | total time: 453.62m
step 01976/21400 (9.23%) | loss: 3.243954 | grad norm: 0.1138 | lrm: 1.00 | dt: 13837.31ms | tok/sec: 37,889 | mfu: 12.17 | total time: 453.85m
step 01977/21400 (9.24%) | loss: 3.236813 | grad norm: 0.1236 | lrm: 1.00 | dt: 13806.09ms | tok/sec: 37,975 | mfu: 12.20 | total time: 454.08m
step 01978/21400 (9.24%) | loss: 3.241306 | grad norm: 0.1213 | lrm: 1.00 | dt: 13811.15ms | tok/sec: 37,961 | mfu: 12.20 | total time: 454.31m
step 01979/21400 (9.25%) | loss: 3.245333 | grad norm: 0.1208 | lrm: 1.00 | dt: 13861.92ms | tok/sec: 37,822 | mfu: 12.15 | total time: 454.54m
step 01980/21400 (9.25%) | loss: 3.245526 | grad norm: 0.1490 | lrm: 1.00 | dt: 13743.38ms | tok/sec: 38,148 | mfu: 12.26 | total time: 454.77m
step 01981/21400 (9.26%) | loss: 3.264285 | grad norm: 0.1355 | lrm: 1.00 | dt: 13857.81ms | tok/sec: 37,833 | mfu: 12.15 | total time: 455.00m
step 01982/21400 (9.26%) | loss: 3.289102 | grad norm: 0.1417 | lrm: 1.00 | dt: 13758.19ms | tok/sec: 38,107 | mfu: 12.24 | total time: 455.23m
step 01983/21400 (9.27%) | loss: 3.281022 | grad norm: 0.1180 | lrm: 1.00 | dt: 13849.97ms | tok/sec: 37,854 | mfu: 12.16 | total time: 455.46m
step 01984/21400 (9.27%) | loss: 3.267712 | grad norm: 0.1122 | lrm: 1.00 | dt: 13768.63ms | tok/sec: 38,078 | mfu: 12.23 | total time: 455.69m
step 01985/21400 (9.28%) | loss: 3.248904 | grad norm: 0.1109 | lrm: 1.00 | dt: 13842.99ms | tok/sec: 37,873 | mfu: 12.17 | total time: 455.93m
step 01986/21400 (9.28%) | loss: 3.238822 | grad norm: 0.1227 | lrm: 1.00 | dt: 13781.38ms | tok/sec: 38,043 | mfu: 12.22 | total time: 456.15m
step 01987/21400 (9.29%) | loss: 3.235168 | grad norm: 0.1152 | lrm: 1.00 | dt: 13842.79ms | tok/sec: 37,874 | mfu: 12.17 | total time: 456.39m
step 01988/21400 (9.29%) | loss: 3.292771 | grad norm: 0.1217 | lrm: 1.00 | dt: 13783.80ms | tok/sec: 38,036 | mfu: 12.22 | total time: 456.62m
step 01989/21400 (9.29%) | loss: 3.325243 | grad norm: 0.1134 | lrm: 1.00 | dt: 13823.32ms | tok/sec: 37,927 | mfu: 12.18 | total time: 456.85m
step 01990/21400 (9.30%) | loss: 3.325302 | grad norm: 0.1116 | lrm: 1.00 | dt: 13815.29ms | tok/sec: 37,949 | mfu: 12.19 | total time: 457.08m
step 01991/21400 (9.30%) | loss: 3.307389 | grad norm: 0.1151 | lrm: 1.00 | dt: 13809.92ms | tok/sec: 37,964 | mfu: 12.20 | total time: 457.31m
step 01992/21400 (9.31%) | loss: 3.273098 | grad norm: 0.1279 | lrm: 1.00 | dt: 13858.20ms | tok/sec: 37,832 | mfu: 12.15 | total time: 457.54m
step 01993/21400 (9.31%) | loss: 3.333953 | grad norm: 0.1185 | lrm: 1.00 | dt: 13760.59ms | tok/sec: 38,100 | mfu: 12.24 | total time: 457.77m
step 01994/21400 (9.32%) | loss: 3.332278 | grad norm: 0.1217 | lrm: 1.00 | dt: 13844.16ms | tok/sec: 37,870 | mfu: 12.17 | total time: 458.00m
step 01995/21400 (9.32%) | loss: 3.327910 | grad norm: 0.1173 | lrm: 1.00 | dt: 14244.19ms | tok/sec: 36,807 | mfu: 11.82 | total time: 458.23m
step 01996/21400 (9.33%) | loss: 3.331089 | grad norm: 0.1129 | lrm: 1.00 | dt: 13847.91ms | tok/sec: 37,860 | mfu: 12.16 | total time: 458.47m
step 01997/21400 (9.33%) | loss: 3.322921 | grad norm: 0.1219 | lrm: 1.00 | dt: 13757.20ms | tok/sec: 38,110 | mfu: 12.24 | total time: 458.69m
step 01998/21400 (9.34%) | loss: 3.293081 | grad norm: 0.1301 | lrm: 1.00 | dt: 13844.22ms | tok/sec: 37,870 | mfu: 12.17 | total time: 458.93m
step 01999/21400 (9.34%) | loss: 3.307826 | grad norm: 0.1120 | lrm: 1.00 | dt: 13770.38ms | tok/sec: 38,073 | mfu: 12.23 | total time: 459.15m
Step 02000 | Validation bpb: 0.9579
Downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...
Downloaded to /home/henny/.cache/nanochat/eval_bundle.zip
Placed eval_bundle directory at /home/henny/.cache/nanochat/eval_bundle
Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3300 | centered: 0.1067 | time: 7.70s
Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0060 | centered: 0.0060 | time: 6.91s
Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.3200 | centered: 0.3200 | time: 6.84s
Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.5520 | centered: 0.4027 | time: 9.97s
Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2880 | centered: 0.0507 | time: 11.15s
Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.5800 | centered: 0.1600 | time: 1.34s
Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.3280 | centered: 0.1600 | time: 12.53s
Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6300 | centered: 0.2600 | time: 8.02s
Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.2860 | centered: 0.0480 | time: 6.97s
Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.2200 | centered: 0.2200 | time: 7.01s
Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.3320 | centered: 0.1093 | time: 22.98s
Evaluating: winograd (0-shot, type: schema)... accuracy: 0.5421 | centered: 0.0842 | time: 3.87s
Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5180 | centered: 0.0360 | time: 6.95s
Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1080 | centered: 0.1080 | time: 7.90s
Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2565 | centered: 0.0707 | time: 9.96s
Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3940 | centered: 0.3940 | time: 8.07s
Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1095 | centered: 0.1095 | time: 3.36s
Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.53s
Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0300 | centered: 0.0300 | time: 14.31s
Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.0880 | centered: 0.0880 | time: 7.96s
Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.4680 | centered: -0.4000 | time: 21.16s
Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2680 | centered: 0.1947 | time: 37.48s
Step 02000 | CORE metric: 0.1163
<|bos|>The capital of France is the city of Paris. The capital of France is Paris. The capital of France
<|bos|>The chemical symbol of gold is gold. It is a symbol of the earth, the earth, and the sky
<|bos|>If yesterday was Friday, then tomorrow will be Friday, and so on. The day of the week is a day of celebration
<|bos|>The opposite of hot is cold. The opposite of hot is hot.
The opposite of hot is hot.

<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Earth, Mars, Jupiter, Saturn, Uranus,
<|bos|>My favorite color is red. I love to use it in my art projects. I love to use
<|bos|>If 5*x + 3 = 13, then x is 5*x + 3 = 13. If 5*x
step 02000/21400 (9.35%) | loss: 3.294498 | grad norm: 0.1153 | lrm: 1.00 | dt: 13769.89ms | tok/sec: 38,074 | mfu: 12.23 | total time: 459.38m
2025-11-11 06:59:53,744 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model file to: /home/henny/.cache/nanochat/base_checkpoints/d20/model_002000.pt
2025-11-11 06:59:58,584 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer file to: /home/henny/.cache/nanochat/base_checkpoints/d20/optim_002000.pt
2025-11-11 06:59:58,585 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata file to: /home/henny/.cache/nanochat/base_checkpoints/d20/meta_002000.json
step 02001/21400 (9.35%) | loss: 3.286313 | grad norm: 0.1183 | lrm: 1.00 | dt: 14115.84ms | tok/sec: 37,141 | mfu: 11.93 | total time: 459.62m
step 02002/21400 (9.36%) | loss: 3.349757 | grad norm: 0.1089 | lrm: 1.00 | dt: 13806.54ms | tok/sec: 37,973 | mfu: 12.20 | total time: 459.85m
step 02003/21400 (9.36%) | loss: 3.394441 | grad norm: 0.1112 | lrm: 1.00 | dt: 13750.53ms | tok/sec: 38,128 | mfu: 12.25 | total time: 460.08m
step 02004/21400 (9.36%) | loss: 3.367453 | grad norm: 0.1133 | lrm: 1.00 | dt: 13784.30ms | tok/sec: 38,035 | mfu: 12.22 | total time: 460.31m
step 02005/21400 (9.37%) | loss: 3.363137 | grad norm: 0.1086 | lrm: 1.00 | dt: 13842.89ms | tok/sec: 37,874 | mfu: 12.17 | total time: 460.54m
step 02006/21400 (9.37%) | loss: 3.345914 | grad norm: 0.1161 | lrm: 1.00 | dt: 13768.87ms | tok/sec: 38,077 | mfu: 12.23 | total time: 460.77m
step 02007/21400 (9.38%) | loss: 3.361754 | grad norm: 0.1118 | lrm: 1.00 | dt: 13833.02ms | tok/sec: 37,901 | mfu: 12.18 | total time: 461.00m
step 02008/21400 (9.38%) | loss: 3.362700 | grad norm: 0.1172 | lrm: 1.00 | dt: 13783.11ms | tok/sec: 38,038 | mfu: 12.22 | total time: 461.23m
step 02009/21400 (9.39%) | loss: 3.378960 | grad norm: 0.1157 | lrm: 1.00 | dt: 13824.03ms | tok/sec: 37,925 | mfu: 12.18 | total time: 461.46m
step 02010/21400 (9.39%) | loss: 3.374523 | grad norm: 0.1142 | lrm: 1.00 | dt: 13799.85ms | tok/sec: 37,992 | mfu: 12.21 | total time: 461.69m
step 02011/21400 (9.40%) | loss: 3.374977 | grad norm: 0.1202 | lrm: 1.00 | dt: 13805.92ms | tok/sec: 37,975 | mfu: 12.20 | total time: 461.92m
step 02012/21400 (9.40%) | loss: 3.364588 | grad norm: 0.1178 | lrm: 1.00 | dt: 13852.32ms | tok/sec: 37,848 | mfu: 12.16 | total time: 462.15m
step 02013/21400 (9.41%) | loss: 3.384511 | grad norm: 0.1122 | lrm: 1.00 | dt: 13749.72ms | tok/sec: 38,130 | mfu: 12.25 | total time: 462.38m
step 02014/21400 (9.41%) | loss: 3.376613 | grad norm: 0.1221 | lrm: 1.00 | dt: 13860.28ms | tok/sec: 37,826 | mfu: 12.15 | total time: 462.61m
step 02015/21400 (9.42%) | loss: 3.353230 | grad norm: 0.1240 | lrm: 1.00 | dt: 13765.00ms | tok/sec: 38,088 | mfu: 12.24 | total time: 462.84m
step 02016/21400 (9.42%) | loss: 3.347657 | grad norm: 0.1061 | lrm: 1.00 | dt: 13849.61ms | tok/sec: 37,855 | mfu: 12.16 | total time: 463.07m
step 02017/21400 (9.43%) | loss: 3.364092 | grad norm: 0.1019 | lrm: 1.00 | dt: 13761.11ms | tok/sec: 38,099 | mfu: 12.24 | total time: 463.30m
step 02018/21400 (9.43%) | loss: 3.350818 | grad norm: 0.1110 | lrm: 1.00 | dt: 13830.46ms | tok/sec: 37,908 | mfu: 12.18 | total time: 463.53m
step 02019/21400 (9.43%) | loss: 3.317689 | grad norm: 0.1188 | lrm: 1.00 | dt: 13783.32ms | tok/sec: 38,037 | mfu: 12.22 | total time: 463.76m
step 02020/21400 (9.44%) | loss: 3.307150 | grad norm: 0.1117 | lrm: 1.00 | dt: 13840.24ms | tok/sec: 37,881 | mfu: 12.17 | total time: 463.99m
step 02021/21400 (9.44%) | loss: 3.346198 | grad norm: 0.1109 | lrm: 1.00 | dt: 13792.93ms | tok/sec: 38,011 | mfu: 12.21 | total time: 464.22m
step 02022/21400 (9.45%) | loss: 3.376154 | grad norm: 0.1157 | lrm: 1.00 | dt: 13811.93ms | tok/sec: 37,959 | mfu: 12.19 | total time: 464.45m
step 02023/21400 (9.45%) | loss: 3.369755 | grad norm: 0.1179 | lrm: 1.00 | dt: 13800.98ms | tok/sec: 37,989 | mfu: 12.20 | total time: 464.68m
step 02024/21400 (9.46%) | loss: 3.286430 | grad norm: 0.1075 | lrm: 1.00 | dt: 13816.79ms | tok/sec: 37,945 | mfu: 12.19 | total time: 464.91m
step 02025/21400 (9.46%) | loss: 3.269172 | grad norm: 0.1245 | lrm: 1.00 | dt: 13851.32ms | tok/sec: 37,851 | mfu: 12.16 | total time: 465.14m
step 02026/21400 (9.47%) | loss: 3.241008 | grad norm: 0.1175 | lrm: 1.00 | dt: 13756.08ms | tok/sec: 38,113 | mfu: 12.24 | total time: 465.37m
step 02027/21400 (9.47%) | loss: 3.227182 | grad norm: 0.1170 | lrm: 1.00 | dt: 13867.26ms | tok/sec: 37,807 | mfu: 12.15 | total time: 465.60m
step 02028/21400 (9.48%) | loss: 3.202216 | grad norm: 0.1090 | lrm: 1.00 | dt: 13761.67ms | tok/sec: 38,097 | mfu: 12.24 | total time: 465.83m
step 02029/21400 (9.48%) | loss: 3.175567 | grad norm: 0.1041 | lrm: 1.00 | dt: 13850.66ms | tok/sec: 37,852 | mfu: 12.16 | total time: 466.06m
step 02030/21400 (9.49%) | loss: 3.142545 | grad norm: 0.1014 | lrm: 1.00 | dt: 13758.46ms | tok/sec: 38,106 | mfu: 12.24 | total time: 466.29m
step 02031/21400 (9.49%) | loss: 3.155635 | grad norm: 0.1130 | lrm: 1.00 | dt: 14355.43ms | tok/sec: 36,521 | mfu: 11.73 | total time: 466.53m
step 02032/21400 (9.50%) | loss: 3.166022 | grad norm: 0.1114 | lrm: 1.00 | dt: 13762.47ms | tok/sec: 38,095 | mfu: 12.24 | total time: 466.76m
step 02033/21400 (9.50%) | loss: 3.170093 | grad norm: 0.1176 | lrm: 1.00 | dt: 13834.25ms | tok/sec: 37,897 | mfu: 12.17 | total time: 466.99m
step 02034/21400 (9.50%) | loss: 3.184774 | grad norm: 0.1160 | lrm: 1.00 | dt: 13795.61ms | tok/sec: 38,003 | mfu: 12.21 | total time: 467.22m
step 02035/21400 (9.51%) | loss: 3.219556 | grad norm: 0.1090 | lrm: 1.00 | dt: 13826.83ms | tok/sec: 37,918 | mfu: 12.18 | total time: 467.45m
step 02036/21400 (9.51%) | loss: 3.241863 | grad norm: 0.1137 | lrm: 1.00 | dt: 13798.70ms | tok/sec: 37,995 | mfu: 12.21 | total time: 467.68m
step 02037/21400 (9.52%) | loss: 3.239827 | grad norm: 0.1114 | lrm: 1.00 | dt: 13807.72ms | tok/sec: 37,970 | mfu: 12.20 | total time: 467.91m
step 02038/21400 (9.52%) | loss: 3.214792 | grad norm: 0.1167 | lrm: 1.00 | dt: 13849.23ms | tok/sec: 37,856 | mfu: 12.16 | total time: 468.14m
step 02039/21400 (9.53%) | loss: 3.226267 | grad norm: 0.1122 | lrm: 1.00 | dt: 13755.40ms | tok/sec: 38,115 | mfu: 12.24 | total time: 468.37m
step 02040/21400 (9.53%) | loss: 3.270057 | grad norm: 0.1135 | lrm: 1.00 | dt: 13846.35ms | tok/sec: 37,864 | mfu: 12.16 | total time: 468.60m
step 02041/21400 (9.54%) | loss: 3.266390 | grad norm: 0.1173 | lrm: 1.00 | dt: 13755.35ms | tok/sec: 38,115 | mfu: 12.24 | total time: 468.83m
step 02042/21400 (9.54%) | loss: 3.253122 | grad norm: 0.1193 | lrm: 1.00 | dt: 13864.91ms | tok/sec: 37,814 | mfu: 12.15 | total time: 469.06m
step 02043/21400 (9.55%) | loss: 3.251301 | grad norm: 0.1159 | lrm: 1.00 | dt: 13765.16ms | tok/sec: 38,088 | mfu: 12.24 | total time: 469.29m
step 02044/21400 (9.55%) | loss: 3.253561 | grad norm: 0.1127 | lrm: 1.00 | dt: 13845.72ms | tok/sec: 37,866 | mfu: 12.16 | total time: 469.52m
step 02045/21400 (9.56%) | loss: 3.242596 | grad norm: 0.1155 | lrm: 1.00 | dt: 13769.34ms | tok/sec: 38,076 | mfu: 12.23 | total time: 469.75m
step 02046/21400 (9.56%) | loss: 3.206481 | grad norm: 0.1228 | lrm: 1.00 | dt: 13834.73ms | tok/sec: 37,896 | mfu: 12.17 | total time: 469.98m
step 02047/21400 (9.57%) | loss: 3.218988 | grad norm: 0.1234 | lrm: 1.00 | dt: 13784.69ms | tok/sec: 38,034 | mfu: 12.22 | total time: 470.21m
step 02048/21400 (9.57%) | loss: 3.205314 | grad norm: 0.1158 | lrm: 1.00 | dt: 13828.89ms | tok/sec: 37,912 | mfu: 12.18 | total time: 470.44m
step 02049/21400 (9.57%) | loss: 3.183104 | grad norm: 0.1116 | lrm: 1.00 | dt: 13810.61ms | tok/sec: 37,962 | mfu: 12.20 | total time: 470.67m
step 02050/21400 (9.58%) | loss: 3.215611 | grad norm: 0.1229 | lrm: 1.00 | dt: 13813.25ms | tok/sec: 37,955 | mfu: 12.19 | total time: 470.90m
step 02051/21400 (9.58%) | loss: 3.220791 | grad norm: 0.1108 | lrm: 1.00 | dt: 13856.77ms | tok/sec: 37,836 | mfu: 12.15 | total time: 471.13m
step 02052/21400 (9.59%) | loss: 3.231471 | grad norm: 0.1111 | lrm: 1.00 | dt: 13750.14ms | tok/sec: 38,129 | mfu: 12.25 | total time: 471.36m
step 02053/21400 (9.59%) | loss: 3.207725 | grad norm: 0.1260 | lrm: 1.00 | dt: 13847.88ms | tok/sec: 37,860 | mfu: 12.16 | total time: 471.59m
step 02054/21400 (9.60%) | loss: 3.199732 | grad norm: 0.1239 | lrm: 1.00 | dt: 13758.61ms | tok/sec: 38,106 | mfu: 12.24 | total time: 471.82m
step 02055/21400 (9.60%) | loss: 3.200313 | grad norm: 0.1166 | lrm: 1.00 | dt: 13848.48ms | tok/sec: 37,858 | mfu: 12.16 | total time: 472.05m
step 02056/21400 (9.61%) | loss: 3.214246 | grad norm: 0.1200 | lrm: 1.00 | dt: 13763.48ms | tok/sec: 38,092 | mfu: 12.24 | total time: 472.28m
step 02057/21400 (9.61%) | loss: 3.209558 | grad norm: 0.1196 | lrm: 1.00 | dt: 13840.10ms | tok/sec: 37,881 | mfu: 12.17 | total time: 472.52m
step 02058/21400 (9.62%) | loss: 3.201505 | grad norm: 0.1069 | lrm: 1.00 | dt: 13780.87ms | tok/sec: 38,044 | mfu: 12.22 | total time: 472.74m
step 02059/21400 (9.62%) | loss: 3.212734 | grad norm: 0.1197 | lrm: 1.00 | dt: 13824.85ms | tok/sec: 37,923 | mfu: 12.18 | total time: 472.98m
step 02060/21400 (9.63%) | loss: 3.035754 | grad norm: 0.1302 | lrm: 1.00 | dt: 13786.99ms | tok/sec: 38,027 | mfu: 12.22 | total time: 473.20m
step 02061/21400 (9.63%) | loss: 3.084297 | grad norm: 0.1225 | lrm: 1.00 | dt: 13816.81ms | tok/sec: 37,945 | mfu: 12.19 | total time: 473.44m
step 02062/21400 (9.64%) | loss: 3.096923 | grad norm: 0.1213 | lrm: 1.00 | dt: 13809.80ms | tok/sec: 37,964 | mfu: 12.20 | total time: 473.67m
step 02063/21400 (9.64%) | loss: 3.118384 | grad norm: 0.1237 | lrm: 1.00 | dt: 13804.74ms | tok/sec: 37,978 | mfu: 12.20 | total time: 473.90m
step 02064/21400 (9.64%) | loss: 3.400017 | grad norm: 0.1281 | lrm: 1.00 | dt: 13855.61ms | tok/sec: 37,839 | mfu: 12.16 | total time: 474.13m
step 02065/21400 (9.65%) | loss: 3.364031 | grad norm: 0.1261 | lrm: 1.00 | dt: 13761.95ms | tok/sec: 38,096 | mfu: 12.24 | total time: 474.36m
step 02066/21400 (9.65%) | loss: 3.380175 | grad norm: 0.1212 | lrm: 1.00 | dt: 13858.16ms | tok/sec: 37,832 | mfu: 12.15 | total time: 474.59m
step 02067/21400 (9.66%) | loss: 3.350955 | grad norm: 0.1267 | lrm: 1.00 | dt: 13766.69ms | tok/sec: 38,083 | mfu: 12.23 | total time: 474.82m
step 02068/21400 (9.66%) | loss: 3.352994 | grad norm: 0.1211 | lrm: 1.00 | dt: 13845.13ms | tok/sec: 37,868 | mfu: 12.17 | total time: 475.05m
step 02069/21400 (9.67%) | loss: 3.318935 | grad norm: 0.1080 | lrm: 1.00 | dt: 13774.52ms | tok/sec: 38,062 | mfu: 12.23 | total time: 475.28m
step 02070/21400 (9.67%) | loss: 3.292985 | grad norm: 0.1098 | lrm: 1.00 | dt: 13832.11ms | tok/sec: 37,903 | mfu: 12.18 | total time: 475.51m
step 02071/21400 (9.68%) | loss: 3.282822 | grad norm: 0.1146 | lrm: 1.00 | dt: 13781.09ms | tok/sec: 38,044 | mfu: 12.22 | total time: 475.74m
step 02072/21400 (9.68%) | loss: 3.265415 | grad norm: 0.1045 | lrm: 1.00 | dt: 13829.35ms | tok/sec: 37,911 | mfu: 12.18 | total time: 475.97m
step 02073/21400 (9.69%) | loss: 3.289250 | grad norm: 0.1030 | lrm: 1.00 | dt: 13806.13ms | tok/sec: 37,975 | mfu: 12.20 | total time: 476.20m
step 02074/21400 (9.69%) | loss: 3.273787 | grad norm: 0.1104 | lrm: 1.00 | dt: 13819.65ms | tok/sec: 37,937 | mfu: 12.19 | total time: 476.43m
step 02075/21400 (9.70%) | loss: 3.247994 | grad norm: 0.1013 | lrm: 1.00 | dt: 13800.00ms | tok/sec: 37,991 | mfu: 12.20 | total time: 476.66m
step 02076/21400 (9.70%) | loss: 3.195462 | grad norm: 0.0967 | lrm: 1.00 | dt: 13808.35ms | tok/sec: 37,968 | mfu: 12.20 | total time: 476.89m
step 02077/21400 (9.71%) | loss: 3.168534 | grad norm: 0.1032 | lrm: 1.00 | dt: 14354.54ms | tok/sec: 36,524 | mfu: 11.73 | total time: 477.13m
step 02078/21400 (9.71%) | loss: 3.178984 | grad norm: 0.1092 | lrm: 1.00 | dt: 13757.02ms | tok/sec: 38,110 | mfu: 12.24 | total time: 477.36m
step 02079/21400 (9.71%) | loss: 3.169139 | grad norm: 0.1121 | lrm: 1.00 | dt: 13857.31ms | tok/sec: 37,834 | mfu: 12.15 | total time: 477.59m
step 02080/21400 (9.72%) | loss: 3.180775 | grad norm: 0.1152 | lrm: 1.00 | dt: 13758.73ms | tok/sec: 38,105 | mfu: 12.24 | total time: 477.82m
step 02081/21400 (9.72%) | loss: 3.164889 | grad norm: 0.1199 | lrm: 1.00 | dt: 13846.87ms | tok/sec: 37,863 | mfu: 12.16 | total time: 478.05m
step 02082/21400 (9.73%) | loss: 3.158530 | grad norm: 0.1148 | lrm: 1.00 | dt: 13762.39ms | tok/sec: 38,095 | mfu: 12.24 | total time: 478.28m
step 02083/21400 (9.73%) | loss: 3.120313 | grad norm: 0.1161 | lrm: 1.00 | dt: 13841.07ms | tok/sec: 37,879 | mfu: 12.17 | total time: 478.51m
step 02084/21400 (9.74%) | loss: 3.143437 | grad norm: 0.1175 | lrm: 1.00 | dt: 13773.53ms | tok/sec: 38,064 | mfu: 12.23 | total time: 478.74m
step 02085/21400 (9.74%) | loss: 3.176294 | grad norm: 0.1167 | lrm: 1.00 | dt: 13833.26ms | tok/sec: 37,900 | mfu: 12.18 | total time: 478.97m
step 02086/21400 (9.75%) | loss: 3.199641 | grad norm: 0.1140 | lrm: 1.00 | dt: 13786.98ms | tok/sec: 38,027 | mfu: 12.22 | total time: 479.20m
step 02087/21400 (9.75%) | loss: 3.152749 | grad norm: 0.1125 | lrm: 1.00 | dt: 13815.57ms | tok/sec: 37,949 | mfu: 12.19 | total time: 479.43m
step 02088/21400 (9.76%) | loss: 3.174689 | grad norm: 0.1345 | lrm: 1.00 | dt: 13800.88ms | tok/sec: 37,989 | mfu: 12.20 | total time: 479.66m
step 02089/21400 (9.76%) | loss: 3.133332 | grad norm: 0.1378 | lrm: 1.00 | dt: 13806.64ms | tok/sec: 37,973 | mfu: 12.20 | total time: 479.89m
step 02090/21400 (9.77%) | loss: 3.159932 | grad norm: 0.1223 | lrm: 1.00 | dt: 13855.69ms | tok/sec: 37,839 | mfu: 12.16 | total time: 480.12m
step 02091/21400 (9.77%) | loss: 3.162975 | grad norm: 0.1176 | lrm: 1.00 | dt: 13751.39ms | tok/sec: 38,126 | mfu: 12.25 | total time: 480.35m
step 02092/21400 (9.78%) | loss: 3.154991 | grad norm: 0.1156 | lrm: 1.00 | dt: 13866.98ms | tok/sec: 37,808 | mfu: 12.15 | total time: 480.58m
step 02093/21400 (9.78%) | loss: 3.180682 | grad norm: 0.1190 | lrm: 1.00 | dt: 13776.06ms | tok/sec: 38,057 | mfu: 12.23 | total time: 480.81m
step 02094/21400 (9.79%) | loss: 3.189019 | grad norm: 0.1203 | lrm: 1.00 | dt: 13855.24ms | tok/sec: 37,840 | mfu: 12.16 | total time: 481.04m
step 02095/21400 (9.79%) | loss: 3.201832 | grad norm: 0.1228 | lrm: 1.00 | dt: 13757.68ms | tok/sec: 38,108 | mfu: 12.24 | total time: 481.27m
step 02096/21400 (9.79%) | loss: 3.230015 | grad norm: 0.1161 | lrm: 1.00 | dt: 13835.72ms | tok/sec: 37,893 | mfu: 12.17 | total time: 481.50m
step 02097/21400 (9.80%) | loss: 3.234852 | grad norm: 0.1160 | lrm: 1.00 | dt: 13781.93ms | tok/sec: 38,041 | mfu: 12.22 | total time: 481.73m
step 02098/21400 (9.80%) | loss: 3.184981 | grad norm: 0.1126 | lrm: 1.00 | dt: 13831.19ms | tok/sec: 37,906 | mfu: 12.18 | total time: 481.96m
step 02099/21400 (9.81%) | loss: 3.177212 | grad norm: 0.1095 | lrm: 1.00 | dt: 13790.01ms | tok/sec: 38,019 | mfu: 12.21 | total time: 482.19m
step 02100/21400 (9.81%) | loss: 3.216229 | grad norm: 0.1142 | lrm: 1.00 | dt: 13815.65ms | tok/sec: 37,948 | mfu: 12.19 | total time: 482.42m
step 02101/21400 (9.82%) | loss: 3.215600 | grad norm: 0.1114 | lrm: 1.00 | dt: 13807.80ms | tok/sec: 37,970 | mfu: 12.20 | total time: 482.65m
step 02102/21400 (9.82%) | loss: 3.206719 | grad norm: 0.1101 | lrm: 1.00 | dt: 13801.10ms | tok/sec: 37,988 | mfu: 12.20 | total time: 482.88m
step 02103/21400 (9.83%) | loss: 3.226708 | grad norm: 0.1280 | lrm: 1.00 | dt: 13857.31ms | tok/sec: 37,834 | mfu: 12.15 | total time: 483.11m
step 02104/21400 (9.83%) | loss: 3.212594 | grad norm: 0.1256 | lrm: 1.00 | dt: 13762.93ms | tok/sec: 38,094 | mfu: 12.24 | total time: 483.34m
step 02105/21400 (9.84%) | loss: 3.247084 | grad norm: 0.1050 | lrm: 1.00 | dt: 13846.27ms | tok/sec: 37,864 | mfu: 12.16 | total time: 483.57m
step 02106/21400 (9.84%) | loss: 3.263674 | grad norm: 0.1077 | lrm: 1.00 | dt: 13777.22ms | tok/sec: 38,054 | mfu: 12.23 | total time: 483.80m
step 02107/21400 (9.85%) | loss: 3.260488 | grad norm: 0.1162 | lrm: 1.00 | dt: 13858.54ms | tok/sec: 37,831 | mfu: 12.15 | total time: 484.03m
step 02108/21400 (9.85%) | loss: 3.263222 | grad norm: 0.1175 | lrm: 1.00 | dt: 13791.76ms | tok/sec: 38,014 | mfu: 12.21 | total time: 484.26m
step 02109/21400 (9.86%) | loss: 3.239444 | grad norm: 0.1128 | lrm: 1.00 | dt: 13841.47ms | tok/sec: 37,878 | mfu: 12.17 | total time: 484.49m
step 02110/21400 (9.86%) | loss: 3.267492 | grad norm: 0.1074 | lrm: 1.00 | dt: 13784.28ms | tok/sec: 38,035 | mfu: 12.22 | total time: 484.72m
step 02111/21400 (9.86%) | loss: 3.275886 | grad norm: 0.1157 | lrm: 1.00 | dt: 13822.60ms | tok/sec: 37,929 | mfu: 12.19 | total time: 484.95m
step 02112/21400 (9.87%) | loss: 3.257169 | grad norm: 0.1213 | lrm: 1.00 | dt: 13798.14ms | tok/sec: 37,997 | mfu: 12.21 | total time: 485.18m
step 02113/21400 (9.87%) | loss: 3.195610 | grad norm: 0.1152 | lrm: 1.00 | dt: 13812.57ms | tok/sec: 37,957 | mfu: 12.19 | total time: 485.41m
step 02114/21400 (9.88%) | loss: 3.187082 | grad norm: 0.1115 | lrm: 1.00 | dt: 13817.99ms | tok/sec: 37,942 | mfu: 12.19 | total time: 485.64m
step 02115/21400 (9.88%) | loss: 3.206910 | grad norm: 0.1254 | lrm: 1.00 | dt: 13793.80ms | tok/sec: 38,008 | mfu: 12.21 | total time: 485.87m
step 02116/21400 (9.89%) | loss: 3.219499 | grad norm: 0.1185 | lrm: 1.00 | dt: 13858.96ms | tok/sec: 37,830 | mfu: 12.15 | total time: 486.10m
step 02117/21400 (9.89%) | loss: 3.210816 | grad norm: 0.1173 | lrm: 1.00 | dt: 13750.31ms | tok/sec: 38,129 | mfu: 12.25 | total time: 486.33m
step 02118/21400 (9.90%) | loss: 3.232414 | grad norm: 0.1371 | lrm: 1.00 | dt: 13844.59ms | tok/sec: 37,869 | mfu: 12.17 | total time: 486.56m
step 02119/21400 (9.90%) | loss: 3.232472 | grad norm: 0.1276 | lrm: 1.00 | dt: 13761.25ms | tok/sec: 38,098 | mfu: 12.24 | total time: 486.79m
step 02120/21400 (9.91%) | loss: 3.254730 | grad norm: 0.1062 | lrm: 1.00 | dt: 13836.99ms | tok/sec: 37,890 | mfu: 12.17 | total time: 487.02m
step 02121/21400 (9.91%) | loss: 3.262983 | grad norm: 0.1043 | lrm: 1.00 | dt: 13819.28ms | tok/sec: 37,938 | mfu: 12.19 | total time: 487.25m
step 02122/21400 (9.92%) | loss: 3.254787 | grad norm: 0.1225 | lrm: 1.00 | dt: 13775.27ms | tok/sec: 38,060 | mfu: 12.23 | total time: 487.48m
step 02123/21400 (9.92%) | loss: 3.228887 | grad norm: 0.1163 | lrm: 1.00 | dt: 14290.41ms | tok/sec: 36,688 | mfu: 11.79 | total time: 487.72m
step 02124/21400 (9.93%) | loss: 3.227365 | grad norm: 0.1052 | lrm: 1.00 | dt: 13822.40ms | tok/sec: 37,930 | mfu: 12.19 | total time: 487.95m
step 02125/21400 (9.93%) | loss: 3.189495 | grad norm: 0.1025 | lrm: 1.00 | dt: 13804.42ms | tok/sec: 37,979 | mfu: 12.20 | total time: 488.18m
step 02126/21400 (9.93%) | loss: 3.183829 | grad norm: 0.1155 | lrm: 1.00 | dt: 13824.73ms | tok/sec: 37,923 | mfu: 12.18 | total time: 488.41m
step 02127/21400 (9.94%) | loss: 3.193196 | grad norm: 0.1243 | lrm: 1.00 | dt: 13827.81ms | tok/sec: 37,915 | mfu: 12.18 | total time: 488.64m
step 02128/21400 (9.94%) | loss: 3.160870 | grad norm: 0.1190 | lrm: 1.00 | dt: 13791.14ms | tok/sec: 38,016 | mfu: 12.21 | total time: 488.87m
step 02129/21400 (9.95%) | loss: 3.160872 | grad norm: 0.1139 | lrm: 1.00 | dt: 13853.59ms | tok/sec: 37,844 | mfu: 12.16 | total time: 489.10m
step 02130/21400 (9.95%) | loss: 3.153459 | grad norm: 0.1179 | lrm: 1.00 | dt: 13757.20ms | tok/sec: 38,110 | mfu: 12.24 | total time: 489.33m
step 02131/21400 (9.96%) | loss: 3.147762 | grad norm: 0.1179 | lrm: 1.00 | dt: 13856.63ms | tok/sec: 37,836 | mfu: 12.16 | total time: 489.56m
step 02132/21400 (9.96%) | loss: 3.160743 | grad norm: 0.1207 | lrm: 1.00 | dt: 13758.36ms | tok/sec: 38,106 | mfu: 12.24 | total time: 489.79m
step 02133/21400 (9.97%) | loss: 3.158730 | grad norm: 0.1134 | lrm: 1.00 | dt: 13841.86ms | tok/sec: 37,876 | mfu: 12.17 | total time: 490.02m
step 02134/21400 (9.97%) | loss: 3.169205 | grad norm: 0.1088 | lrm: 1.00 | dt: 13773.89ms | tok/sec: 38,063 | mfu: 12.23 | total time: 490.25m
step 02135/21400 (9.98%) | loss: 3.176751 | grad norm: 0.1108 | lrm: 1.00 | dt: 13842.63ms | tok/sec: 37,874 | mfu: 12.17 | total time: 490.48m
step 02136/21400 (9.98%) | loss: 3.188250 | grad norm: 0.1123 | lrm: 1.00 | dt: 13773.07ms | tok/sec: 38,066 | mfu: 12.23 | total time: 490.71m
step 02137/21400 (9.99%) | loss: 3.196720 | grad norm: 0.1104 | lrm: 1.00 | dt: 13838.94ms | tok/sec: 37,884 | mfu: 12.17 | total time: 490.94m
step 02138/21400 (9.99%) | loss: 3.217270 | grad norm: 0.1023 | lrm: 1.00 | dt: 13790.49ms | tok/sec: 38,018 | mfu: 12.21 | total time: 491.17m
step 02139/21400 (10.00%) | loss: 3.201237 | grad norm: 0.1066 | lrm: 1.00 | dt: 13809.02ms | tok/sec: 37,967 | mfu: 12.20 | total time: 491.40m
step 02140/21400 (10.00%) | loss: 3.224646 | grad norm: 0.1105 | lrm: 1.00 | dt: 13818.90ms | tok/sec: 37,939 | mfu: 12.19 | total time: 491.63m
step 02141/21400 (10.00%) | loss: 3.184080 | grad norm: 0.1096 | lrm: 1.00 | dt: 13793.87ms | tok/sec: 38,008 | mfu: 12.21 | total time: 491.86m
step 02142/21400 (10.01%) | loss: 3.245408 | grad norm: 0.1266 | lrm: 1.00 | dt: 13870.54ms | tok/sec: 37,798 | mfu: 12.14 | total time: 492.10m
step 02143/21400 (10.01%) | loss: 3.252964 | grad norm: 0.1206 | lrm: 1.00 | dt: 13760.53ms | tok/sec: 38,100 | mfu: 12.24 | total time: 492.32m
step 02144/21400 (10.02%) | loss: 3.255814 | grad norm: 0.1248 | lrm: 1.00 | dt: 13851.21ms | tok/sec: 37,851 | mfu: 12.16 | total time: 492.56m
step 02145/21400 (10.02%) | loss: 3.225400 | grad norm: 0.1217 | lrm: 1.00 | dt: 13770.91ms | tok/sec: 38,072 | mfu: 12.23 | total time: 492.79m
step 02146/21400 (10.03%) | loss: 3.251668 | grad norm: 0.1292 | lrm: 1.00 | dt: 13846.04ms | tok/sec: 37,865 | mfu: 12.16 | total time: 493.02m
step 02147/21400 (10.03%) | loss: 3.242036 | grad norm: 0.1221 | lrm: 1.00 | dt: 13765.96ms | tok/sec: 38,085 | mfu: 12.24 | total time: 493.25m
step 02148/21400 (10.04%) | loss: 3.277700 | grad norm: 0.1163 | lrm: 1.00 | dt: 13834.42ms | tok/sec: 37,897 | mfu: 12.17 | total time: 493.48m
step 02149/21400 (10.04%) | loss: 3.267903 | grad norm: 0.1167 | lrm: 1.00 | dt: 13790.62ms | tok/sec: 38,017 | mfu: 12.21 | total time: 493.71m
step 02150/21400 (10.05%) | loss: 3.242988 | grad norm: 0.1107 | lrm: 1.00 | dt: 13828.99ms | tok/sec: 37,912 | mfu: 12.18 | total time: 493.94m
step 02151/21400 (10.05%) | loss: 3.307421 | grad norm: 0.1103 | lrm: 1.00 | dt: 13797.83ms | tok/sec: 37,997 | mfu: 12.21 | total time: 494.17m
step 02152/21400 (10.06%) | loss: 3.304425 | grad norm: 0.1147 | lrm: 1.00 | dt: 13819.74ms | tok/sec: 37,937 | mfu: 12.19 | total time: 494.40m
step 02153/21400 (10.06%) | loss: 3.301016 | grad norm: 0.1271 | lrm: 1.00 | dt: 13829.71ms | tok/sec: 37,910 | mfu: 12.18 | total time: 494.63m
step 02154/21400 (10.07%) | loss: 3.308125 | grad norm: 0.1183 | lrm: 1.00 | dt: 13799.11ms | tok/sec: 37,994 | mfu: 12.21 | total time: 494.86m
step 02155/21400 (10.07%) | loss: 3.285235 | grad norm: 0.1107 | lrm: 1.00 | dt: 13852.65ms | tok/sec: 37,847 | mfu: 12.16 | total time: 495.09m
step 02156/21400 (10.07%) | loss: 3.269559 | grad norm: 0.1103 | lrm: 1.00 | dt: 13754.42ms | tok/sec: 38,117 | mfu: 12.25 | total time: 495.32m
step 02157/21400 (10.08%) | loss: 3.252659 | grad norm: 0.1126 | lrm: 1.00 | dt: 13853.55ms | tok/sec: 37,845 | mfu: 12.16 | total time: 495.55m
step 02158/21400 (10.08%) | loss: 3.242902 | grad norm: 0.1172 | lrm: 1.00 | dt: 13754.35ms | tok/sec: 38,117 | mfu: 12.25 | total time: 495.78m
step 02159/21400 (10.09%) | loss: 3.222219 | grad norm: 0.1115 | lrm: 1.00 | dt: 13837.52ms | tok/sec: 37,888 | mfu: 12.17 | total time: 496.01m
step 02160/21400 (10.09%) | loss: 3.230133 | grad norm: 0.1484 | lrm: 1.00 | dt: 13775.83ms | tok/sec: 38,058 | mfu: 12.23 | total time: 496.24m
step 02161/21400 (10.10%) | loss: 3.230158 | grad norm: 0.1336 | lrm: 1.00 | dt: 13840.05ms | tok/sec: 37,881 | mfu: 12.17 | total time: 496.47m
step 02162/21400 (10.10%) | loss: 3.214071 | grad norm: 0.1163 | lrm: 1.00 | dt: 13789.62ms | tok/sec: 38,020 | mfu: 12.21 | total time: 496.70m
step 02163/21400 (10.11%) | loss: 3.202892 | grad norm: 0.1147 | lrm: 1.00 | dt: 13824.44ms | tok/sec: 37,924 | mfu: 12.18 | total time: 496.93m
step 02164/21400 (10.11%) | loss: 3.126732 | grad norm: 0.1095 | lrm: 1.00 | dt: 13797.47ms | tok/sec: 37,998 | mfu: 12.21 | total time: 497.16m
step 02165/21400 (10.12%) | loss: 3.121805 | grad norm: 0.1081 | lrm: 1.00 | dt: 13808.77ms | tok/sec: 37,967 | mfu: 12.20 | total time: 497.39m
step 02166/21400 (10.12%) | loss: 3.121024 | grad norm: 0.0964 | lrm: 1.00 | dt: 13817.42ms | tok/sec: 37,943 | mfu: 12.19 | total time: 497.62m
step 02167/21400 (10.13%) | loss: 3.093723 | grad norm: 0.0986 | lrm: 1.00 | dt: 13801.11ms | tok/sec: 37,988 | mfu: 12.20 | total time: 497.85m
step 02168/21400 (10.13%) | loss: 3.118969 | grad norm: 0.1215 | lrm: 1.00 | dt: 13870.43ms | tok/sec: 37,798 | mfu: 12.14 | total time: 498.08m
step 02169/21400 (10.14%) | loss: 3.112915 | grad norm: 0.1090 | lrm: 1.00 | dt: 13763.06ms | tok/sec: 38,093 | mfu: 12.24 | total time: 498.31m
step 02170/21400 (10.14%) | loss: 3.148539 | grad norm: 0.1064 | lrm: 1.00 | dt: 14403.47ms | tok/sec: 36,400 | mfu: 11.69 | total time: 498.55m
step 02171/21400 (10.14%) | loss: 3.113105 | grad norm: 0.1092 | lrm: 1.00 | dt: 13751.92ms | tok/sec: 38,124 | mfu: 12.25 | total time: 498.78m
step 02172/21400 (10.15%) | loss: 3.123142 | grad norm: 0.1069 | lrm: 1.00 | dt: 13845.77ms | tok/sec: 37,866 | mfu: 12.16 | total time: 499.01m
step 02173/21400 (10.15%) | loss: 3.143489 | grad norm: 0.1045 | lrm: 1.00 | dt: 13766.24ms | tok/sec: 38,085 | mfu: 12.23 | total time: 499.24m
step 02174/21400 (10.16%) | loss: 3.175345 | grad norm: 0.1397 | lrm: 1.00 | dt: 13833.13ms | tok/sec: 37,900 | mfu: 12.18 | total time: 499.47m
step 02175/21400 (10.16%) | loss: 3.186711 | grad norm: 0.1147 | lrm: 1.00 | dt: 13779.97ms | tok/sec: 38,047 | mfu: 12.22 | total time: 499.70m
step 02176/21400 (10.17%) | loss: 3.192215 | grad norm: 0.1118 | lrm: 1.00 | dt: 13820.48ms | tok/sec: 37,935 | mfu: 12.19 | total time: 499.93m
step 02177/21400 (10.17%) | loss: 3.174225 | grad norm: 0.1058 | lrm: 1.00 | dt: 13791.18ms | tok/sec: 38,016 | mfu: 12.21 | total time: 500.16m
step 02178/21400 (10.18%) | loss: 3.155763 | grad norm: 0.1192 | lrm: 1.00 | dt: 13804.74ms | tok/sec: 37,978 | mfu: 12.20 | total time: 500.39m
step 02179/21400 (10.18%) | loss: 3.156367 | grad norm: 0.1218 | lrm: 1.00 | dt: 13816.85ms | tok/sec: 37,945 | mfu: 12.19 | total time: 500.62m
step 02180/21400 (10.19%) | loss: 3.202105 | grad norm: 0.1062 | lrm: 1.00 | dt: 13788.87ms | tok/sec: 38,022 | mfu: 12.21 | total time: 500.85m
step 02181/21400 (10.19%) | loss: 3.199654 | grad norm: 0.1049 | lrm: 1.00 | dt: 13852.82ms | tok/sec: 37,847 | mfu: 12.16 | total time: 501.08m
step 02182/21400 (10.20%) | loss: 3.191512 | grad norm: 0.1119 | lrm: 1.00 | dt: 13754.83ms | tok/sec: 38,116 | mfu: 12.25 | total time: 501.31m
step 02183/21400 (10.20%) | loss: 3.208929 | grad norm: 0.1104 | lrm: 1.00 | dt: 13860.15ms | tok/sec: 37,826 | mfu: 12.15 | total time: 501.54m
step 02184/21400 (10.21%) | loss: 3.200071 | grad norm: 0.1139 | lrm: 1.00 | dt: 13757.01ms | tok/sec: 38,110 | mfu: 12.24 | total time: 501.77m
step 02185/21400 (10.21%) | loss: 3.207342 | grad norm: 0.1105 | lrm: 1.00 | dt: 13841.01ms | tok/sec: 37,879 | mfu: 12.17 | total time: 502.00m
step 02186/21400 (10.21%) | loss: 3.259060 | grad norm: 0.1109 | lrm: 1.00 | dt: 13779.34ms | tok/sec: 38,048 | mfu: 12.22 | total time: 502.23m
step 02187/21400 (10.22%) | loss: 3.247204 | grad norm: 0.1152 | lrm: 1.00 | dt: 13832.93ms | tok/sec: 37,901 | mfu: 12.18 | total time: 502.46m
step 02188/21400 (10.22%) | loss: 3.220022 | grad norm: 0.1082 | lrm: 1.00 | dt: 13786.07ms | tok/sec: 38,030 | mfu: 12.22 | total time: 502.69m
step 02189/21400 (10.23%) | loss: 3.181924 | grad norm: 0.1093 | lrm: 1.00 | dt: 13824.85ms | tok/sec: 37,923 | mfu: 12.18 | total time: 502.92m
step 02190/21400 (10.23%) | loss: 3.212690 | grad norm: 0.1200 | lrm: 1.00 | dt: 13816.37ms | tok/sec: 37,946 | mfu: 12.19 | total time: 503.15m
step 02191/21400 (10.24%) | loss: 3.206926 | grad norm: 0.1056 | lrm: 1.00 | dt: 13804.83ms | tok/sec: 37,978 | mfu: 12.20 | total time: 503.38m
step 02192/21400 (10.24%) | loss: 3.175859 | grad norm: 0.1094 | lrm: 1.00 | dt: 13819.58ms | tok/sec: 37,938 | mfu: 12.19 | total time: 503.61m
step 02193/21400 (10.25%) | loss: 3.204157 | grad norm: 0.1178 | lrm: 1.00 | dt: 13786.91ms | tok/sec: 38,027 | mfu: 12.22 | total time: 503.84m
step 02194/21400 (10.25%) | loss: 3.205403 | grad norm: 0.1124 | lrm: 1.00 | dt: 13851.64ms | tok/sec: 37,850 | mfu: 12.16 | total time: 504.07m
step 02195/21400 (10.26%) | loss: 3.194954 | grad norm: 0.1118 | lrm: 1.00 | dt: 13754.38ms | tok/sec: 38,117 | mfu: 12.25 | total time: 504.30m
step 02196/21400 (10.26%) | loss: 3.197745 | grad norm: 0.1116 | lrm: 1.00 | dt: 13849.85ms | tok/sec: 37,855 | mfu: 12.16 | total time: 504.53m
step 02197/21400 (10.27%) | loss: 3.205844 | grad norm: 0.1036 | lrm: 1.00 | dt: 13769.71ms | tok/sec: 38,075 | mfu: 12.23 | total time: 504.76m
step 02198/21400 (10.27%) | loss: 3.189659 | grad norm: 0.1050 | lrm: 1.00 | dt: 13843.17ms | tok/sec: 37,873 | mfu: 12.17 | total time: 504.99m
step 02199/21400 (10.28%) | loss: 3.202662 | grad norm: 0.1317 | lrm: 1.00 | dt: 13767.27ms | tok/sec: 38,082 | mfu: 12.23 | total time: 505.22m
step 02200/21400 (10.28%) | loss: 3.195316 | grad norm: 0.1302 | lrm: 1.00 | dt: 13832.31ms | tok/sec: 37,903 | mfu: 12.18 | total time: 505.45m
step 02201/21400 (10.29%) | loss: 3.194367 | grad norm: 0.1146 | lrm: 1.00 | dt: 13784.69ms | tok/sec: 38,034 | mfu: 12.22 | total time: 505.68m
step 02202/21400 (10.29%) | loss: 3.198770 | grad norm: 0.1046 | lrm: 1.00 | dt: 13815.92ms | tok/sec: 37,948 | mfu: 12.19 | total time: 505.91m
step 02203/21400 (10.29%) | loss: 3.194081 | grad norm: 0.1040 | lrm: 1.00 | dt: 13793.78ms | tok/sec: 38,009 | mfu: 12.21 | total time: 506.14m
step 02204/21400 (10.30%) | loss: 3.204206 | grad norm: 0.1024 | lrm: 1.00 | dt: 13805.27ms | tok/sec: 37,977 | mfu: 12.20 | total time: 506.37m
step 02205/21400 (10.30%) | loss: 3.208763 | grad norm: 0.1043 | lrm: 1.00 | dt: 13814.84ms | tok/sec: 37,951 | mfu: 12.19 | total time: 506.60m
step 02206/21400 (10.31%) | loss: 3.220392 | grad norm: 0.1096 | lrm: 1.00 | dt: 13797.14ms | tok/sec: 37,999 | mfu: 12.21 | total time: 506.83m
step 02207/21400 (10.31%) | loss: 3.214554 | grad norm: 0.1179 | lrm: 1.00 | dt: 13856.09ms | tok/sec: 37,838 | mfu: 12.16 | total time: 507.06m
step 02208/21400 (10.32%) | loss: 3.281286 | grad norm: 0.1140 | lrm: 1.00 | dt: 13749.67ms | tok/sec: 38,130 | mfu: 12.25 | total time: 507.29m
step 02209/21400 (10.32%) | loss: 3.234327 | grad norm: 0.1118 | lrm: 1.00 | dt: 13855.63ms | tok/sec: 37,839 | mfu: 12.16 | total time: 507.52m
step 02210/21400 (10.33%) | loss: 3.221448 | grad norm: 0.1096 | lrm: 1.00 | dt: 13774.34ms | tok/sec: 38,062 | mfu: 12.23 | total time: 507.75m
step 02211/21400 (10.33%) | loss: 3.214314 | grad norm: 0.1126 | lrm: 1.00 | dt: 13865.35ms | tok/sec: 37,812 | mfu: 12.15 | total time: 507.98m
step 02212/21400 (10.34%) | loss: 3.239958 | grad norm: 0.1197 | lrm: 1.00 | dt: 13781.68ms | tok/sec: 38,042 | mfu: 12.22 | total time: 508.21m
step 02213/21400 (10.34%) | loss: 3.271922 | grad norm: 0.1211 | lrm: 1.00 | dt: 13835.07ms | tok/sec: 37,895 | mfu: 12.17 | total time: 508.44m
step 02214/21400 (10.35%) | loss: 3.279777 | grad norm: 0.1244 | lrm: 1.00 | dt: 13782.50ms | tok/sec: 38,040 | mfu: 12.22 | total time: 508.67m
step 02215/21400 (10.35%) | loss: 3.277068 | grad norm: 0.1234 | lrm: 1.00 | dt: 13827.25ms | tok/sec: 37,917 | mfu: 12.18 | total time: 508.91m
step 02216/21400 (10.36%) | loss: 3.273737 | grad norm: 0.1138 | lrm: 1.00 | dt: 13811.17ms | tok/sec: 37,961 | mfu: 12.20 | total time: 509.14m
step 02217/21400 (10.36%) | loss: 3.238738 | grad norm: 0.1156 | lrm: 1.00 | dt: 14305.28ms | tok/sec: 36,649 | mfu: 11.77 | total time: 509.37m
step 02218/21400 (10.36%) | loss: 3.258964 | grad norm: 0.1147 | lrm: 1.00 | dt: 13815.90ms | tok/sec: 37,948 | mfu: 12.19 | total time: 509.60m
step 02219/21400 (10.37%) | loss: 3.270374 | grad norm: 0.1185 | lrm: 1.00 | dt: 13785.23ms | tok/sec: 38,032 | mfu: 12.22 | total time: 509.83m
step 02220/21400 (10.37%) | loss: 3.258497 | grad norm: 0.1233 | lrm: 1.00 | dt: 13853.95ms | tok/sec: 37,843 | mfu: 12.16 | total time: 510.06m
step 02221/21400 (10.38%) | loss: 3.249871 | grad norm: 0.1187 | lrm: 1.00 | dt: 13751.69ms | tok/sec: 38,125 | mfu: 12.25 | total time: 510.29m
step 02222/21400 (10.38%) | loss: 3.234931 | grad norm: 0.1092 | lrm: 1.00 | dt: 13849.47ms | tok/sec: 37,856 | mfu: 12.16 | total time: 510.52m
step 02223/21400 (10.39%) | loss: 3.242760 | grad norm: 0.1273 | lrm: 1.00 | dt: 13762.96ms | tok/sec: 38,094 | mfu: 12.24 | total time: 510.75m
step 02224/21400 (10.39%) | loss: 3.210282 | grad norm: 0.1019 | lrm: 1.00 | dt: 13839.58ms | tok/sec: 37,883 | mfu: 12.17 | total time: 510.98m
step 02225/21400 (10.40%) | loss: 3.148126 | grad norm: 0.1063 | lrm: 1.00 | dt: 13777.82ms | tok/sec: 38,053 | mfu: 12.22 | total time: 511.21m
step 02226/21400 (10.40%) | loss: 3.172390 | grad norm: 0.1321 | lrm: 1.00 | dt: 13828.66ms | tok/sec: 37,913 | mfu: 12.18 | total time: 511.44m
step 02227/21400 (10.41%) | loss: 3.106273 | grad norm: 0.1177 | lrm: 1.00 | dt: 13780.90ms | tok/sec: 38,044 | mfu: 12.22 | total time: 511.67m
step 02228/21400 (10.41%) | loss: 3.085617 | grad norm: 0.1219 | lrm: 1.00 | dt: 13815.32ms | tok/sec: 37,949 | mfu: 12.19 | total time: 511.90m
step 02229/21400 (10.42%) | loss: 3.107224 | grad norm: 0.1233 | lrm: 1.00 | dt: 13799.15ms | tok/sec: 37,994 | mfu: 12.21 | total time: 512.13m
step 02230/21400 (10.42%) | loss: 3.105389 | grad norm: 0.1101 | lrm: 1.00 | dt: 13805.05ms | tok/sec: 37,977 | mfu: 12.20 | total time: 512.36m
step 02231/21400 (10.43%) | loss: 3.138998 | grad norm: 0.1112 | lrm: 1.00 | dt: 13812.09ms | tok/sec: 37,958 | mfu: 12.19 | total time: 512.59m
step 02232/21400 (10.43%) | loss: 3.157267 | grad norm: 0.1223 | lrm: 1.00 | dt: 13796.27ms | tok/sec: 38,002 | mfu: 12.21 | total time: 512.82m
step 02233/21400 (10.43%) | loss: 3.148040 | grad norm: 0.1278 | lrm: 1.00 | dt: 13850.33ms | tok/sec: 37,853 | mfu: 12.16 | total time: 513.06m
step 02234/21400 (10.44%) | loss: 3.147850 | grad norm: 0.1236 | lrm: 1.00 | dt: 13749.39ms | tok/sec: 38,131 | mfu: 12.25 | total time: 513.28m
step 02235/21400 (10.44%) | loss: 3.122073 | grad norm: 0.1167 | lrm: 1.00 | dt: 13833.98ms | tok/sec: 37,898 | mfu: 12.18 | total time: 513.52m
step 02236/21400 (10.45%) | loss: 3.156621 | grad norm: 0.1508 | lrm: 1.00 | dt: 13768.60ms | tok/sec: 38,078 | mfu: 12.23 | total time: 513.74m
step 02237/21400 (10.45%) | loss: 3.173036 | grad norm: 0.1248 | lrm: 1.00 | dt: 13843.55ms | tok/sec: 37,872 | mfu: 12.17 | total time: 513.98m
step 02238/21400 (10.46%) | loss: 3.127332 | grad norm: 0.1069 | lrm: 1.00 | dt: 13782.12ms | tok/sec: 38,041 | mfu: 12.22 | total time: 514.21m
step 02239/21400 (10.46%) | loss: 3.124170 | grad norm: 0.1209 | lrm: 1.00 | dt: 13837.46ms | tok/sec: 37,889 | mfu: 12.17 | total time: 514.44m
step 02240/21400 (10.47%) | loss: 3.087739 | grad norm: 0.1116 | lrm: 1.00 | dt: 13790.94ms | tok/sec: 38,016 | mfu: 12.21 | total time: 514.67m
step 02241/21400 (10.47%) | loss: 3.102566 | grad norm: 0.1136 | lrm: 1.00 | dt: 13817.01ms | tok/sec: 37,945 | mfu: 12.19 | total time: 514.90m
step 02242/21400 (10.48%) | loss: 3.119102 | grad norm: 0.1189 | lrm: 1.00 | dt: 13805.47ms | tok/sec: 37,976 | mfu: 12.20 | total time: 515.13m
step 02243/21400 (10.48%) | loss: 3.141323 | grad norm: 0.1127 | lrm: 1.00 | dt: 13804.11ms | tok/sec: 37,980 | mfu: 12.20 | total time: 515.36m
step 02244/21400 (10.49%) | loss: 3.091589 | grad norm: 0.1098 | lrm: 1.00 | dt: 13854.73ms | tok/sec: 37,841 | mfu: 12.16 | total time: 515.59m
step 02245/21400 (10.49%) | loss: 3.114481 | grad norm: 0.1146 | lrm: 1.00 | dt: 13754.99ms | tok/sec: 38,116 | mfu: 12.24 | total time: 515.82m
step 02246/21400 (10.50%) | loss: 3.156506 | grad norm: 0.1057 | lrm: 1.00 | dt: 13845.66ms | tok/sec: 37,866 | mfu: 12.16 | total time: 516.05m
step 02247/21400 (10.50%) | loss: 3.166877 | grad norm: 0.1041 | lrm: 1.00 | dt: 13754.18ms | tok/sec: 38,118 | mfu: 12.25 | total time: 516.28m
step 02248/21400 (10.50%) | loss: 3.146852 | grad norm: 0.1052 | lrm: 1.00 | dt: 13837.29ms | tok/sec: 37,889 | mfu: 12.17 | total time: 516.51m
step 02249/21400 (10.51%) | loss: 3.176538 | grad norm: 0.1055 | lrm: 1.00 | dt: 13766.77ms | tok/sec: 38,083 | mfu: 12.23 | total time: 516.74m
Step 02250 | Validation bpb: 0.9506
step 02250/21400 (10.51%) | loss: 3.151512 | grad norm: 0.1054 | lrm: 1.00 | dt: 13822.80ms | tok/sec: 37,929 | mfu: 12.18 | total time: 516.97m
step 02251/21400 (10.52%) | loss: 3.181755 | grad norm: 0.1082 | lrm: 1.00 | dt: 13784.55ms | tok/sec: 38,034 | mfu: 12.22 | total time: 517.20m
step 02252/21400 (10.52%) | loss: 3.165693 | grad norm: 0.1073 | lrm: 1.00 | dt: 13806.62ms | tok/sec: 37,973 | mfu: 12.20 | total time: 517.43m
step 02253/21400 (10.53%) | loss: 3.167051 | grad norm: 0.1182 | lrm: 1.00 | dt: 13813.35ms | tok/sec: 37,955 | mfu: 12.19 | total time: 517.66m
step 02254/21400 (10.53%) | loss: 3.244645 | grad norm: 0.1267 | lrm: 1.00 | dt: 13791.69ms | tok/sec: 38,014 | mfu: 12.21 | total time: 517.89m
step 02255/21400 (10.54%) | loss: 3.272807 | grad norm: 0.1235 | lrm: 1.00 | dt: 13851.78ms | tok/sec: 37,849 | mfu: 12.16 | total time: 518.12m
step 02256/21400 (10.54%) | loss: 3.261599 | grad norm: 0.1156 | lrm: 1.00 | dt: 13750.78ms | tok/sec: 38,127 | mfu: 12.25 | total time: 518.35m
step 02257/21400 (10.55%) | loss: 3.264764 | grad norm: 0.1135 | lrm: 1.00 | dt: 13851.43ms | tok/sec: 37,850 | mfu: 12.16 | total time: 518.58m
step 02258/21400 (10.55%) | loss: 3.233631 | grad norm: 0.1182 | lrm: 1.00 | dt: 13755.69ms | tok/sec: 38,114 | mfu: 12.24 | total time: 518.81m
step 02259/21400 (10.56%) | loss: 3.202094 | grad norm: 0.1170 | lrm: 1.00 | dt: 13830.43ms | tok/sec: 37,908 | mfu: 12.18 | total time: 519.04m
step 02260/21400 (10.56%) | loss: 3.170512 | grad norm: 0.1075 | lrm: 1.00 | dt: 13775.30ms | tok/sec: 38,060 | mfu: 12.23 | total time: 519.27m
step 02261/21400 (10.57%) | loss: 3.171817 | grad norm: 0.1058 | lrm: 1.00 | dt: 13832.71ms | tok/sec: 37,902 | mfu: 12.18 | total time: 519.50m
step 02262/21400 (10.57%) | loss: 3.165544 | grad norm: 0.0991 | lrm: 1.00 | dt: 13782.55ms | tok/sec: 38,039 | mfu: 12.22 | total time: 519.73m
step 02263/21400 (10.57%) | loss: 3.196905 | grad norm: 0.1039 | lrm: 1.00 | dt: 14365.85ms | tok/sec: 36,495 | mfu: 11.72 | total time: 519.97m
step 02264/21400 (10.58%) | loss: 3.195123 | grad norm: 0.1079 | lrm: 1.00 | dt: 13782.71ms | tok/sec: 38,039 | mfu: 12.22 | total time: 520.20m
step 02265/21400 (10.58%) | loss: 3.176436 | grad norm: 0.1078 | lrm: 1.00 | dt: 13813.46ms | tok/sec: 37,954 | mfu: 12.19 | total time: 520.43m
step 02266/21400 (10.59%) | loss: 3.155025 | grad norm: 0.1398 | lrm: 1.00 | dt: 13828.39ms | tok/sec: 37,913 | mfu: 12.18 | total time: 520.66m
step 02267/21400 (10.59%) | loss: 3.094719 | grad norm: 0.1366 | lrm: 1.00 | dt: 13818.02ms | tok/sec: 37,942 | mfu: 12.19 | total time: 520.89m
step 02268/21400 (10.60%) | loss: 3.104698 | grad norm: 0.1329 | lrm: 1.00 | dt: 13864.16ms | tok/sec: 37,816 | mfu: 12.15 | total time: 521.12m
step 02269/21400 (10.60%) | loss: 3.084169 | grad norm: 0.1143 | lrm: 1.00 | dt: 13759.52ms | tok/sec: 38,103 | mfu: 12.24 | total time: 521.35m
step 02270/21400 (10.61%) | loss: 3.100491 | grad norm: 0.1047 | lrm: 1.00 | dt: 13856.48ms | tok/sec: 37,837 | mfu: 12.16 | total time: 521.58m
step 02271/21400 (10.61%) | loss: 3.074577 | grad norm: 0.1118 | lrm: 1.00 | dt: 13768.93ms | tok/sec: 38,077 | mfu: 12.23 | total time: 521.81m
step 02272/21400 (10.62%) | loss: 3.124356 | grad norm: 0.1072 | lrm: 1.00 | dt: 13853.53ms | tok/sec: 37,845 | mfu: 12.16 | total time: 522.04m
step 02273/21400 (10.62%) | loss: 3.099358 | grad norm: 0.1116 | lrm: 1.00 | dt: 13763.04ms | tok/sec: 38,093 | mfu: 12.24 | total time: 522.27m
step 02274/21400 (10.63%) | loss: 3.096083 | grad norm: 0.1150 | lrm: 1.00 | dt: 13837.01ms | tok/sec: 37,890 | mfu: 12.17 | total time: 522.50m
step 02275/21400 (10.63%) | loss: 3.093874 | grad norm: 0.1166 | lrm: 1.00 | dt: 13798.15ms | tok/sec: 37,996 | mfu: 12.21 | total time: 522.73m
step 02276/21400 (10.64%) | loss: 3.119290 | grad norm: 0.1212 | lrm: 1.00 | dt: 13849.12ms | tok/sec: 37,857 | mfu: 12.16 | total time: 522.96m
step 02277/21400 (10.64%) | loss: 3.107199 | grad norm: 0.1115 | lrm: 1.00 | dt: 13793.53ms | tok/sec: 38,009 | mfu: 12.21 | total time: 523.19m
step 02278/21400 (10.64%) | loss: 3.120348 | grad norm: 0.1064 | lrm: 1.00 | dt: 13809.88ms | tok/sec: 37,964 | mfu: 12.20 | total time: 523.42m
step 02279/21400 (10.65%) | loss: 3.153451 | grad norm: 0.1107 | lrm: 1.00 | dt: 13821.30ms | tok/sec: 37,933 | mfu: 12.19 | total time: 523.65m
step 02280/21400 (10.65%) | loss: 3.166900 | grad norm: 0.1122 | lrm: 1.00 | dt: 13797.13ms | tok/sec: 37,999 | mfu: 12.21 | total time: 523.88m
step 02281/21400 (10.66%) | loss: 3.184543 | grad norm: 0.0987 | lrm: 1.00 | dt: 13857.59ms | tok/sec: 37,833 | mfu: 12.15 | total time: 524.11m
step 02282/21400 (10.66%) | loss: 3.203838 | grad norm: 0.1017 | lrm: 1.00 | dt: 13761.47ms | tok/sec: 38,098 | mfu: 12.24 | total time: 524.34m
step 02283/21400 (10.67%) | loss: 3.222497 | grad norm: 0.1095 | lrm: 1.00 | dt: 13859.61ms | tok/sec: 37,828 | mfu: 12.15 | total time: 524.57m
step 02284/21400 (10.67%) | loss: 3.205163 | grad norm: 0.1027 | lrm: 1.00 | dt: 13760.79ms | tok/sec: 38,100 | mfu: 12.24 | total time: 524.80m
step 02285/21400 (10.68%) | loss: 3.217603 | grad norm: 0.1093 | lrm: 1.00 | dt: 13849.81ms | tok/sec: 37,855 | mfu: 12.16 | total time: 525.03m
step 02286/21400 (10.68%) | loss: 3.193829 | grad norm: 0.1140 | lrm: 1.00 | dt: 13773.94ms | tok/sec: 38,063 | mfu: 12.23 | total time: 525.26m
step 02287/21400 (10.69%) | loss: 3.264874 | grad norm: 0.1066 | lrm: 1.00 | dt: 13836.85ms | tok/sec: 37,890 | mfu: 12.17 | total time: 525.49m
step 02288/21400 (10.69%) | loss: 3.252131 | grad norm: 0.1152 | lrm: 1.00 | dt: 13785.66ms | tok/sec: 38,031 | mfu: 12.22 | total time: 525.72m
step 02289/21400 (10.70%) | loss: 3.253756 | grad norm: 0.1196 | lrm: 1.00 | dt: 13824.38ms | tok/sec: 37,924 | mfu: 12.18 | total time: 525.95m
step 02290/21400 (10.70%) | loss: 3.252598 | grad norm: 0.1233 | lrm: 1.00 | dt: 13800.30ms | tok/sec: 37,991 | mfu: 12.20 | total time: 526.18m
step 02291/21400 (10.71%) | loss: 3.232058 | grad norm: 0.1104 | lrm: 1.00 | dt: 13809.63ms | tok/sec: 37,965 | mfu: 12.20 | total time: 526.41m
step 02292/21400 (10.71%) | loss: 3.256178 | grad norm: 0.1049 | lrm: 1.00 | dt: 13826.88ms | tok/sec: 37,918 | mfu: 12.18 | total time: 526.64m
step 02293/21400 (10.71%) | loss: 3.284497 | grad norm: 0.1029 | lrm: 1.00 | dt: 13792.49ms | tok/sec: 38,012 | mfu: 12.21 | total time: 526.87m
step 02294/21400 (10.72%) | loss: 3.246354 | grad norm: 0.0978 | lrm: 1.00 | dt: 13855.90ms | tok/sec: 37,838 | mfu: 12.16 | total time: 527.10m
step 02295/21400 (10.72%) | loss: 3.245352 | grad norm: 0.0929 | lrm: 1.00 | dt: 13749.61ms | tok/sec: 38,131 | mfu: 12.25 | total time: 527.33m
step 02296/21400 (10.73%) | loss: 3.239874 | grad norm: 0.0951 | lrm: 1.00 | dt: 13852.22ms | tok/sec: 37,848 | mfu: 12.16 | total time: 527.56m
step 02297/21400 (10.73%) | loss: 3.244596 | grad norm: 0.1049 | lrm: 1.00 | dt: 13757.02ms | tok/sec: 38,110 | mfu: 12.24 | total time: 527.79m
step 02298/21400 (10.74%) | loss: 3.198683 | grad norm: 0.1107 | lrm: 1.00 | dt: 13840.29ms | tok/sec: 37,881 | mfu: 12.17 | total time: 528.02m
step 02299/21400 (10.74%) | loss: 3.244515 | grad norm: 0.1106 | lrm: 1.00 | dt: 13770.51ms | tok/sec: 38,073 | mfu: 12.23 | total time: 528.25m
step 02300/21400 (10.75%) | loss: 3.206969 | grad norm: 0.1101 | lrm: 1.00 | dt: 13836.05ms | tok/sec: 37,892 | mfu: 12.17 | total time: 528.48m
step 02301/21400 (10.75%) | loss: 3.226201 | grad norm: 0.1159 | lrm: 1.00 | dt: 13779.36ms | tok/sec: 38,048 | mfu: 12.22 | total time: 528.71m
step 02302/21400 (10.76%) | loss: 3.215169 | grad norm: 0.1270 | lrm: 1.00 | dt: 13819.53ms | tok/sec: 37,938 | mfu: 12.19 | total time: 528.94m
step 02303/21400 (10.76%) | loss: 3.199128 | grad norm: 0.1311 | lrm: 1.00 | dt: 13807.41ms | tok/sec: 37,971 | mfu: 12.20 | total time: 529.17m
step 02304/21400 (10.77%) | loss: 3.250603 | grad norm: 0.1271 | lrm: 1.00 | dt: 13800.94ms | tok/sec: 37,989 | mfu: 12.20 | total time: 529.40m
step 02305/21400 (10.77%) | loss: 3.249960 | grad norm: 0.1279 | lrm: 1.00 | dt: 13808.72ms | tok/sec: 37,967 | mfu: 12.20 | total time: 529.63m
step 02306/21400 (10.78%) | loss: 3.230160 | grad norm: 0.1190 | lrm: 1.00 | dt: 13791.94ms | tok/sec: 38,014 | mfu: 12.21 | total time: 529.86m
step 02307/21400 (10.78%) | loss: 3.221621 | grad norm: 0.1115 | lrm: 1.00 | dt: 13855.74ms | tok/sec: 37,839 | mfu: 12.16 | total time: 530.09m
step 02308/21400 (10.79%) | loss: 3.232152 | grad norm: 0.1224 | lrm: 1.00 | dt: 13744.26ms | tok/sec: 38,145 | mfu: 12.25 | total time: 530.32m
step 02309/21400 (10.79%) | loss: 3.227850 | grad norm: 0.1212 | lrm: 1.00 | dt: 13856.10ms | tok/sec: 37,838 | mfu: 12.16 | total time: 530.55m
step 02310/21400 (10.79%) | loss: 3.251552 | grad norm: 0.1100 | lrm: 1.00 | dt: 13757.23ms | tok/sec: 38,110 | mfu: 12.24 | total time: 530.78m
step 02311/21400 (10.80%) | loss: 3.253596 | grad norm: 0.1151 | lrm: 1.00 | dt: 13852.02ms | tok/sec: 37,849 | mfu: 12.16 | total time: 531.01m
step 02312/21400 (10.80%) | loss: 3.211021 | grad norm: 0.1200 | lrm: 1.00 | dt: 14341.04ms | tok/sec: 36,558 | mfu: 11.74 | total time: 531.25m
step 02313/21400 (10.81%) | loss: 3.202167 | grad norm: 0.1028 | lrm: 1.00 | dt: 13836.61ms | tok/sec: 37,891 | mfu: 12.17 | total time: 531.48m
step 02314/21400 (10.81%) | loss: 3.180934 | grad norm: 0.1062 | lrm: 1.00 | dt: 13772.46ms | tok/sec: 38,067 | mfu: 12.23 | total time: 531.71m
step 02315/21400 (10.82%) | loss: 3.194084 | grad norm: 0.1147 | lrm: 1.00 | dt: 13824.33ms | tok/sec: 37,925 | mfu: 12.18 | total time: 531.94m
step 02316/21400 (10.82%) | loss: 3.213937 | grad norm: 0.1041 | lrm: 1.00 | dt: 13800.68ms | tok/sec: 37,990 | mfu: 12.20 | total time: 532.17m
step 02317/21400 (10.83%) | loss: 3.222547 | grad norm: 0.1036 | lrm: 1.00 | dt: 13813.48ms | tok/sec: 37,954 | mfu: 12.19 | total time: 532.40m
step 02318/21400 (10.83%) | loss: 3.237763 | grad norm: 0.1078 | lrm: 1.00 | dt: 13811.18ms | tok/sec: 37,961 | mfu: 12.20 | total time: 532.63m
step 02319/21400 (10.84%) | loss: 3.219448 | grad norm: 0.0986 | lrm: 1.00 | dt: 13782.91ms | tok/sec: 38,038 | mfu: 12.22 | total time: 532.86m
step 02320/21400 (10.84%) | loss: 3.203576 | grad norm: 0.1011 | lrm: 1.00 | dt: 13861.50ms | tok/sec: 37,823 | mfu: 12.15 | total time: 533.10m
step 02321/21400 (10.85%) | loss: 3.202301 | grad norm: 0.1132 | lrm: 1.00 | dt: 13757.63ms | tok/sec: 38,108 | mfu: 12.24 | total time: 533.32m
step 02322/21400 (10.85%) | loss: 3.218065 | grad norm: 0.1142 | lrm: 1.00 | dt: 13849.94ms | tok/sec: 37,854 | mfu: 12.16 | total time: 533.56m
step 02323/21400 (10.86%) | loss: 3.223044 | grad norm: 0.1043 | lrm: 1.00 | dt: 13762.30ms | tok/sec: 38,095 | mfu: 12.24 | total time: 533.79m
step 02324/21400 (10.86%) | loss: 3.220935 | grad norm: 0.0993 | lrm: 1.00 | dt: 13848.81ms | tok/sec: 37,857 | mfu: 12.16 | total time: 534.02m
step 02325/21400 (10.86%) | loss: 3.160153 | grad norm: 0.0995 | lrm: 1.00 | dt: 13757.39ms | tok/sec: 38,109 | mfu: 12.24 | total time: 534.25m
step 02326/21400 (10.87%) | loss: 3.194313 | grad norm: 0.1192 | lrm: 1.00 | dt: 13833.84ms | tok/sec: 37,898 | mfu: 12.18 | total time: 534.48m
step 02327/21400 (10.87%) | loss: 3.214577 | grad norm: 0.1300 | lrm: 1.00 | dt: 13777.11ms | tok/sec: 38,054 | mfu: 12.23 | total time: 534.71m
step 02328/21400 (10.88%) | loss: 3.202054 | grad norm: 0.1146 | lrm: 1.00 | dt: 13823.90ms | tok/sec: 37,926 | mfu: 12.18 | total time: 534.94m
step 02329/21400 (10.88%) | loss: 3.247987 | grad norm: 0.1110 | lrm: 1.00 | dt: 13801.94ms | tok/sec: 37,986 | mfu: 12.20 | total time: 535.17m
step 02330/21400 (10.89%) | loss: 3.269812 | grad norm: 0.1141 | lrm: 1.00 | dt: 13809.03ms | tok/sec: 37,967 | mfu: 12.20 | total time: 535.40m
step 02331/21400 (10.89%) | loss: 3.242531 | grad norm: 0.1112 | lrm: 1.00 | dt: 13826.61ms | tok/sec: 37,918 | mfu: 12.18 | total time: 535.63m
step 02332/21400 (10.90%) | loss: 3.239416 | grad norm: 0.1030 | lrm: 1.00 | dt: 13797.29ms | tok/sec: 37,999 | mfu: 12.21 | total time: 535.86m
step 02333/21400 (10.90%) | loss: 3.240479 | grad norm: 0.1166 | lrm: 1.00 | dt: 13859.17ms | tok/sec: 37,829 | mfu: 12.15 | total time: 536.09m
step 02334/21400 (10.91%) | loss: 3.217411 | grad norm: 0.1041 | lrm: 1.00 | dt: 13752.59ms | tok/sec: 38,122 | mfu: 12.25 | total time: 536.32m
step 02335/21400 (10.91%) | loss: 3.198770 | grad norm: 0.0903 | lrm: 1.00 | dt: 13865.04ms | tok/sec: 37,813 | mfu: 12.15 | total time: 536.55m
step 02336/21400 (10.92%) | loss: 3.201132 | grad norm: 0.1051 | lrm: 1.00 | dt: 13776.75ms | tok/sec: 38,055 | mfu: 12.23 | total time: 536.78m
step 02337/21400 (10.92%) | loss: 3.222660 | grad norm: 0.1025 | lrm: 1.00 | dt: 13847.74ms | tok/sec: 37,860 | mfu: 12.16 | total time: 537.01m
step 02338/21400 (10.93%) | loss: 3.257231 | grad norm: 0.1112 | lrm: 1.00 | dt: 13773.38ms | tok/sec: 38,065 | mfu: 12.23 | total time: 537.24m
step 02339/21400 (10.93%) | loss: 3.290031 | grad norm: 0.1113 | lrm: 1.00 | dt: 13843.12ms | tok/sec: 37,873 | mfu: 12.17 | total time: 537.47m
step 02340/21400 (10.93%) | loss: 3.274900 | grad norm: 0.1130 | lrm: 1.00 | dt: 13782.55ms | tok/sec: 38,039 | mfu: 12.22 | total time: 537.70m
step 02341/21400 (10.94%) | loss: 3.242972 | grad norm: 0.1131 | lrm: 1.00 | dt: 13824.03ms | tok/sec: 37,925 | mfu: 12.18 | total time: 537.93m
step 02342/21400 (10.94%) | loss: 3.220306 | grad norm: 0.1083 | lrm: 1.00 | dt: 13802.78ms | tok/sec: 37,984 | mfu: 12.20 | total time: 538.16m
step 02343/21400 (10.95%) | loss: 3.229519 | grad norm: 0.1080 | lrm: 1.00 | dt: 13810.25ms | tok/sec: 37,963 | mfu: 12.20 | total time: 538.39m
step 02344/21400 (10.95%) | loss: 3.225421 | grad norm: 0.1036 | lrm: 1.00 | dt: 13833.84ms | tok/sec: 37,898 | mfu: 12.18 | total time: 538.62m
step 02345/21400 (10.96%) | loss: 3.211959 | grad norm: 0.1011 | lrm: 1.00 | dt: 13781.56ms | tok/sec: 38,042 | mfu: 12.22 | total time: 538.85m
step 02346/21400 (10.96%) | loss: 3.199667 | grad norm: 0.1024 | lrm: 1.00 | dt: 13855.64ms | tok/sec: 37,839 | mfu: 12.16 | total time: 539.08m
step 02347/21400 (10.97%) | loss: 3.218823 | grad norm: 0.1111 | lrm: 1.00 | dt: 13753.35ms | tok/sec: 38,120 | mfu: 12.25 | total time: 539.31m
step 02348/21400 (10.97%) | loss: 3.193482 | grad norm: 0.1153 | lrm: 1.00 | dt: 13854.51ms | tok/sec: 37,842 | mfu: 12.16 | total time: 539.54m
step 02349/21400 (10.98%) | loss: 3.213806 | grad norm: 0.1085 | lrm: 1.00 | dt: 13751.08ms | tok/sec: 38,127 | mfu: 12.25 | total time: 539.77m
step 02350/21400 (10.98%) | loss: 3.200771 | grad norm: 0.1118 | lrm: 1.00 | dt: 13842.48ms | tok/sec: 37,875 | mfu: 12.17 | total time: 540.00m
step 02351/21400 (10.99%) | loss: 3.296476 | grad norm: 0.1045 | lrm: 1.00 | dt: 13774.50ms | tok/sec: 38,062 | mfu: 12.23 | total time: 540.23m
step 02352/21400 (10.99%) | loss: 3.250112 | grad norm: 0.1080 | lrm: 1.00 | dt: 13829.26ms | tok/sec: 37,911 | mfu: 12.18 | total time: 540.46m
step 02353/21400 (11.00%) | loss: 3.246687 | grad norm: 0.1126 | lrm: 1.00 | dt: 13783.10ms | tok/sec: 38,038 | mfu: 12.22 | total time: 540.69m
step 02354/21400 (11.00%) | loss: 3.254453 | grad norm: 0.1175 | lrm: 1.00 | dt: 13818.83ms | tok/sec: 37,940 | mfu: 12.19 | total time: 540.92m
step 02355/21400 (11.00%) | loss: 3.254703 | grad norm: 0.1272 | lrm: 1.00 | dt: 13813.36ms | tok/sec: 37,955 | mfu: 12.19 | total time: 541.15m
step 02356/21400 (11.01%) | loss: 3.203848 | grad norm: 0.1310 | lrm: 1.00 | dt: 13795.99ms | tok/sec: 38,002 | mfu: 12.21 | total time: 541.38m
step 02357/21400 (11.01%) | loss: 3.155422 | grad norm: 0.1158 | lrm: 1.00 | dt: 13870.37ms | tok/sec: 37,799 | mfu: 12.14 | total time: 541.61m
step 02358/21400 (11.02%) | loss: 3.207469 | grad norm: 0.1121 | lrm: 1.00 | dt: 13747.74ms | tok/sec: 38,136 | mfu: 12.25 | total time: 541.84m
step 02359/21400 (11.02%) | loss: 3.199045 | grad norm: 0.1145 | lrm: 1.00 | dt: 13855.15ms | tok/sec: 37,840 | mfu: 12.16 | total time: 542.07m
step 02360/21400 (11.03%) | loss: 3.193348 | grad norm: 0.1075 | lrm: 1.00 | dt: 13756.20ms | tok/sec: 38,112 | mfu: 12.24 | total time: 542.30m
step 02361/21400 (11.03%) | loss: 3.159047 | grad norm: 0.1126 | lrm: 1.00 | dt: 14387.95ms | tok/sec: 36,439 | mfu: 11.71 | total time: 542.54m
step 02362/21400 (11.04%) | loss: 3.172284 | grad norm: 0.1038 | lrm: 1.00 | dt: 13755.25ms | tok/sec: 38,115 | mfu: 12.24 | total time: 542.77m
step 02363/21400 (11.04%) | loss: 3.163019 | grad norm: 0.0965 | lrm: 1.00 | dt: 13852.80ms | tok/sec: 37,847 | mfu: 12.16 | total time: 543.00m
step 02364/21400 (11.05%) | loss: 3.159508 | grad norm: 0.1096 | lrm: 1.00 | dt: 13769.75ms | tok/sec: 38,075 | mfu: 12.23 | total time: 543.23m
step 02365/21400 (11.05%) | loss: 3.158022 | grad norm: 0.1132 | lrm: 1.00 | dt: 13855.51ms | tok/sec: 37,839 | mfu: 12.16 | total time: 543.46m
step 02366/21400 (11.06%) | loss: 3.185534 | grad norm: 0.1077 | lrm: 1.00 | dt: 13791.14ms | tok/sec: 38,016 | mfu: 12.21 | total time: 543.69m
step 02367/21400 (11.06%) | loss: 3.202435 | grad norm: 0.1037 | lrm: 1.00 | dt: 13834.43ms | tok/sec: 37,897 | mfu: 12.17 | total time: 543.92m
step 02368/21400 (11.07%) | loss: 3.206142 | grad norm: 0.1015 | lrm: 1.00 | dt: 13799.14ms | tok/sec: 37,994 | mfu: 12.21 | total time: 544.15m
step 02369/21400 (11.07%) | loss: 3.205507 | grad norm: 0.1101 | lrm: 1.00 | dt: 13810.81ms | tok/sec: 37,962 | mfu: 12.20 | total time: 544.38m
